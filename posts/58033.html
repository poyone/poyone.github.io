<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>NLP Baseline 01 ç¿»è¯‘ | Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="ä»å¤´è®­ç»ƒï¼Œä¸å¦‚fine-tuneï¼Œå¦‚æœä½ æ¯”Google &amp; Mate æœ‰é’±å½“æˆ‘æ²¡è¯´  å¾…å®Œæˆ Accelarator get_scheduler custom_wandb  Translationè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨zh-ençš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œè¿›è¡Œç¿»è¯‘ä»»åŠ¡ è¿™é‡Œéœ€è¦æ³¨å†Œä¸€ä¸ªwandbçš„è´¦å·ï¼Œè®°å¾—å•Šã€‚ ç¤ºä¾‹æŸ¥çœ‹123456789101112131415161718192021from transfo">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP Baseline 01 ç¿»è¯‘">
<meta property="og:url" content="https://poyone.github.io/posts/58033.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:description" content="ä»å¤´è®­ç»ƒï¼Œä¸å¦‚fine-tuneï¼Œå¦‚æœä½ æ¯”Google &amp; Mate æœ‰é’±å½“æˆ‘æ²¡è¯´  å¾…å®Œæˆ Accelarator get_scheduler custom_wandb  Translationè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨zh-ençš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œè¿›è¡Œç¿»è¯‘ä»»åŠ¡ è¿™é‡Œéœ€è¦æ³¨å†Œä¸€ä¸ªwandbçš„è´¦å·ï¼Œè®°å¾—å•Šã€‚ ç¤ºä¾‹æŸ¥çœ‹123456789101112131415161718192021from transfo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/Asuka/Asuka26.webp">
<meta property="article:published_time" content="2022-12-04T16:10:48.719Z">
<meta property="article:modified_time" content="2022-12-06T10:49:56.861Z">
<meta property="article:author" content="Poy One">
<meta property="article:tag" content="Huggingface">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/Asuka/Asuka26.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/posts/58033"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP Baseline 01 ç¿»è¯‘',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-06 18:49:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://npm.elemecdn.com/poyone1222/Asuka/Asuka26.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP Baseline 01 ç¿»è¯‘</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-04T16:10:48.719Z" title="å‘è¡¨äº 2022-12-05 00:10:48">2022-12-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-06T10:49:56.861Z" title="æ›´æ–°äº 2022-12-06 18:49:56">2022-12-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">3.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>17åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP Baseline 01 ç¿»è¯‘"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>ä»å¤´è®­ç»ƒï¼Œä¸å¦‚fine-tuneï¼Œå¦‚æœä½ æ¯”Google &amp; Mate æœ‰é’±å½“æˆ‘æ²¡è¯´</p>
</blockquote>
<h1 id="å¾…å®Œæˆ"><a href="#å¾…å®Œæˆ" class="headerlink" title="å¾…å®Œæˆ"></a>å¾…å®Œæˆ</h1><ul>
<li>Accelarator</li>
<li>get_scheduler</li>
<li>custom_wandb</li>
</ul>
<h1 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h1><p>è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨zh-ençš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œè¿›è¡Œç¿»è¯‘ä»»åŠ¡</p>
<p>è¿™é‡Œéœ€è¦æ³¨å†Œä¸€ä¸ªwandbçš„è´¦å·ï¼Œè®°å¾—å•Šã€‚</p>
<h1 id="ç¤ºä¾‹æŸ¥çœ‹"><a href="#ç¤ºä¾‹æŸ¥çœ‹" class="headerlink" title="ç¤ºä¾‹æŸ¥çœ‹"></a>ç¤ºä¾‹æŸ¥çœ‹</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">prx = &#123;<span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://127.0.0.1:7890&#x27;</span>&#125;</span><br><span class="line">model_name = <span class="string">&quot;Helsinki-NLP/opus-mt-zh-en&quot;</span></span><br><span class="line">save_path = <span class="string">r&#x27;D:\00mydataset\huggingface model&#x27;</span></span><br><span class="line">data_path = <span class="string">r&#x27;D:\00mydataset\huggingface dataset&#x27;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name, proxies=prx, cache_dir=save_path)</span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name, proxies=prx, cache_dir=save_path)</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&#x27;news_commentary&#x27;</span>,<span class="string">&#x27;en-zh&#x27;</span>,cache_dir=data_path)</span><br><span class="line">dataset</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;id&#x27;, &#x27;translation&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 69206</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>è¿™ä¸ªæŒ‚ä¸ªä»£ç†åŠ é€Ÿä¸‹</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">PreTrainedTokenizer(name_or_path=&#x27;Helsinki-NLP/opus-mt-zh-en&#x27;, vocab_size=65001, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;&#125;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">dataset[&#x27;</span>train<span class="string">&#x27;][1][&#x27;</span>translation<span class="string">&#x27;]</span></span><br><span class="line"><span class="string">&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;translation&#x27;</span>: &#123;<span class="string">&#x27;en&#x27;</span>: <span class="string">&#x27;PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening. At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;zh&#x27;</span>: <span class="string">&#x27;å·´é»-éšç€ç»æµå±æœºä¸æ–­åŠ æ·±å’Œè”“å»¶ï¼Œæ•´ä¸ªä¸–ç•Œä¸€ç›´åœ¨å¯»æ‰¾å†å²ä¸Šçš„ç±»ä¼¼äº‹ä»¶å¸Œæœ›æœ‰åŠ©äºæˆ‘ä»¬äº†è§£ç›®å‰æ­£åœ¨å‘ç”Ÿçš„æƒ…å†µã€‚ä¸€å¼€å§‹ï¼Œå¾ˆå¤šäººæŠŠè¿™æ¬¡å±æœºæ¯”ä½œ1982å¹´æˆ–1973å¹´æ‰€å‘ç”Ÿçš„æƒ…å†µï¼Œè¿™æ ·å¾—ç±»æ¯”æ˜¯ä»¤äººå®½å¿ƒçš„ï¼Œå› ä¸ºè¿™ä¸¤æ®µæ—¶æœŸæ„å‘³ç€å…¸å‹çš„å‘¨æœŸæ€§è¡°é€€ã€‚&#x27;</span>&#125;&#125;</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  </span></span><br></pre></td></tr></table></figure>

<p>æŸ¥çœ‹ä¸‹æ•°æ®, å¯ä»¥çœ‹åˆ°è¿”å›çš„æ˜¯å­—å…¸å½¢å¼ï¼Œæˆ‘ä»¬ä¸»è¦ç”¨åˆ°translationä¸‹çš„enã€zh</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">s1 = <span class="string">&#x27;å¤©ä¸‹ç¬¬ä¸€ç¾å°‘å¥³, ç½¢äº†&#x27;</span></span><br><span class="line">inputs = tokenizer(s1, return_tensors=<span class="string">&#x27;pt&#x27;</span>,)</span><br><span class="line">inputs</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&#123;&#x27;input_ids&#x27;: tensor([[ 9705,   359,  3615,  2797, 14889,     2,     7, 40798,     0]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])&#125;,)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">outputs = model.generate(**inputs)</span><br><span class="line">tokenizer.batch_decode(outputs, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&quot;The most beautiful girl in the world, that&#x27;s all.&quot;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>çœ‹ä¸‹è¾“å‡ºï¼Œè¿˜å¯ä»¥</p>
<blockquote>
<p>æ³¨æ„ï¼ŒAutoModelForSeq2SeqLMä¸åŒäºAutoModelçš„å°±æ˜¯åŠ å…¥äº†<code>model.generate</code>è¿™ä¸ªç‰¹æ€§ã€‚</p>
<p>ä¸ç„¶model(**inputs)æ˜¯è¦ä½ è¡¥å……ç›®æ ‡è¯­è¨€çš„ã€‚</p>
</blockquote>
<blockquote>
<p>If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting <code>tokenizer.src_lang</code> and <code>tokenizer.tgt_lang</code> to the right values.</p>
<ul>
<li>â€‹	å¦‚æœä½ ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹ï¼Œä½ å¾—æŒ‡å®šä½ çš„æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„å‚æ•°</li>
</ul>
</blockquote>
<hr>
<h1 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">split_datasets = raw_datasets[<span class="string">&quot;train&quot;</span>].train_test_split(train_size=<span class="number">0.9</span>, seed=<span class="number">20</span>)</span><br><span class="line">split_datasets</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;id&#x27;, &#x27;translation&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 189155</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    test: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;id&#x27;, &#x27;translation&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 21018</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">split_datasets[<span class="string">&quot;validation&quot;</span>] = split_datasets.pop(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><p>HFçš„datasetå¯ä»¥ç›´æ¥è°ƒç”¨<code>.train_test_split(train_size=0.9, seed=20)</code></p>
<ul>
<li>HFçš„datasetå¯ä»¥ç›´æ¥è½¬DataFrameï¼Œè¿™æ ·ä½ ä¹Ÿå¯ä»¥ç›´æ¥é…åˆSklearnä½¿ç”¨</li>
</ul>
</li>
<li><p>ç»™testé‡å‘½åä¸ºvalidation</p>
</li>
</ul>
<h2 id="DataCollatorForSeq2Seq"><a href="#DataCollatorForSeq2Seq" class="headerlink" title="DataCollatorForSeq2Seq"></a>DataCollatorForSeq2Seq</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">max_length = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = [ex[<span class="string">&quot;en&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    targets = [ex[<span class="string">&quot;fr&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    model_inputs = tokenizer(</span><br><span class="line">        inputs, text_target=targets, max_length=max_length, truncation=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br><span class="line">    </span><br><span class="line">tokenized_datasets = split_datasets.<span class="built_in">map</span>(</span><br><span class="line">    preprocess_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=split_datasets[<span class="string">&quot;train&quot;</span>].column_names,)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">è¿™é‡Œæœ¬æ¥è¿˜æœ‰[&#x27;id&#x27;, &#x27;translation&#x27;],é€šè¿‡ä¸‹é¢çš„è®¾ç½®å°±åˆ é™¤äº†ã€‚</span></span><br><span class="line"><span class="string">remove_columns:</span></span><br><span class="line"><span class="string">    Remove a selection of columns while doing the mapping.</span></span><br><span class="line"><span class="string">    Columns will be removed before updating the examples with the output of `function`,i.e. </span></span><br><span class="line"><span class="string">    if `function` is adding columns with names in `remove_columns`, these columns will be kept.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>We donâ€™t pay attention to the attention mask of the targets, as the model wonâ€™t expect it. Instead, <strong>the labels corresponding to a padding token should be set to <code>-100</code> so they are ignored in the loss computatio</strong>n. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to <code>-100</code>.</p>
<p>è¿™é‡Œæˆ‘ä»¬ä¸ä¼šåŠ å…¥paddingï¼Œmaskã€‚ä¹‹åæˆ‘ä»¬çš„maskä¼šè®¾æˆ-100 ä½¿å…¶ä¸ä¼šè®¡ç®—æŸå¤±ã€‚è¿™äº›éƒ½æ˜¯ä¸‹ä¸€æ­¥çš„æ“ä½œ</p>
</blockquote>
<p>ps: ä»Šå¤©çœ‹åˆ°ä¸ªbugï¼Œåº”è¯¥æ˜¯æ²¡æœ‰æ›´æ–°åˆ°æœ€æ–°ç‰ˆç‰ˆï¼Œå…·ä½“æ¥è¯´å°±æ˜¯tokenizerä¹‹åæ²¡æœ‰labelï¼Œå¦‚æœbugäº†ï¼Œå¯ä»¥è¿›è¡Œä»¥ä¸‹æ›¿æ¢</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = [ex[<span class="string">&quot;zh&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    targets = [ex[<span class="string">&quot;en&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up the tokenizer for targets</span></span><br><span class="line">    <span class="keyword">with</span> tokenizer.as_target_tokenizer():</span><br><span class="line">        labels = tokenizer(targets, max_length=max_target_length, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model_inputs[<span class="string">&quot;labels&quot;</span>] = labels[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br><span class="line"></span><br><span class="line">tokenized_datasets = split_datasets.<span class="built_in">map</span>(</span><br><span class="line">    preprocess_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=split_datasets[<span class="string">&quot;train&quot;</span>].column_names,)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForSeq2Seq</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)</span><br><span class="line">batch = data_collator([tokenized_datasets[<span class="string">&quot;train&quot;</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>)])</span><br><span class="line">batch.keys()</span><br><span class="line"><span class="comment"># dict_keys([&#x27;attention_mask&#x27;, &#x27;input_ids&#x27;, &#x27;labels&#x27;, &#x27;decoder_input_ids&#x27;])</span></span><br><span class="line"></span><br><span class="line">batch[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> tensor([[57483,     7,  3241,   403,     3,   289,  1817, 25787,    22,     6,</span></span><br><span class="line"><span class="string">          38697,    22,     2,     3,   426,    64,    72, 27734,    14,  9054,</span></span><br><span class="line"><span class="string">          56467,  6667,     8,   721,   512,  2498,   209,    64,    72, 11468,</span></span><br><span class="line"><span class="string">              5,   393,     3,  2597,     4,     3,  1817,     2,   469,   235,</span></span><br><span class="line"><span class="string">            238, 24898,    39,     8, 13579,    50, 17528,     2,    60,    42,</span></span><br><span class="line"><span class="string">          56548,     2,   695,   443, 10119,  5543,     8, 53617,     7, 38261,</span></span><br><span class="line"><span class="string">          40490,    22,     5,     0],</span></span><br><span class="line"><span class="string">         [   24, 22026,    30,  2329, 10349, 22901,    20, 52813,    17,    50,</span></span><br><span class="line"><span class="string">             12, 29940,     4,     3,  2121,    20,  1843,    45,    67,   243,</span></span><br><span class="line"><span class="string">           1945,    30,   368, 36681,    10,     3,  1796,     4, 14961,  2203,</span></span><br><span class="line"><span class="string">              6, 28291,     3, 22986,     2, 11355,     3,  3368,    64,  8700,</span></span><br><span class="line"><span class="string">             18,   469, 38575,    10,   278,    54,     8,  4291,    57, 22301,</span></span><br><span class="line"><span class="string">           1718,     8,   959, 30229,  1294,  6855,  4298,     5,     0,  -100,</span></span><br><span class="line"><span class="string">           -100,  -100,  -100,  -100]])&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># çœ‹ä¸‹åŸæ¥çš„token</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokenized_datasets[<span class="string">&quot;train&quot;</span>][i][<span class="string">&quot;labels&quot;</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[57483, 7, 3241, 403, 3, 289, 1817, 25787, 22, 6, 38697, 22, 2, 3, 426, 64, 72, 27734, 14, 9054, 56467, 6667, 8, 721, 512, 2498, 209, 64, 72, 11468, 5, 393, 3, 2597, 4, 3, 1817, 2, 469, 235, 238, 24898, 39, 8, 13579, 50, 17528, 2, 60, 42, 56548, 2, 695, 443, 10119, 5543, 8, 53617, 7, 38261, 40490, 22, 5, 0]</span></span><br><span class="line"><span class="string">[24, 22026, 30, 2329, 10349, 22901, 20, 52813, 17, 50, 12, 29940, 4, 3, 2121, 20, 1843, 45, 67, 243, 1945, 30, 368, 36681, 10, 3, 1796, 4, 14961, 2203, 6, 28291, 3, 22986, 2, 11355, 3, 3368, 64, 8700, 18, 469, 38575, 10, 278, 54, 8, 4291, 57, 22301, 1718, 8, 959, 30229, 1294, 6855, 4298, 5, 0]&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>å¯ä»¥çœ‹åˆ°paddingçš„ä½ç½®éƒ½å˜æˆ-100äº†ï¼Œ<a href="https://poyone.github.io/posts/13310.html">pytorchä¸­ä¹Ÿæœ‰è¿™ä¸ªè®¾å®šå¯è§æˆ‘ä¹‹å‰è®²Transformerçš„å†…å®¹</a></li>
</ul>
<blockquote>
<p> This is all done by a <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq"><code>DataCollatorForSeq2Seq</code></a>. Like the <code>DataCollatorWithPadding</code>, it takes the <code>tokenizer</code> used to preprocess the inputs, but it also takes the <code>model</code>. This is because this <strong>data collator will also be responsible for preparing the decoder input IDs</strong>, which are <strong>shifted versions of the labels</strong>ï¼ˆç§»åŠ¨ç‰ˆçš„æ ‡ç­¾ï¼‰ with a special token at the beginning. Since this shift is done slightly differently for different architectures, the <code>DataCollatorForSeq2Seq</code> needs to know the <code>model</code> object<del>æå¾—è¿˜æŒºå¤æ‚</del></p>
</blockquote>
<hr>
<h1 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h1><blockquote>
<p>One weakness with <strong>BLEU is that it expects the text to already be tokenized</strong>, <strong>which makes it difficult to compare scores</strong> <strong>between models that use different tokenizers</strong>. So instead, the most commonly used metric for <strong>benchmarking translation models today is <a target="_blank" rel="noopener" href="https://github.com/mjpost/sacrebleu">SacreBLEU</a>,</strong> which addresses this weakness (and others) by standardizing the tokenization step</p>
</blockquote>
<ul>
<li>è¿™é‡Œæˆ‘ä»¬åŠ å…¥SacreBLEUä½œä¸ºè¯„åˆ†æ ‡å‡†</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!pip install sacrebleu	</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line">metric = evaluate.load(<span class="string">&quot;sacrebleu&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>ç¤ºä¾‹1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">predictions = [</span><br><span class="line">    <span class="string">&quot;This plugin lets you translate web pages between several languages automatically.&quot;</span></span><br><span class="line">]</span><br><span class="line">references = [</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span></span><br><span class="line">    ]</span><br><span class="line">]</span><br><span class="line">metric.compute(predictions=predictions, references=references)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;score&#x27;: 46.750469682990165,</span></span><br><span class="line"><span class="string"> &#x27;counts&#x27;: [11, 6, 4, 3],</span></span><br><span class="line"><span class="string"> &#x27;totals&#x27;: [12, 11, 10, 9],</span></span><br><span class="line"><span class="string"> &#x27;precisions&#x27;: [91.67, 54.54, 40.0, 33.33],</span></span><br><span class="line"><span class="string"> &#x27;bp&#x27;: 0.9200444146293233,</span></span><br><span class="line"><span class="string"> &#x27;sys_len&#x27;: 12,</span></span><br><span class="line"><span class="string"> &#x27;ref_len&#x27;: 13&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure>



<p>ç¤ºä¾‹2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">predictions = [<span class="string">&quot;This This This This&quot;</span>]</span><br><span class="line">references = [</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span></span><br><span class="line">    ]</span><br><span class="line">]</span><br><span class="line">metric.compute(predictions=predictions, references=references)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;score&#x27;: 1.683602693167689,</span></span><br><span class="line"><span class="string"> &#x27;counts&#x27;: [1, 0, 0, 0],</span></span><br><span class="line"><span class="string"> &#x27;totals&#x27;: [4, 3, 2, 1],</span></span><br><span class="line"><span class="string"> &#x27;precisions&#x27;: [25.0, 16.67, 12.5, 12.5],</span></span><br><span class="line"><span class="string"> &#x27;bp&#x27;: 0.10539922456186433,</span></span><br><span class="line"><span class="string"> &#x27;sys_len&#x27;: 4,</span></span><br><span class="line"><span class="string"> &#x27;ref_len&#x27;: 13&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<blockquote>
<p>This gets a BLEU score of 46.75, which is rather good â€” for reference, the original Transformer model in the <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">â€œAttention Is All You Needâ€ paper</a> achieved a BLEU score of 41.8 on a similar translation task between English and French! (For more information about the individual metrics, like <code>counts</code> and <code>bp</code>, see the <a target="_blank" rel="noopener" href="https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74">SacreBLEU repository</a>.) </p>
<ul>
<li>å·²ç»æ¯”æ“å¤©æŸ±è¿˜å‰å®³äº†ï¼Œå¦å¤–çš„æ ‡å‡†å¦‚ä¸‹:<ul>
<li>score: The BLEU score.  </li>
<li>counts: List of counts of correct ngrams, 1 &lt;&#x3D; n &lt;&#x3D; max_ngram_order  </li>
<li>totals: List of counts of total ngrams, 1 &lt;&#x3D; n &lt;&#x3D; max_ngram_order </li>
<li>precisions: List of precisions, 1 &lt;&#x3D; n &lt;&#x3D; max_ngram_order  </li>
<li>bp: The brevity penalty. </li>
<li>sys_len: The cumulative system length.</li>
<li>ref_len: The cumulative reference length.</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="Compute-metrics"><a href="#Compute-metrics" class="headerlink" title="Compute_metrics"></a>Compute_metrics</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_preds</span>):</span><br><span class="line">    preds, labels = eval_preds</span><br><span class="line">    <span class="comment"># In case the model returns more than the prediction logits</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(preds, <span class="built_in">tuple</span>):</span><br><span class="line">        preds = preds[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Replace -100s in the labels as we can&#x27;t decode them</span></span><br><span class="line">    labels = np.where(labels != -<span class="number">100</span>, labels, tokenizer.pad_token_id)</span><br><span class="line">    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Some simple post-processing</span></span><br><span class="line">    decoded_preds = [pred.strip() <span class="keyword">for</span> pred <span class="keyword">in</span> decoded_preds]</span><br><span class="line">    decoded_labels = [[label.strip()] <span class="keyword">for</span> label <span class="keyword">in</span> decoded_labels]</span><br><span class="line"></span><br><span class="line">    result = metric.compute(predictions=decoded_preds, references=decoded_labels)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;bleu&quot;</span>: result[<span class="string">&quot;score&quot;</span>]&#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<ul>
<li>å› ä¸ºdecodeä¼šè‡ªåŠ¨å¤„ç†pad_tokenæ‰€ä»¥ä½¿ç”¨<code>np.where</code>å°†-100éƒ½æ›¿æ¢æˆpad_token<ul>
<li>numpy.where(condition,  xï¼Œy) ï¼Œxä¸­æ¡ä»¶ä¸æˆç«‹çš„éƒ½ä¼šè¢«å¡«å……y</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h1><p>é¦–å…ˆçœ‹ä¸‹<code>Seq2SeqTrainer</code> ç„¶åå›åˆ°è‡ªå®šä¹‰çš„Loop</p>
<h2 id="Seq2SeqTrainer"><a href="#Seq2SeqTrainer" class="headerlink" title="Seq2SeqTrainer"></a>Seq2SeqTrainer</h2><p>è¿™é‡Œä¸æ˜¯é‡ç‚¹ï¼Œä½†æ˜¯æœ‰äº›ç»†èŠ‚å¯åœˆå¯ç‚¹ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainingArguments</span><br><span class="line"></span><br><span class="line">args = Seq2SeqTrainingArguments(</span><br><span class="line">    <span class="string">f&quot;marian-finetuned-kde4-en-to-fr&quot;</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;no&quot;</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">32</span>,</span><br><span class="line">    per_device_eval_batch_size=<span class="number">64</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    save_total_limit=<span class="number">3</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    predict_with_generate=<span class="literal">True</span>,</span><br><span class="line">    fp16=<span class="literal">True</span>,</span><br><span class="line">    push_to_hub=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>å®šä¹‰å‚æ•°</p>
<ul>
<li>We donâ€™t set any regular evaluation, as evaluation takes a while; we will just evaluate our model once before training and after.<ul>
<li>ç”±äºæˆ‘ä»¬è‡ªå®šä¹‰è¯„ä»·æ ‡å‡†ï¼Œè¿™é‡Œæˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒå‰æµ‹ä¸€æ¬¡scoresï¼Œè®­ç»ƒå®Œæˆåå†æµ‹ä¸€æ¬¡</li>
</ul>
</li>
<li>We set <code>fp16=True</code>, which speeds up training on modern GPUs.</li>
<li>We set <code>predict_with_generate=True</code>, as discussed above.<ul>
<li>the decoder performs inference by predicting tokens one by one â€” something thatâ€™s implemented behind the scenes in ğŸ¤— Transformers by the <code>generate()</code> method. The <code>Seq2SeqTrainer</code> will let us use that method for evaluation if we set <code>predict_with_generate=True</code>.</li>
</ul>
</li>
<li>We use <code>push_to_hub=True</code> to upload the model to the Hub at the end of each epoch</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainer</span><br><span class="line"></span><br><span class="line">trainer = Seq2SeqTrainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.evaluate(max_length=max_length)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;eval_loss&#x27;: 1.6964408159255981,</span></span><br><span class="line"><span class="string"> &#x27;eval_bleu&#x27;: 39.26865061007616,</span></span><br><span class="line"><span class="string"> &#x27;eval_runtime&#x27;: 965.8884,</span></span><br><span class="line"><span class="string"> &#x27;eval_samples_per_second&#x27;: 21.76,</span></span><br><span class="line"><span class="string"> &#x27;eval_steps_per_second&#x27;: 0.341&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line"> trainer.train()</span><br><span class="line"> trainer.evaluate(max_length=max_length)</span><br><span class="line"> <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> &#123;&#x27;eval_loss&#x27;: 0.8558505773544312,</span></span><br><span class="line"><span class="string"> &#x27;eval_bleu&#x27;: 52.94161337775576,</span></span><br><span class="line"><span class="string"> &#x27;eval_runtime&#x27;: 714.2576,</span></span><br><span class="line"><span class="string"> &#x27;eval_samples_per_second&#x27;: 29.426,</span></span><br><span class="line"><span class="string"> &#x27;eval_steps_per_second&#x27;: 0.461,</span></span><br><span class="line"><span class="string"> &#x27;epoch&#x27;: 3.0&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p><strong>Thatâ€™s a nearly 14-point improvement, which is great.</strong></p>
<h2 id="Custom-Training-Loop"><a href="#Custom-Training-Loop" class="headerlink" title="Custom Training Loop"></a>Custom Training Loop</h2><p>æ¥ä¸‹æ¥å°±æ˜¯é‡ç‚¹äº†</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=data_collator, <span class="comment"># å°±æ˜¯ä¸Šé¢çš„DataCollatorForSeq2Seq(tokenizer, model=model)</span></span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=<span class="number">8</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ä»¥ä¸Šæ˜¯å¸¸è§„å®šä¹‰</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line">model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(</span><br><span class="line">    model, optimizer, train_dataloader, eval_dataloader)</span><br><span class="line"></span><br><span class="line">num_train_epochs = <span class="number">3</span></span><br><span class="line">num_update_steps_per_epoch = <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">num_training_steps = num_train_epochs * num_update_steps_per_epoch</span><br><span class="line"></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>è¿™ä¸¤éƒ¨åˆ†éƒ½éœ€è¦æ³¨æ„</p>
<ul>
<li>Once we have all those objects, we can send them to the <code>accelerator.prepare()</code> method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldnâ€™t execute any cell that instantiates an <code>Accelerator</code><ul>
<li>ä¸è¦åœ¨æ²¡æœ‰æŠŠæ‰€æœ‰éƒ¨ä»¶è½¬åˆ°TPUå‰ä½¿ç”¨<code>Accelerator</code></li>
</ul>
</li>
<li>we can use its length to compute the number of training steps. Remember we should always do this after preparing the dataloader, as that method will change the length of the <code>DataLoader</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">predictions, labels</span>):</span><br><span class="line">    predictions = predictions.cpu().numpy()</span><br><span class="line">    labels = labels.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Replace -100 in the labels as we can&#x27;t decode them.</span></span><br><span class="line">    labels = np.where(labels != -<span class="number">100</span>, labels, tokenizer.pad_token_id)</span><br><span class="line">    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Some simple post-processing</span></span><br><span class="line">    decoded_preds = [pred.strip() <span class="keyword">for</span> pred <span class="keyword">in</span> decoded_preds]</span><br><span class="line">    decoded_labels = [[label.strip()] <span class="keyword">for</span> label <span class="keyword">in</span> decoded_labels]</span><br><span class="line">    <span class="keyword">return</span> decoded_preds, decoded_labels</span><br></pre></td></tr></table></figure>

<p>æ­£å¼è®­ç»ƒä¹‹å‰ï¼Œå…ˆå®šä¹‰ä¸€ä¸‹åå¤„ç†å‡½æ•°ï¼Œè¾“å‡ºæˆ‘ä»¬é¢„æµ‹çš„æ ‡ç­¾ç»™Sacrebleu</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_train_epochs):</span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluation</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(eval_dataloader):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            generated_tokens = accelerator.unwrap_model(model).generate(</span><br><span class="line">                batch[<span class="string">&quot;input_ids&quot;</span>],</span><br><span class="line">                attention_mask=batch[<span class="string">&quot;attention_mask&quot;</span>],</span><br><span class="line">                max_length=<span class="number">128</span>,</span><br><span class="line">            )</span><br><span class="line">        labels = batch[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Necessary to pad predictions and labels for being gathered</span></span><br><span class="line">        generated_tokens = accelerator.pad_across_processes(</span><br><span class="line">            generated_tokens, dim=<span class="number">1</span>, pad_index=tokenizer.pad_token_id</span><br><span class="line">        )</span><br><span class="line">        labels = accelerator.pad_across_processes(labels, dim=<span class="number">1</span>, pad_index=-<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        predictions_gathered = accelerator.gather(generated_tokens)</span><br><span class="line">        labels_gathered = accelerator.gather(labels)</span><br><span class="line"></span><br><span class="line">        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)</span><br><span class="line">        metric.add_batch(predictions=decoded_preds, references=decoded_labels)</span><br><span class="line"></span><br><span class="line">    results = metric.compute()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch <span class="subst">&#123;epoch&#125;</span>, BLEU score: <span class="subst">&#123;results[<span class="string">&#x27;score&#x27;</span>]:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save and upload</span></span><br><span class="line">    <span class="comment"># accelerator.wait_for_everyone()</span></span><br><span class="line"><span class="comment">#     unwrapped_model = accelerator.unwrap_model(model)</span></span><br><span class="line"><span class="comment">#     unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)</span></span><br><span class="line"><span class="comment">#     if accelerator.is_main_process:</span></span><br><span class="line"><span class="comment">#         tokenizer.save_pretrained(output_dir)</span></span><br><span class="line"><span class="comment">#         repo.push_to_hub(</span></span><br><span class="line"><span class="comment">#             commit_message=f&quot;Training in progress epoch &#123;epoch&#125;&quot;, blocking=False</span></span><br><span class="line"><span class="comment">#         )</span></span><br></pre></td></tr></table></figure>





<hr>
<p>æœ€åé€šè¿‡pipelineæµ‹è¯•ä¸‹è¾“å‡º</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace this with your own checkpoint</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;huggingface-course/marian-finetuned-kde4-en-to-fr&quot;</span></span><br><span class="line">translator = pipeline(<span class="string">&quot;translation&quot;</span>, model=model_checkpoint)</span><br><span class="line">translator(<span class="string">&quot;Default to expanded threads&quot;</span>)</span><br></pre></td></tr></table></figure>





<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><ul>
<li>é¦–å…ˆæ˜¯pipeline</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">!pip install sacrebleu</span><br><span class="line">!pip install evaluate</span><br><span class="line">!pip install accelerate</span><br><span class="line">!pip install --upgrade transformers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (AdamW, AutoModelForSeq2SeqLM, AutoTokenizer,</span><br><span class="line">                          DataCollatorForSeq2Seq, Seq2SeqTrainer,</span><br><span class="line">                          Seq2SeqTrainingArguments, get_scheduler,pipeline)</span><br><span class="line">                          </span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">logging.disable(logging.WARNING)</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_LAUNCH_BLOCKING&#x27;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">CONFIG = &#123;<span class="string">&quot;seed&quot;</span>: <span class="number">2021</span>,</span><br><span class="line">          <span class="string">&quot;epochs&quot;</span>: <span class="number">3</span>,</span><br><span class="line">          <span class="string">&quot;model_name&quot;</span>: <span class="string">&quot;roberta-base&quot;</span>,</span><br><span class="line">          <span class="string">&quot;train_batch_size&quot;</span>: <span class="number">32</span>,</span><br><span class="line">          <span class="string">&quot;valid_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">          <span class="string">&quot;max_length&quot;</span>: <span class="number">128</span>,</span><br><span class="line">          <span class="string">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">          <span class="string">&quot;scheduler&quot;</span>: <span class="string">&#x27;CosineAnnealingLR&#x27;</span>,</span><br><span class="line">          <span class="string">&quot;min_lr&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">          <span class="string">&quot;T_max&quot;</span>: <span class="number">500</span>,</span><br><span class="line">          <span class="string">&quot;weight_decay&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">          <span class="string">&quot;n_fold&quot;</span>: <span class="number">5</span>,</span><br><span class="line">          <span class="string">&quot;n_accumulate&quot;</span>: <span class="number">1</span>,</span><br><span class="line">          <span class="string">&quot;num_classes&quot;</span>: <span class="number">1</span>,</span><br><span class="line">          <span class="string">&quot;margin&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">          <span class="string">&quot;device&quot;</span>: torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>),</span><br><span class="line">          <span class="string">&quot;hash_name&quot;</span>: HASH_NAME</span><br><span class="line">          &#125;</span><br><span class="line">          </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed=<span class="number">42</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Sets the seed of the entire notebook so results are the same every time we run.</span></span><br><span class="line"><span class="string">    This is for REPRODUCIBILITY.&#x27;&#x27;&#x27;</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    <span class="comment"># When running on the CuDNN backend, two further options must be set</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    <span class="comment"># Set a fixed value for the hash seed</span></span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    </span><br><span class="line">set_seed(CONFIG[<span class="string">&#x27;seed&#x27;</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_name = <span class="string">&quot;Helsinki-NLP/opus-mt-zh-en&quot;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">dataset_raw = load_dataset(<span class="string">&#x27;news_commentary&#x27;</span>,<span class="string">&#x27;en-zh&#x27;</span>)</span><br><span class="line">dataset_raw, dataset_raw[<span class="string">&#x27;train&#x27;</span>][<span class="string">&#x27;translation&#x27;</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>NLP Baseline 01 ç¿»è¯‘</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://poyone.github.io/posts/58033.html">https://poyone.github.io/posts/58033.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>ä½œè€…</h><div class="post-copyright-cc-info"><h>Poy One</h></div></div><div class="post-copyright-c"><h>å‘å¸ƒäº</h><div class="post-copyright-cc-info"><h>2022-12-05</h></div></div><div class="post-copyright-u"><h>æ›´æ–°äº</h><div class="post-copyright-cc-info"><h>2022-12-06</h></div></div><div class="post-copyright-c"><h>è®¸å¯åè®®</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Huggingface/">Huggingface</a></div><div class="post_share"><div class="social-share" data-image="https://npm.elemecdn.com/poyone1222/Asuka/Asuka26.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/16149.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">Huggingface Course 01 åŸºç¡€æ¦‚å¿µ</div></div></a></div><div class="next-post pull-right"><a href="/posts/2534.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/made in Abyss/nanachi02.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">22-12-3 e.g etc. è¯¦è§£</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/posts/16149.html" title="Huggingface Course 01 åŸºç¡€æ¦‚å¿µ"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">Huggingface Course 01 åŸºç¡€æ¦‚å¿µ</div></div></a></div><div><a href="/posts/64185.html" title="Huggingface Course 02 APIæ¦‚è¦"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">Huggingface Course 02 APIæ¦‚è¦</div></div></a></div><div><a href="/posts/18130.html" title="Huggingface Course 03 å¾®è°ƒèŒƒå¼"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">Huggingface Course 03 å¾®è°ƒèŒƒå¼</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>ğŸ›´å‰å¾€github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%85%E5%AE%8C%E6%88%90"><span class="toc-text">å¾…å®Œæˆ</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Translation"><span class="toc-text">Translation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E6%9F%A5%E7%9C%8B"><span class="toc-text">ç¤ºä¾‹æŸ¥çœ‹</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Preprocessing"><span class="toc-text">Preprocessing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DataCollatorForSeq2Seq"><span class="toc-text">DataCollatorForSeq2Seq</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Metrics"><span class="toc-text">Metrics</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Compute-metrics"><span class="toc-text">Compute_metrics</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Training-Loop"><span class="toc-text">Training Loop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Seq2SeqTrainer"><span class="toc-text">Seq2SeqTrainer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Custom-Training-Loop"><span class="toc-text">Custom Training Loop</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">æ€»ç»“</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è®¡ç®—æœºè§†è§‰</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è‡ªç„¶è¯­è¨€å¤„ç†</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">è®ºæ–‡ç²¾è¯»</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">æµç•…çš„Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">8</span><span class="categoryBar-list-descr">æ‹¥æœ‰ä¸€åˆ‡ å´å˜æˆå¤ªç©º</span></li></ul></div></div>';
      console.log('å·²æŒ‚è½½butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>