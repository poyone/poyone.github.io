<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>HF Course 09 Custom Tokenizer | Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="ä¸ä¹‹å‰ç»§æ‰¿å¼çš„åˆ†è¯å™¨ä¸åŒï¼Œè¿™è¯æˆ‘ä»¬å°†ä»è¯­æ–™åº“ä¸­è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„åˆ†è¯å™¨ é¦–å…ˆæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªWordPieceç±»å‹çš„åˆ†è¯å™¨  åŠ è½½æ–‡æ¡£12345678910111213from datasets import load_datasetdataset &#x3D; load_dataset(&quot;wikitext&quot;, name&#x3D;&quot;wikitext-2-raw-v1&quot;, split">
<meta property="og:type" content="article">
<meta property="og:title" content="HF Course 09 Custom Tokenizer">
<meta property="og:url" content="https://poyone.github.io/posts/7194.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:description" content="ä¸ä¹‹å‰ç»§æ‰¿å¼çš„åˆ†è¯å™¨ä¸åŒï¼Œè¿™è¯æˆ‘ä»¬å°†ä»è¯­æ–™åº“ä¸­è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„åˆ†è¯å™¨ é¦–å…ˆæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªWordPieceç±»å‹çš„åˆ†è¯å™¨  åŠ è½½æ–‡æ¡£12345678910111213from datasets import load_datasetdataset &#x3D; load_dataset(&quot;wikitext&quot;, name&#x3D;&quot;wikitext-2-raw-v1&quot;, split">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp">
<meta property="article:published_time" content="2022-12-12T08:15:06.518Z">
<meta property="article:modified_time" content="2022-12-12T14:31:52.517Z">
<meta property="article:author" content="Poy One">
<meta property="article:tag" content="Huggingface">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/posts/7194"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HF Course 09 Custom Tokenizer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-12 22:31:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://npm.elemecdn.com/poyone1222/eris/Eris11.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">HF Course 09 Custom Tokenizer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-12T08:15:06.518Z" title="å‘è¡¨äº 2022-12-12 16:15:06">2022-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:31:52.517Z" title="æ›´æ–°äº 2022-12-12 22:31:52">2022-12-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Universe/">Universe</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">2.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>11åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HF Course 09 Custom Tokenizer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>ä¸ä¹‹å‰ç»§æ‰¿å¼çš„åˆ†è¯å™¨ä¸åŒï¼Œè¿™è¯æˆ‘ä»¬å°†ä»è¯­æ–™åº“ä¸­è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„åˆ†è¯å™¨</p>
<p>é¦–å…ˆæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªWordPieceç±»å‹çš„åˆ†è¯å™¨</p>
</blockquote>
<h1 id="åŠ è½½æ–‡æ¡£"><a href="#åŠ è½½æ–‡æ¡£" class="headerlink" title="åŠ è½½æ–‡æ¡£"></a>åŠ è½½æ–‡æ¡£</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), <span class="number">1000</span>):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + <span class="number">1000</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¹Ÿå¯ä»¥ä»æœ¬åœ°æ‰“å¼€æ–‡æ¡£</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;wikitext-2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">        f.write(dataset[i][<span class="string">&quot;text&quot;</span>] + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>



<h1 id="åŠ è½½æ„ä»¶"><a href="#åŠ è½½æ„ä»¶" class="headerlink" title="åŠ è½½æ„ä»¶"></a>åŠ è½½æ„ä»¶</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (</span><br><span class="line">    decoders,</span><br><span class="line">    models,</span><br><span class="line">    normalizers,</span><br><span class="line">    pre_tokenizers,</span><br><span class="line">    processors,</span><br><span class="line">    trainers,</span><br><span class="line">    Tokenizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>æˆ‘ä»¬ä»<code>tokenizeråº“</code>ä¸­åŠ è½½ç‰¹æ®Šçš„<code>model</code>æ„ä»¶ï¼Œæ¥ä½¿ç”¨<code>WordPiece</code>æ–¹æ³•</p>
<p>è®¾å®šé‡åˆ°æ²¡è§è¿‡çš„è¯æ ‡è®°ä¸º[UNK]ï¼Œ åŒæ—¶å¯ä»¥è®¾ç½®<code>max_input_chars_per_word</code>ä½œä¸ºæœ€å¤§è¯é•¿</p>
<h2 id="è®¾ç½®Normalizer"><a href="#è®¾ç½®Normalizer" class="headerlink" title="è®¾ç½®Normalizer"></a>è®¾ç½®Normalizer</h2><p>è¿™é‡Œæˆ‘ä»¬é€‰æ‹©bertçš„è®¾ç½®ï¼ŒåŒ…æ‹¬: </p>
<p>æ‰€æœ‰å­—æ¯å°å†™ã€strip_accentsé™¤å»é‡éŸ³ã€åˆ é™¤æ§åˆ¶å­—ç¬¦ã€å°†æ‰€æœ‰å¤šä¸ªç©ºæ ¼è®¾ç½®ä¸ºå•ä¸ªç©ºæ ¼ã€æ±‰å­—å‘¨å›´æ”¾ç½®ç©ºæ ¼ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä½ ä¹Ÿå¯ä»¥è‡ªå®šä¹‰</span></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="string">&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;</span>))</span><br><span class="line"><span class="comment"># hello how are u?</span></span><br></pre></td></tr></table></figure>

<p>ä¸Šé¢è‡ªå®šä¹‰ä¸­æˆ‘ä»¬ä½¿ç”¨<code>Sequence</code>æ–¹æ³•å®šä¹‰æˆ‘ä»¬è‡ªå·±çš„è§„èŒƒåŒ–è§„åˆ™</p>
<h2 id="Pre-tokenization"><a href="#Pre-tokenization" class="headerlink" title="Pre-tokenization"></a>Pre-tokenization</h2><p>å’Œä¸Šé¢ä¸€æ ·å¯ä»¥é€šè¿‡<code>tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</code>å¥—ç”¨bertçš„è®¾ç½®</p>
<p>ä¸‹é¢æ˜¯customç‰ˆæœ¬</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()</span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)),</span></span><br><span class="line"><span class="string"> (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<p><code>.Whitespace()</code>æ˜¯å¯¹æ ‡ç‚¹ç©ºæ ¼åˆ†éš”ï¼Œä½ å¯ç”¨ä¸‹é¢çš„åˆ†éš”</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pre_tokenizer = pre_tokenizers.WhitespaceSplit()</span><br><span class="line">pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&quot;Let&#x27;s&quot;, (0, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre-tokenizer.&#x27;, (14, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<p>æ¨èä½¿ç”¨Sequenceæ–¹æ³•ç»„åˆä½ çš„é¢„åˆ†è¯</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pre_tokenizer = pre_tokenizers.<span class="type">Sequence</span>(</span><br><span class="line">    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]</span><br><span class="line">)</span><br><span class="line">pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)),</span></span><br><span class="line"><span class="string"> (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p>è®­ç»ƒä¹‹å‰æˆ‘ä»¬éœ€è¦åŠ å…¥ç‰¹æ®Štokenå› ä¸ºä»–ä¸åœ¨ä½ çš„è¯åº“ä¹‹ä¸­</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br></pre></td></tr></table></figure>

<ul>
<li>As well as specifying the <code>vocab_size</code> and <code>special_tokens</code>, we can set the <code>min_frequency</code> (the number of times a token must appear to be included in the vocabulary) or change the <code>continuing_subword_prefix</code> (if we want to use something different from <code>##</code>).<ul>
<li>æ”¹æŸä¸ªtokenå¿…é¡»å‡ºç°å¤šå°‘æ¬¡ã€æ”¹è¿æ¥å‰ç¼€##ä¸ºåˆ«çš„</li>
</ul>
</li>
</ul>
<p>å¼€å§‹è®­ç»ƒ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å¦å¤–çš„ç‰ˆæœ¬</span></span><br><span class="line">tokenizer.model = models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>)</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure>

<p>ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ç”¨ä¸Šé¢å®šä¹‰çš„ç”Ÿæˆå™¨</p>
<p>ç¬¬äºŒç§ä¼ å…¥â€wikitext-2.txtâ€æ–‡ä»¶</p>
<p>åˆ°æ­¤æˆ‘ä»¬tokenizerå°±å…·æœ‰äº†ä¸€èˆ¬tokenizerçš„æ‰€æœ‰æ–¹æ³•å¦‚<code>encode</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [&#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;]</span></span><br></pre></td></tr></table></figure>



<h2 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h2><p>æœ€åæˆ‘ä»¬éœ€è¦åŒ…è£¹æˆ‘ä»¬çš„tokenåˆ°ç‰¹æ®Šçš„æ ¼å¼å¦‚: [CLS]â€¦[SEP]â€¦[SEP]</p>
<p>é¦–å…ˆæˆ‘ä»¬è·å–æ‰€éœ€çš„ç‰¹æ®Štokençš„ä¸‹æ ‡</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2, 3)</span></span><br></pre></td></tr></table></figure>



<p>æ¥ä¸‹æ¥å¤„ç†æˆ‘ä»¬çš„æ¨¡æ¿</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id)],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>æ¨¡æ¿æˆ‘ä»¬éœ€è¦è®¾ç½®ä¸¤ç§æ¨¡å¼:</p>
<ul>
<li><p>singleâ€“å•ä¸ªå¥å­æƒ…å†µä¸‹</p>
<ul>
<li>[0,0,0,0]</li>
</ul>
</li>
<li><p>pair</p>
<ul>
<li>[0,0,0,1,1,1]</li>
</ul>
</li>
<li><p>æœ€åæŒ‡å®šç‰¹æ®Štokençš„id</p>
</li>
</ul>
<p>æŸ¥çœ‹</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="comment"># [&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span></span><br><span class="line"></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="string">&quot;on a pair of sentences.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="built_in">print</span>(encoding.type_ids)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;...&#x27;, &#x27;[SEP]&#x27;, &#x27;on&#x27;, &#x27;a&#x27;, &#x27;pair&#x27;, &#x27;of&#x27;, &#x27;sentences&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span></span><br><span class="line"><span class="string">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h2><p>æ¥ä¸‹æ¥å¯¹è§£ç å™¨åšä¸€å®šè®¾ç½®</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.decode(encoding.ids)</span><br><span class="line"><span class="comment"># &quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="ä¿å­˜-amp-åŠ è½½-Custom-Tokenizer"><a href="#ä¿å­˜-amp-åŠ è½½-Custom-Tokenizer" class="headerlink" title="ä¿å­˜ &amp; åŠ è½½ Custom Tokenizer"></a>ä¿å­˜ &amp; åŠ è½½ Custom Tokenizer</h2><p><code>tokenizer.save(&quot;tokenizer.json&quot;)</code></p>
<p><code>new_tokenizer = Tokenizer.from_file(&quot;tokenizer.json&quot;)</code></p>
<h1 id="è½¬æˆFast-Tokenizer"><a href="#è½¬æˆFast-Tokenizer" class="headerlink" title="è½¬æˆFast Tokenizer"></a>è½¬æˆFast Tokenizer</h1><p>To use this tokenizer in ğŸ¤— Transformers, we have to wrap it in a <code>PreTrainedTokenizerFast</code>. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, <code>BertTokenizerFast</code>). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.</p>
<ul>
<li>å¯ä»¥ç»§æ‰¿ä½ çš„ç‰¹å®šç±»<code>BertTokenizerFast</code>ï¼Œä¹Ÿå¯ä»¥ç”¨æ³›ç±»<code>PreTrainedTokenizerFast</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    <span class="comment"># tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively</span></span><br><span class="line">    unk_token=<span class="string">&quot;[UNK]&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;[PAD]&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;[CLS]&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;[SEP]&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;[MASK]&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>è¿™é‡Œå¯ä»¥ä»æ–‡ä»¶ä¸­åŠ è½½ä½ çš„tokenizerè®¾ç½®ã€ä¹Ÿå¯ç›´æ¥èµ‹å€¼ã€æ³¨æ„ä½ çš„<code>ç‰¹æ®Šç¬¦å·å¿…é¡»é‡æ–°å®šä¹‰</code></p>
<h1 id="BPEç±»å‹çš„åˆ†è¯å™¨"><a href="#BPEç±»å‹çš„åˆ†è¯å™¨" class="headerlink" title="BPEç±»å‹çš„åˆ†è¯å™¨"></a>BPEç±»å‹çš„åˆ†è¯å™¨</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.BPE())</span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;s&quot;, (3, 5)), (&#x27;Ä test&#x27;, (5, 10)), (&#x27;Ä pre&#x27;, (10, 14)), (&#x27;-&#x27;, (14, 15)),</span></span><br><span class="line"><span class="string"> (&#x27;tokenization&#x27;, (15, 27)), (&#x27;!&#x27;, (27, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<p>GPT2åªéœ€è¦å¼€å§‹å’Œç»“æŸçš„ç‰¹æ®Štoken</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">trainer = trainers.BpeTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>])</span><br><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br><span class="line"></span><br><span class="line">tokenizer.model = models.BPE()</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br><span class="line"></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;L&#x27;, &#x27;et&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;Ä test&#x27;, &#x27;Ä this&#x27;, &#x27;Ä to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span></span><br><span class="line">encoding = tokenizer.encode(sentence)</span><br><span class="line">start, end = encoding.offsets[<span class="number">4</span>]</span><br><span class="line">sentence[start:end]</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27; test&#x27;</span></span><br></pre></td></tr></table></figure>

<p>The <code>trim_offsets = False</code> option indicates to the post-processor that we should leave the offsets of tokens that begin with â€˜Ä â€™ as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Letâ€™s have a look at the result with the text we just encoded, where <code>&#39;Ä test&#39;</code> is the token at index 4</p>
<ul>
<li><code>trim_offsets</code>è®¾å®šæ˜¯å¦ä¿®æ­£å­—ç¬¦çš„ç©ºæ ¼ä½ç½®è¿›å…¥åç§»é‡</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.ByteLevel()</span><br><span class="line">tokenizer.decode(encoding.ids)</span><br><span class="line"><span class="comment"># &quot;Let&#x27;s test this tokenizer.&quot;</span></span><br></pre></td></tr></table></figure>



<p>åŒ…è£…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æˆ–è€…</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2TokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>



<h1 id="Unigramç±»å‹çš„åˆ†è¯å™¨"><a href="#Unigramç±»å‹çš„åˆ†è¯å™¨" class="headerlink" title="Unigramç±»å‹çš„åˆ†è¯å™¨"></a>Unigramç±»å‹çš„åˆ†è¯å™¨</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.Unigram())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Regex</span><br><span class="line"></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [</span><br><span class="line">        normalizers.Replace(<span class="string">&quot;``&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>),</span><br><span class="line">        normalizers.Replace(<span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>),</span><br><span class="line">        normalizers.NFKD(),</span><br><span class="line">        normalizers.StripAccents(),</span><br><span class="line">        normalizers.Replace(Regex(<span class="string">&quot; &#123;2,&#125;&quot;</span>), <span class="string">&quot; &quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>ç¬¬ä¸€ã€äºŒä¸ªnormå°†ç¬¦å·æ›¿æ¢ï¼Œæœ€åä¸€ä¸ªå°†å¤šä¸ªç©ºæ ¼æ›¿æ¢æˆä¸€ä¸ª</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()</span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [(&quot;â–Let&#x27;s&quot;, (0, 5)), (&#x27;â–test&#x27;, (5, 10)), (&#x27;â–the&#x27;, (10, 14)), (&#x27;â–pre-tokenizer!&#x27;, (14, 29))]</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;&lt;cls&gt;&quot;</span>, <span class="string">&quot;&lt;sep&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;mask&gt;&quot;</span>, <span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>]</span><br><span class="line">trainer = trainers.UnigramTrainer(</span><br><span class="line">    vocab_size=<span class="number">25000</span>, special_tokens=special_tokens, unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æˆ–è€…</span></span><br><span class="line">tokenizer.model = models.Unigram()</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;â–Let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;â–test&#x27;, &#x27;â–this&#x27;, &#x27;â–to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;&lt;cls&gt;&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;&lt;sep&gt;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"><span class="comment"># 0 1</span></span><br><span class="line"></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,</span><br><span class="line">    pair=<span class="string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="string">&quot;on a pair of sentences!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="built_in">print</span>(encoding.type_ids)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;â–Let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;â–test&#x27;, &#x27;â–this&#x27;, &#x27;â–to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;, &#x27;.&#x27;, &#x27;.&#x27;, &#x27;&lt;sep&gt;&#x27;, &#x27;â–&#x27;, &#x27;on&#x27;, &#x27;â–&#x27;, &#x27;a&#x27;, &#x27;â–pair&#x27;, </span></span><br><span class="line"><span class="string">  &#x27;â–of&#x27;, &#x27;â–sentence&#x27;, &#x27;s&#x27;, &#x27;!&#x27;, &#x27;&lt;sep&gt;&#x27;, &#x27;&lt;cls&gt;&#x27;]</span></span><br><span class="line"><span class="string">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<p>XLnetå¡«å……padåœ¨å·¦è¾¹ä¸”[CLS]åœ¨æœ€åï¼Œè¿™äº›æˆ‘ä»¬éƒ½éœ€è¦æŒ‡æ˜ç»™Fast</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;s&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span><br><span class="line">    unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;&lt;pad&gt;&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;&lt;cls&gt;&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;&lt;sep&gt;&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;&lt;mask&gt;&quot;</span>,</span><br><span class="line">    padding_side=<span class="string">&quot;left&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>HF Course 09 Custom Tokenizer</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://poyone.github.io/posts/7194.html">https://poyone.github.io/posts/7194.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>ä½œè€…</h><div class="post-copyright-cc-info"><h>Poy One</h></div></div><div class="post-copyright-c"><h>å‘å¸ƒäº</h><div class="post-copyright-cc-info"><h>2022-12-12</h></div></div><div class="post-copyright-u"><h>æ›´æ–°äº</h><div class="post-copyright-cc-info"><h>2022-12-12</h></div></div><div class="post-copyright-c"><h>è®¸å¯åè®®</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Huggingface/">Huggingface</a></div><div class="post_share"><div class="social-share" data-image="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/7193.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">HF Course 08 Tokenizeråº•å±‚ç®—æ³•</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/posts/58033.html" title="NLP Baseline 01 ç¿»è¯‘"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka26.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-05</div><div class="title">NLP Baseline 01 ç¿»è¯‘</div></div></a></div><div><a href="/posts/16149.html" title="HF Course 01 åŸºç¡€æ¦‚å¿µ"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris40.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">HF Course 01 åŸºç¡€æ¦‚å¿µ</div></div></a></div><div><a href="/posts/64185.html" title="HF Course 02 APIæ¦‚è¦"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris40.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">HF Course 02 APIæ¦‚è¦</div></div></a></div><div><a href="/posts/18130.html" title="HF Course 03 å¾®è°ƒèŒƒå¼"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris40.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-06</div><div class="title">HF Course 03 å¾®è°ƒèŒƒå¼</div></div></a></div><div><a href="/posts/4498.html" title="HF Course 04 Dataset"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-11</div><div class="title">HF Course 04 Dataset</div></div></a></div><div><a href="/posts/5596.html" title="HF Course 05 faiss æœç´¢å¼•æ“"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-11</div><div class="title">HF Course 05 faiss æœç´¢å¼•æ“</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>ğŸ›´å‰å¾€github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%96%87%E6%A1%A3"><span class="toc-text">åŠ è½½æ–‡æ¡£</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%9E%84%E4%BB%B6"><span class="toc-text">åŠ è½½æ„ä»¶</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AENormalizer"><span class="toc-text">è®¾ç½®Normalizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pre-tokenization"><span class="toc-text">Pre-tokenization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Trainer"><span class="toc-text">Trainer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Post-processing"><span class="toc-text">Post-processing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder"><span class="toc-text">decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98-amp-%E5%8A%A0%E8%BD%BD-Custom-Tokenizer"><span class="toc-text">ä¿å­˜ &amp; åŠ è½½ Custom Tokenizer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BD%AC%E6%88%90Fast-Tokenizer"><span class="toc-text">è½¬æˆFast Tokenizer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BPE%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">BPEç±»å‹çš„åˆ†è¯å™¨</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Unigram%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">Unigramç±»å‹çš„åˆ†è¯å™¨</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è®¡ç®—æœºè§†è§‰</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è‡ªç„¶è¯­è¨€å¤„ç†</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">è®ºæ–‡ç²¾è¯»</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">æµç•…çš„Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">14</span><span class="categoryBar-list-descr">æ‹¥æœ‰ä¸€åˆ‡ å´å˜æˆå¤ªç©º</span></li></ul></div></div>';
      console.log('å·²æŒ‚è½½butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>