<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/11.17%E5%8F%B7%20%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/11.17%E5%8F%B7%20%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/</url>
      
        <content type="html"><![CDATA[<h1 id="typing库"><a href="#typing库" class="headerlink" title="typing库"></a>typing库</h1><p>主要是进行注解 List Tuple Optional </p><p>def aa( input0: str)  冒号后面为推荐的数据类型，使用Union 进行组合类型推荐 </p><p>如 input1 ：List(str)  推荐传入一个列表，列表内容为字符串</p><h1 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h1><p>使用 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  <span class="comment"># 也可以使用 tensorboardX</span></span><br><span class="line"><span class="comment"># from tensorboardX import SummaryWriter  # 也可以使用 pytorch 集成的 tensorboard</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;add_scalar/squared&#x27;</span>, np.square(epoch), epoch)</span><br><span class="line">    writer.add_scalars(<span class="string">&quot;add_scalars/trigonometric&quot;</span>, &#123;<span class="string">&#x27;xsinx&#x27;</span>: epoch * np.sin(epoch/<span class="number">5</span>), <span class="string">&#x27;xcosx&#x27;</span>: epoch* np.cos(epoch/<span class="number">5</span>), <span class="string">&#x27;xtanx&#x27;</span>: np.tan(epoch/<span class="number">5</span>)&#125;, epoch)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>&#x3D;&#x3D;writer.add_scalar&#x3D;&#x3D;</p><h1 id="PyTorch-修改保存模型"><a href="#PyTorch-修改保存模型" class="headerlink" title="PyTorch 修改保存模型"></a>PyTorch 修改保存模型</h1><p> 修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vgg.classifer[layer_num] = nn.linear(dim1,dim2)</span><br></pre></td></tr></table></figure><p> 保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vgg.save(vagg.state_dict(), <span class="string">&#x27;model_parameters.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><p>加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vgg.load_state_dict(torch.load(<span class="string">&#x27;model_parameters.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/08%20QA_v3/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/08%20QA_v3/</url>
      
        <content type="html"><![CDATA[<h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> fastprogress.fastprogress <span class="keyword">import</span> master_bar, progress_bar</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (AdamW, AutoModel, AutoModelForQuestionAnswering,</span><br><span class="line">                          AutoTokenizer, get_scheduler)</span><br></pre></td></tr></table></figure><h3 id="preprocesing"><a href="#preprocesing" class="headerlink" title="preprocesing"></a>preprocesing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">open_json</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = json.load(f)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">txt_json</span>(<span class="params">file_path</span>):</span><br><span class="line">    dt = open_json(file_path)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dt[<span class="string">&#x27;questions&#x27;</span>])):</span><br><span class="line">        pg_id = dt[<span class="string">&#x27;questions&#x27;</span>][i][<span class="string">&#x27;paragraph_id&#x27;</span>]</span><br><span class="line">        dt[<span class="string">&#x27;questions&#x27;</span>][i][<span class="string">&#x27;context&#x27;</span>] = dt[<span class="string">&#x27;paragraphs&#x27;</span>][pg_id]</span><br><span class="line">    <span class="keyword">return</span> dt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_json</span>(<span class="params">file_path, save_name</span>):</span><br><span class="line">    info = txt_json(file_path)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_name, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(info, f)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    questions = [q.strip() <span class="keyword">for</span> q <span class="keyword">in</span> examples[<span class="string">&quot;question_text&quot;</span>]]</span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        questions,</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">        max_length=<span class="number">384</span>,</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">        stride = <span class="number">128</span>,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line">    sample_map = inputs.pop(<span class="string">&#x27;overflow_to_sample_mapping&#x27;</span>)</span><br><span class="line">    start_positions = []</span><br><span class="line">    end_positions = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, offset <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset_mapping):</span><br><span class="line">        sample_idx = sample_map[i]</span><br><span class="line">        start_char = examples[<span class="string">&quot;answer_start&quot;</span>][sample_idx]</span><br><span class="line">        end_char = examples[<span class="string">&quot;answer_start&quot;</span>][sample_idx]+ <span class="built_in">len</span>(examples[<span class="string">&quot;answer_text&quot;</span>][sample_idx])+<span class="number">1</span></span><br><span class="line">        sequence_ids = inputs.sequence_ids(i)</span><br><span class="line"></span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> sequence_ids[idx] != <span class="number">1</span>:</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        context_start = idx</span><br><span class="line">        <span class="keyword">while</span> sequence_ids[idx] == <span class="number">1</span>:</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        context_end = idx - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> offset[context_start][<span class="number">0</span>] &gt; end_char <span class="keyword">or</span> offset[context_end][<span class="number">1</span>] &lt; start_char:</span><br><span class="line">            start_positions.append(<span class="number">0</span>)</span><br><span class="line">            end_positions.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idx = context_start</span><br><span class="line">            <span class="keyword">while</span> idx &lt;= context_end <span class="keyword">and</span> offset[idx][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line">            start_positions.append(idx - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            idx = context_end</span><br><span class="line">            <span class="keyword">while</span> idx &gt;= context_start <span class="keyword">and</span> offset[idx][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">                idx -= <span class="number">1</span></span><br><span class="line">            end_positions.append(idx + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    inputs[<span class="string">&quot;start_positions&quot;</span>] = start_positions</span><br><span class="line">    inputs[<span class="string">&quot;end_positions&quot;</span>] = end_positions</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deal_dataset</span>(<span class="params">file_path</span>):</span><br><span class="line">    data_type = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=file_path, field=<span class="string">&#x27;questions&#x27;</span>)</span><br><span class="line">    dataset = data_type.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>, remove_columns=data_type[<span class="string">&quot;train&quot;</span>].column_names)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">mydataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        self.data = data[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        input_ids = torch.tensor(self.data[idx][<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">        token_type_ids = torch.tensor(self.data[idx][<span class="string">&#x27;token_type_ids&#x27;</span>])</span><br><span class="line">        attention_mask = torch.tensor(self.data[idx][<span class="string">&#x27;attention_mask&#x27;</span>])</span><br><span class="line">        start_positions = torch.tensor(self.data[idx][<span class="string">&#x27;start_positions&#x27;</span>])</span><br><span class="line">        end_positions = torch.tensor(self.data[idx][<span class="string">&#x27;end_positions&#x27;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;input_ids&#x27;</span>:input_ids, <span class="string">&#x27;token_type_ids&#x27;</span>:token_type_ids,</span><br><span class="line">                <span class="string">&#x27;attention_mask&#x27;</span>:attention_mask, <span class="string">&#x27;start_positions&#x27;</span>:start_positions,</span><br><span class="line">                <span class="string">&#x27;end_positions&#x27;</span>:end_positions&#125;</span><br></pre></td></tr></table></figure><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">net, device, opti, lr, lr_scheduler, batch_size, train_loader, val_loader, epochs, model_name</span>):</span><br><span class="line">    </span><br><span class="line">    best_ep = <span class="number">1</span></span><br><span class="line">    best_loss = <span class="number">4.20051</span></span><br><span class="line">    train_loss, valid_loss = [], []</span><br><span class="line">    mb = master_bar(<span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> mb:</span><br><span class="line">        train_ls, valid_ls = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="comment"># train_part</span></span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">with</span> autocast():</span><br><span class="line">            <span class="keyword">for</span> batch_data <span class="keyword">in</span> progress_bar(train_loader, parent=mb):</span><br><span class="line">                </span><br><span class="line">                batch_data = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch_data.items()&#125;</span><br><span class="line">                outputs = net(**batch_data)</span><br><span class="line">                loss = outputs.loss</span><br><span class="line">                train_ls += loss.item()</span><br><span class="line">                </span><br><span class="line">                opti.zero_grad()</span><br><span class="line">                loss.backward()</span><br><span class="line">                lr_scheduler.step()</span><br><span class="line">                opti.step()</span><br><span class="line">                </span><br><span class="line">            train_loss.append(train_ls/(<span class="built_in">len</span>(train_loader)))</span><br><span class="line">            </span><br><span class="line">                </span><br><span class="line">        <span class="comment"># valid_part</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> progress_bar(val_loader, parent=mb):</span><br><span class="line">            </span><br><span class="line">                batch_data = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch_data.items()&#125;</span><br><span class="line">                outputs = net(**batch_data)</span><br><span class="line">                </span><br><span class="line">                loss = outputs.loss</span><br><span class="line">                valid_ls += loss.item()</span><br><span class="line">                </span><br><span class="line">            valid_loss.append(valid_ls/(<span class="built_in">len</span>(val_loader)))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># plot</span></span><br><span class="line"><span class="comment">#         print(train_loss, &#x27;\n&#x27;, valid_loss) # 经常出错，不知道为什么</span></span><br><span class="line">        plot_loss_update(epoch, mb, train_loss, valid_loss)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># save</span></span><br><span class="line">        valid_loss_now = valid_loss[-<span class="number">1</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> complete! Validation Loss : <span class="subst">&#123;valid_loss_now:<span class="number">.5</span>f&#125;</span>&quot;</span>, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_loss_now &lt; best_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Best validation loss improved from <span class="subst">&#123;best_loss:<span class="number">.5</span>f&#125;</span> to <span class="subst">&#123;valid_loss_now:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            net_copy = copy.deepcopy(net)</span><br><span class="line">            best_loss = valid_loss_now</span><br><span class="line">            best_ep = epoch + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            path_to_model = <span class="string">f&#x27;caofei_model/<span class="subst">&#123;model_name&#125;</span>_lr_<span class="subst">&#123;lr&#125;</span>_val_loss_<span class="subst">&#123;best_loss:<span class="number">.5</span>f&#125;</span>_epoch_<span class="subst">&#123;best_ep&#125;</span>.pt&#x27;</span></span><br><span class="line">            torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;The model has been saved in <span class="subst">&#123;path_to_model&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_loss_update</span>(<span class="params">epoch, mb, train_loss, valid_loss</span>):</span><br><span class="line"></span><br><span class="line">    x = <span class="built_in">range</span>(<span class="number">1</span>, epoch+<span class="number">1</span>)</span><br><span class="line">    y = np.concatenate((train_loss, valid_loss))</span><br><span class="line">    graphs = [[x,train_loss], [x,valid_loss]]</span><br><span class="line">    x_margin = <span class="number">0.2</span></span><br><span class="line">    y_margin = <span class="number">0.05</span></span><br><span class="line">    x_bounds = [<span class="number">1</span>-x_margin, epochs+x_margin]</span><br><span class="line">    y_bounds = [np.<span class="built_in">min</span>(y)-y_margin, np.<span class="built_in">max</span>(y)+y_margin]</span><br><span class="line"></span><br><span class="line">    mb.update_graph(graphs, x_bounds, y_bounds)</span><br></pre></td></tr></table></figure><h3 id="调用"><a href="#调用" class="headerlink" title="调用"></a>调用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 路径设置</span></span><br><span class="line">!mkdir caofei_model  </span><br><span class="line"><span class="comment"># !mkdir results</span></span><br><span class="line"></span><br><span class="line">train_path = <span class="string">&#x27;../input/ml2022spring-hw7/hw7_train.json&#x27;</span></span><br><span class="line">dev_path = <span class="string">&#x27;../input/ml2022spring-hw7/hw7_dev.json&#x27;</span></span><br><span class="line">test_path = <span class="string">&#x27;../input/ml2022spring-hw7/hw7_test.json&#x27;</span></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&#x27;hfl/chinese-bert-wwm-ext&#x27;</span></span><br><span class="line"></span><br><span class="line">json_train_path = <span class="string">&#x27;./qa_dataset.json&#x27;</span></span><br><span class="line">json_dev_path = <span class="string">&#x27;./dev_dataset.json&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练超参数</span></span><br><span class="line">seed = <span class="number">1222</span></span><br><span class="line">bs = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">2e-5</span></span><br><span class="line">epochs = <span class="number">20</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">save_json(train_path, json_train_path)</span><br><span class="line">save_json(dev_path, json_dev_path)</span><br><span class="line"></span><br><span class="line">train_json_dataset = deal_dataset(json_train_path)</span><br><span class="line">dev_json_dataset = deal_dataset(json_dev_path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;TOKENIZERS_PARALLELISM&quot;</span>] = <span class="string">&quot;false&quot;</span></span><br><span class="line"></span><br><span class="line">train_dataset = mydataset(train_json_dataset)</span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=bs, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">dev_dataset = mydataset(dev_json_dataset)</span><br><span class="line">dev_dataloader = DataLoader(dev_dataset, batch_size=bs, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">set_seed(seed)</span><br><span class="line"></span><br><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(model_name)</span><br><span class="line"><span class="comment"># model.load_state_dict(torch.load(&#x27;final_model/chineseQA_model_lr_2e-05_val_loss_4.20051_epoch_21.pt&#x27;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># len(list(model.parameters()))</span></span><br><span class="line"><span class="keyword">for</span> idx, para <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.parameters()):</span><br><span class="line">    para.requires_grad = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">195</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">opti = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=opti,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=epochs*<span class="built_in">len</span>(train_dataloader),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">train_bert(net=model, device=device, opti=opti, lr=lr, l</span><br><span class="line">           r_scheduler=lr_scheduler, batch_size=bs, train_loader=dev_dataloader, </span><br><span class="line">           val_loader=dev_dataloader, epochs=epochs, model_name=<span class="string">&#x27;chineseQA_model&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/08%20QA_v2/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/08%20QA_v2/</url>
      
        <content type="html"><![CDATA[<h1 id="答案偏移"><a href="#答案偏移" class="headerlink" title="答案偏移"></a>答案偏移</h1><p>加入问题之后，答案的偏移</p><h1 id="DataLoader-坐标"><a href="#DataLoader-坐标" class="headerlink" title="DataLoader 坐标"></a>DataLoader 坐标</h1><p>data[‘train’] [idx] [‘input_ids’]</p><p>data[‘train’] [idx] [‘token_type_ids’]</p><p>data[‘train’] [idx] [‘attention_mask’]</p><p>…</p><h1 id="Model-输入"><a href="#Model-输入" class="headerlink" title="Model 输入"></a>Model 输入</h1><p>只能是{‘input_ids’:sample[0], ‘token_type_ids’:sample[1], ‘attention_mask’:sample[2], ‘start_positions’:sample[3], ‘end_positions’:sample[4]}</p><p>进行字典解包 model(**inputs)</p><h1 id="分布训练"><a href="#分布训练" class="headerlink" title="分布训练"></a>分布训练</h1><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><h2 id="1-导包"><a href="#1-导包" class="headerlink" title="1.导包"></a>1.导包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForQuestionAnswering</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br></pre></td></tr></table></figure><h2 id="2-文件路径"><a href="#2-文件路径" class="headerlink" title="2.文件路径"></a>2.文件路径</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_path = <span class="string">&#x27;../input/ml2022spring-hw7/hw7_train.json&#x27;</span></span><br><span class="line">dev_path = <span class="string">&#x27;../input/ml2022spring-hw7/hw7_dev.json&#x27;</span></span><br><span class="line">test_path = <span class="string">&#x27;../input/ml2022spring-hw7/hw7_test.json&#x27;</span></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&#x27;hfl/chinese-bert-wwm-ext&#x27;</span></span><br><span class="line"></span><br><span class="line">!mkdir models  <span class="comment">#可以在之前补充绝对路径</span></span><br><span class="line">!mkdir results</span><br></pre></td></tr></table></figure><h2 id="3-固定种子"><a href="#3-固定种子" class="headerlink" title="3.固定种子"></a>3.固定种子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 固定随机种子，保证结果复现</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">set_seed(<span class="number">1222</span>)</span><br></pre></td></tr></table></figure><h2 id="4-预处理"><a href="#4-预处理" class="headerlink" title="4.预处理"></a>4.预处理</h2><h3 id="4-1转换json格式"><a href="#4-1转换json格式" class="headerlink" title="4.1转换json格式"></a>4.1转换json格式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">open_json</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = json.load(f)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">txt_json</span>(<span class="params">file_path</span>):</span><br><span class="line">    dt = open_json(file_path)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dt[<span class="string">&#x27;questions&#x27;</span>])):</span><br><span class="line">        pg_id = dt[<span class="string">&#x27;questions&#x27;</span>][i][<span class="string">&#x27;paragraph_id&#x27;</span>]</span><br><span class="line">        dt[<span class="string">&#x27;questions&#x27;</span>][i][<span class="string">&#x27;context&#x27;</span>] = dt[<span class="string">&#x27;paragraphs&#x27;</span>][pg_id]</span><br><span class="line">    <span class="keyword">return</span> dt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_json</span>(<span class="params">file_path, save_name</span>):</span><br><span class="line">    info = txt_json(file_path)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_name, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(info, f)</span><br><span class="line">        </span><br><span class="line">save_json(train_path, <span class="string">&#x27;qa_dataset.json&#x27;</span>)</span><br><span class="line">save_json(dev_path, <span class="string">&#x27;dev_dataset.json&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="x3D-x3D-4-2处理答案偏移-x3D-x3D"><a href="#x3D-x3D-4-2处理答案偏移-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;4.2处理答案偏移&#x3D;&#x3D;"></a>&#x3D;&#x3D;4.2处理答案偏移&#x3D;&#x3D;</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    questions = [q.strip() <span class="keyword">for</span> q <span class="keyword">in</span> examples[<span class="string">&quot;question_text&quot;</span>]]</span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        questions,</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">        max_length=<span class="number">384</span>,</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">        stride = <span class="number">128</span>,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line">    sample_map = inputs.pop(<span class="string">&#x27;overflow_to_sample_mapping&#x27;</span>)</span><br><span class="line">    start_positions = []</span><br><span class="line">    end_positions = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, offset <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset_mapping):</span><br><span class="line">        sample_idx = sample_map[i]</span><br><span class="line">        start_char = examples[<span class="string">&quot;answer_start&quot;</span>][sample_idx]</span><br><span class="line">        end_char = examples[<span class="string">&quot;answer_start&quot;</span>][sample_idx]+ <span class="built_in">len</span>(examples[<span class="string">&quot;answer_text&quot;</span>][sample_idx])+<span class="number">1</span></span><br><span class="line">        sequence_ids = inputs.sequence_ids(i)</span><br><span class="line"></span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> sequence_ids[idx] != <span class="number">1</span>:</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        context_start = idx</span><br><span class="line">        <span class="keyword">while</span> sequence_ids[idx] == <span class="number">1</span>:</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        context_end = idx - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> offset[context_start][<span class="number">0</span>] &gt; end_char <span class="keyword">or</span> offset[context_end][<span class="number">1</span>] &lt; start_char:</span><br><span class="line">            start_positions.append(<span class="number">0</span>)</span><br><span class="line">            end_positions.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idx = context_start</span><br><span class="line">            <span class="keyword">while</span> idx &lt;= context_end <span class="keyword">and</span> offset[idx][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line">            start_positions.append(idx - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            idx = context_end</span><br><span class="line">            <span class="keyword">while</span> idx &gt;= context_start <span class="keyword">and</span> offset[idx][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">                idx -= <span class="number">1</span></span><br><span class="line">            end_positions.append(idx + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    inputs[<span class="string">&quot;start_positions&quot;</span>] = start_positions</span><br><span class="line">    inputs[<span class="string">&quot;end_positions&quot;</span>] = end_positions</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deal_dataset</span>(<span class="params">file_path</span>):</span><br><span class="line">    data_type = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=file_path, field=<span class="string">&#x27;questions&#x27;</span>)</span><br><span class="line">    dataset = data_type.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>, remove_columns=data_type[<span class="string">&quot;train&quot;</span>].column_names)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure><h2 id="5-加载数据"><a href="#5-加载数据" class="headerlink" title="5.加载数据"></a>5.加载数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">json_train_path = <span class="string">&#x27;./qa_dataset.json&#x27;</span></span><br><span class="line">json_dev_path = <span class="string">&#x27;./dev_dataset.json&#x27;</span></span><br><span class="line"></span><br><span class="line">train_json_dataset = deal_dataset(json_train_path)</span><br><span class="line">dev_json_dataset = deal_dataset(json_dev_path)</span><br></pre></td></tr></table></figure><h2 id="6-Dataset-DataLoader-Model-三兄弟"><a href="#6-Dataset-DataLoader-Model-三兄弟" class="headerlink" title="6.Dataset DataLoader Model 三兄弟"></a>6.Dataset DataLoader Model 三兄弟</h2><h3 id="6-1dataset-amp-dataloader"><a href="#6-1dataset-amp-dataloader" class="headerlink" title="6.1dataset &amp; dataloader"></a>6.1dataset &amp; dataloader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">mydataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        self.data = data[<span class="string">&#x27;train&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        input_ids = torch.tensor(self.data[idx][<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">        token_type_ids = torch.tensor(self.data[idx][<span class="string">&#x27;token_type_ids&#x27;</span>])</span><br><span class="line">        attention_mask = torch.tensor(self.data[idx][<span class="string">&#x27;attention_mask&#x27;</span>])</span><br><span class="line">        start_positions = torch.tensor(self.data[idx][<span class="string">&#x27;start_positions&#x27;</span>])</span><br><span class="line">        end_positions = torch.tensor(self.data[idx][<span class="string">&#x27;end_positions&#x27;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> input_ids, token_type_ids, attention_mask, start_positions, end_positions</span><br><span class="line">    </span><br><span class="line">train_dataset = mydataset(train_json_dataset)</span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">dev_dataset = mydataset(dev_json_dataset)</span><br><span class="line">dev_dataloader = DataLoader(dev_dataset, batch_size=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="6-2model"><a href="#6-2model" class="headerlink" title="6.2model"></a>6.2model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(model_name)</span><br></pre></td></tr></table></figure><h2 id="7-训练"><a href="#7-训练" class="headerlink" title="7.训练"></a>7.训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">net, device, opti, lr, train_loader, val_loader, epochs, iters_to_accumulate, bert_model</span>):</span><br><span class="line"></span><br><span class="line">    best_loss = np.Inf</span><br><span class="line">    best_ep = <span class="number">1</span></span><br><span class="line">    nb_iterations = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    print_every = nb_iterations // <span class="number">5</span>  <span class="comment"># 打印频率</span></span><br><span class="line">    iters = []</span><br><span class="line">    train_losses = []</span><br><span class="line">    val_losses = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line"></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> it, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_loader)):</span><br><span class="line">            <span class="keyword">with</span> autocast():</span><br><span class="line">                <span class="comment">#这里的解包，可以换成元组或者字典</span></span><br><span class="line">                input_ids, token_type_ids, attention_mask, start_positions, end_positions = batch</span><br><span class="line">                input_ids, token_type_ids, attention_mask, start_positions, end_positions = input_ids.to(device), token_type_ids.to(device), attention_mask.to(device), start_positions.to(device), end_positions.to(device)</span><br><span class="line">                </span><br><span class="line">                outputs = net(**&#123;<span class="string">&#x27;input_ids&#x27;</span>:input_ids, <span class="string">&#x27;token_type_ids&#x27;</span>:token_type_ids, <span class="string">&#x27;attention_mask&#x27;</span>:attention_mask, <span class="string">&#x27;start_positions&#x27;</span>:start_positions, <span class="string">&#x27;end_positions&#x27;</span>:end_positions&#125;)</span><br><span class="line">                </span><br><span class="line">                loss = outputs.loss</span><br><span class="line">                loss.backward()</span><br><span class="line">                opti.zero_grad()</span><br><span class="line">                opti.step()</span><br><span class="line">                </span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % print_every == <span class="number">0</span>: </span><br><span class="line">                <span class="built_in">print</span>()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;it+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;nb_iterations&#125;</span> of epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete. \</span></span><br><span class="line"><span class="string">                Loss : <span class="subst">&#123;running_loss / print_every&#125;</span> &quot;</span>)</span><br><span class="line"></span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        val_loss = evaluate_loss(net, device, val_loader)  <span class="comment"># Compute validation loss</span></span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete! Validation Loss : <span class="subst">&#123;val_loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;</span>.<span class="built_in">format</span>(best_loss, val_loss))</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">            net_copy = copy.deepcopy(net)  </span><br><span class="line">            best_loss = val_loss</span><br><span class="line">            best_ep = ep + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    path_to_model=<span class="string">f&#x27;models/<span class="subst">&#123;bert_model&#125;</span>_lr_<span class="subst">&#123;lr&#125;</span>_val_loss_<span class="subst">&#123;<span class="built_in">round</span>(best_loss, <span class="number">5</span>)&#125;</span>_ep_<span class="subst">&#123;best_ep&#125;</span>.pt&#x27;</span></span><br><span class="line">    torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model has been saved in &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_model))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> loss</span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, device, dataloader</span>):</span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    mean_loss = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> it, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(dataloader)):</span><br><span class="line">            input_ids, token_type_ids, attention_mask, start_positions, end_positions = batch</span><br><span class="line">            input_ids, token_type_ids, attention_mask, start_positions, end_positions = input_ids.to(device), token_type_ids.to(device), attention_mask.to(device), start_positions.to(device), end_positions.to(device)</span><br><span class="line">            </span><br><span class="line">            outputs = net(**&#123;<span class="string">&#x27;input_ids&#x27;</span>:input_ids, <span class="string">&#x27;token_type_ids&#x27;</span>:token_type_ids, <span class="string">&#x27;attention_mask&#x27;</span>:attention_mask, <span class="string">&#x27;start_positions&#x27;</span>:start_positions, <span class="string">&#x27;end_positions&#x27;</span>:end_positions&#125;)</span><br><span class="line">            loss = outputs.loss</span><br><span class="line">            mean_loss += loss.item()</span><br><span class="line">            </span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mean_loss / count</span><br></pre></td></tr></table></figure><h2 id="8-超参数-amp-分布式训练"><a href="#8-超参数-amp-分布式训练" class="headerlink" title="8.超参数 &amp; 分布式训练"></a>8.超参数 &amp; 分布式训练</h2><p>&#x3D;&#x3D;nn.DataParallel(model)方式已经过时&#x3D;&#x3D;，现在使用<code>nn.parallel.DistributedDataParallel</code>的API进行处理</p><p>DistributedDataParallel主要通过三个函数布置且需要安装NVIDIA的apex库</p><ul><li>使用argparse布置通道、进程编号、指定的GPU等</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-n&#x27;</span>, <span class="string">&#x27;--nodes&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, metavar=<span class="string">&#x27;N&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-g&#x27;</span>, <span class="string">&#x27;--gpus&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of gpus per node&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-nr&#x27;</span>, <span class="string">&#x27;--nr&#x27;</span>, default=<span class="number">0</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;ranking within the nodes&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, default=<span class="number">2</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of total epochs to run&#x27;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 以下为布置的端口以及环境初始化和GPU</span></span><br><span class="line">args.world_size = args.gpus * args.nodes                </span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="string">&#x27;10.57.23.164&#x27;</span>             </span><br><span class="line">    os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>] = <span class="string">&#x27;8888&#x27;</span>                      </span><br><span class="line">    mp.spawn(train, nprocs=args.gpus, args=(args,)) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> apex <span class="keyword">import</span> amp</span><br></pre></td></tr></table></figure><ul><li>使用nn.parallel.DistributedDataParallel包装 model 即可使模型分布在GPU上但还需要将数据分布式处理</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from apex.parallel import DistributedDataParallel as DDP</span><br><span class="line"></span><br><span class="line">model = nn.parallel.DistributedDataParallel(model,device_ids=[gpu])</span><br></pre></td></tr></table></figure><ul><li>将数据使用DistributedSampler分布式采样，使同一个批次不会重复处理，最后载入DataLoader</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,</span><br><span class="line">                                                                num_replicas=args.world_size,</span><br><span class="line">                                                                rank=rank)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,</span><br><span class="line">                                           shuffle=<span class="literal">False</span>,num_workers=<span class="number">0</span>,pin_memory=<span class="literal">True</span>,</span><br><span class="line">         sampler=train_sampler) </span><br></pre></td></tr></table></figure><h3 id="8-1超参数"><a href="#8-1超参数" class="headerlink" title="8.1超参数"></a>8.1超参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">2e-5</span></span><br><span class="line">epochs = <span class="number">2</span> </span><br><span class="line">iters_to_accumulate = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">save_model_name = <span class="string">&#x27;WH_chineseQA_model&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="8-2-分布式"><a href="#8-2-分布式" class="headerlink" title="8.2 分布式"></a>8.2 分布式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">opti = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/08%20QA_v1/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/08%20QA_v1/</url>
      
        <content type="html"><![CDATA[<h1 id="API-amp-trick"><a href="#API-amp-trick" class="headerlink" title="API &amp; trick"></a>API &amp; trick</h1><h2 id="tokenizer中的"><a href="#tokenizer中的" class="headerlink" title="tokenizer中的"></a>tokenizer中的</h2><ul><li><p>return_offsets_mapping  返回一个元组如( 0 , 1 ) 由于[CLS]的加入第一个字符的位置从0 边为 1</p><ul><li>即(x , y)  x表示元素在这句截断的话中的</li></ul><p></p></li><li><p>return_overflowing_tokens  #似乎没什么必要</p><ul><li>在设定了stride才有用</li><li>由于滑窗将句子分割，此参数为True之后，将标记每个部分属于哪个序号<ul><li>如长为 500的句子， stride &#x3D; 50， max_length&#x3D;200,</li><li>分组为【0,199】【149,349】【300,499】</li><li>设置参数后返回一个 [0，0，0] 表示三个列表属于0号段落</li></ul></li></ul></li><li><p>truncation ‘only second’ 表示 传入tonkinzer的一对句子( question， txt) 对第二个 (txt) 截断 question就不截断</p></li><li><p>inputs.sequence_ids(2) 表示 return_overflowing_tokens 后 input_ids的第三个列表</p></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h2><ol><li><p>return_overflowing_tokens 返回的是这种，使得截断后滑动窗口，返回多个数组，不滑动就没用了</p><p>且设定的每组都是【question，strided_context】</p><p>&#x3D;&#x3D;这是问题(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 0)&#x3D;&#x3D; 每个分组都有吧</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (0, 0)], [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 0), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (0, 0)], </span><br></pre></td></tr></table></figure><ol start="2"><li><p>return_offsets_mapping 返回的数组如上图所示，[CLS]的加入使得每次断句滑动之后起始下标都会加一，</p><p>而分组内的下标属于继承制。</p><p> (0, 0), (15, 16), (16, 17), 如同这个，这里stride&#x3D;5 ，在第一句(6, 7), (0, 0), (0, 1),。。。。 (18, 19), (19, 20), (0, 0)这里结束</p><p>倒数五个数，拿到(15, 16), 即从这里开始</p></li><li><p>inputs.sequence_ids() 括号内设定为 input_ids的分组的编号</p><p>inputs.sequence_ids(0) 如下</p><p>&#x3D;&#x3D;None 0 0 0 0 0 0 0 None 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 None&#x3D;&#x3D;</p><p>[0,0] 代表【CLS】会被直接设成none， 将question和context 以0,1分离表示，方便下面处理</p><p>[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (0, 0)]</p></li></ol><p>​</p><h2 id="官网套路"><a href="#官网套路" class="headerlink" title="官网套路"></a>官网套路</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    questions = [q.strip() <span class="keyword">for</span> q <span class="keyword">in</span> examples[<span class="string">&quot;question&quot;</span>]]</span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        questions,</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">        max_length=<span class="number">384</span>,</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line">    answers = examples[<span class="string">&quot;answers&quot;</span>]</span><br><span class="line">    start_positions = []</span><br><span class="line">    end_positions = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, offset <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset_mapping):</span><br><span class="line">        answer = answers[i]</span><br><span class="line">        start_char = answer[<span class="string">&quot;answer_start&quot;</span>][<span class="number">0</span>]</span><br><span class="line">        end_char = answer[<span class="string">&quot;answer_start&quot;</span>][<span class="number">0</span>] + <span class="built_in">len</span>(answer[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])</span><br><span class="line">        sequence_ids = inputs.sequence_ids(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Find the start and end of the context</span></span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> sequence_ids[idx] != <span class="number">1</span>:</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        context_start = idx</span><br><span class="line">        <span class="keyword">while</span> sequence_ids[idx] == <span class="number">1</span>:</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        context_end = idx - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the answer is not fully inside the context, label it (0, 0)</span></span><br><span class="line">        <span class="keyword">if</span> offset[context_start][<span class="number">0</span>] &gt; end_char <span class="keyword">or</span> offset[context_end][<span class="number">1</span>] &lt; start_char:</span><br><span class="line">            start_positions.append(<span class="number">0</span>)</span><br><span class="line">            end_positions.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Otherwise it&#x27;s the start and end token positions</span></span><br><span class="line">            idx = context_start</span><br><span class="line">            <span class="keyword">while</span> idx &lt;= context_end <span class="keyword">and</span> offset[idx][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line">            start_positions.append(idx - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            idx = context_end</span><br><span class="line">            <span class="keyword">while</span> idx &gt;= context_start <span class="keyword">and</span> offset[idx][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">                idx -= <span class="number">1</span></span><br><span class="line">            end_positions.append(idx + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    inputs[<span class="string">&quot;start_positions&quot;</span>] = start_positions</span><br><span class="line">    inputs[<span class="string">&quot;end_positions&quot;</span>] = end_positions</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">tokenized_squad = squad.<span class="built_in">map</span>(preprocess_function,batched=<span class="literal">True</span>,remove_columns=squad[<span class="string">&quot;train&quot;</span>].column_names)</span><br></pre></td></tr></table></figure><p>offset内是本批次的字符排序，包含了CLS等特殊符的占位，</p><p>这种：(6, 7), (0, 0), (15, 16), (16, 17), (17, 18),</p><p>想把answer的原始位置映射，如原始answer【515,519】</p><p>首先offset是个列表idx表示坐标，由第一个while找到（x，y）中x大于515的第一个 那个idx就是 answer的起始</p><p>​找到第一个小于 y的那个idx 对应的就是 answer的结束了。</p><p>每次处理的是一个问答格式(json)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">squad[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;answers&#x27;</span>: &#123;<span class="string">&#x27;answer_start&#x27;</span>: [<span class="number">515</span>], <span class="string">&#x27;text&#x27;</span>: [<span class="string">&#x27;Saint Bernadette Soubirous&#x27;</span>]&#125;,</span><br><span class="line"> <span class="string">&#x27;context&#x27;</span>: <span class="string">&#x27;Architecturally, the school has a Catholic character. Atop the Main Building\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;5733be284776f41900661182&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;University_of_Notre_Dame&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真不容易啊这波，加油</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>以下为零号样本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;TRAIN_186_QUERY_0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;范廷颂&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;context&#x27;</span>: <span class="string">&#x27;范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;范廷颂是什么时候被任为主教的？&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;answers&#x27;</span>: &#123;<span class="string">&#x27;text&#x27;</span>: [<span class="string">&#x27;1963年&#x27;</span>], <span class="string">&#x27;answer_start&#x27;</span>: [<span class="number">30</span>]&#125;&#125;</span><br></pre></td></tr></table></figure><p>对其编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">context = train_data[<span class="number">0</span>][<span class="string">&quot;context&quot;</span>] <span class="comment"># </span></span><br><span class="line">question = train_data[<span class="number">0</span>][<span class="string">&quot;question&quot;</span>]</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(</span><br><span class="line">    question,</span><br><span class="line">    context,</span><br><span class="line">    max_length=<span class="number">300</span>,<span class="comment"># 最大长度</span></span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,<span class="comment"># 仅对第二个输入进行截断</span></span><br><span class="line">    stride=<span class="number">50</span>,<span class="comment"># 滑动窗口大小为50</span></span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,<span class="comment">#设定分词器支持返回重叠 token。</span></span><br><span class="line">    return_offsets_mapping=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>结果如下，input_ids、token_type_ids、attention_mask就省略了</p><p>滑动窗口50，最大长度300，对814的样本做四个分割，（0,299）（250,549）（500,799）（749,815）四份</p><p>offset记录的就是一个字符【‘汗’】对应的token_id, 它可能中间有【SEP】等符号，所以有些offset不只是1-2，可能是1-4等跨度</p><p>可以看到每一份都是【question+context[i]】的组合，前面都是问题，后面是文章</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;offset_mapping&#x27;: [[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 105), (105, 106), (106, 107), (107, 108), (108, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 119), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (133, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 192), (192, 193), (193, 194), (194, 195), (195, 196), (196, 197), (197, 198), (198, 199), (199, 200), (200, 201), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 207), (207, 208), (208, 209), (209, 210), (210, 211), (211, 212), (212, 213), (213, 214), (214, 215), (215, 216), (216, 217), (217, 218), (218, 222), (222, 223), (223, 224), (224, 225), (225, 226), (226, 227), (227, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 243), (243, 244), (244, 245), (245, 246), (246, 247), (247, 248), (248, 249), (249, 250), (250, 251), (251, 252), (252, 253), (253, 257), (257, 258), (258, 259), (259, 260), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (0, 0)], </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (332, 333), (333, 334), (334, 335), (335, 336), (336, 337), (337, 338), (338, 339), (339, 340), (340, 341), (341, 342), (342, 343), (343, 344), (344, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 360), (360, 361), (361, 362), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 368), (368, 369), (369, 370), (370, 371), (371, 372), (372, 373), (373, 374), (374, 375), (375, 376), (376, 377), (377, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 385), (385, 386), (386, 387), (387, 388), (388, 390), (390, 391), (391, 392), (392, 393), (393, 394), (394, 395), (395, 396), (396, 397), (397, 398), (398, 399), (399, 400), (400, 401), (401, 402), (402, 403), (403, 404), (404, 405), (405, 406), (406, 407), (407, 408), (408, 409), (409, 410), (410, 411), (411, 412), (412, 413), (413, 414), (414, 415), (415, 416), (416, 417), (417, 418), (418, 419), (419, 420), (420, 421), (421, 422), (422, 424), (424, 425), (425, 426), (426, 427), (427, 428), (428, 429), (429, 430), (430, 431), (431, 432), (432, 433), (433, 434), (434, 435), (435, 436), (436, 437), (437, 438), (438, 439), (439, 440), (440, 441), (441, 442), (442, 443), (443, 444), (444, 445), (445, 446), (446, 447), (447, 448), (448, 449), (449, 450), (450, 451), (451, 452), (452, 453), (453, 454), (454, 455), (455, 456), (456, 457), (457, 458), (458, 459), (459, 460), (460, 461), (461, 462), (462, 463), (463, 464), (464, 465), (465, 466), (466, 467), (467, 468), (468, 469), (469, 470), (470, 471), (471, 472), (472, 473), (473, 474), (474, 475), (475, 476), (476, 477), (477, 478), (478, 479), (479, 480), (480, 481), (481, 482), (482, 483), (483, 484), (484, 485), (485, 486), (486, 487), (487, 488), (488, 489), (489, 490), (490, 491), (491, 492), (492, 493), (493, 494), (494, 495), (495, 499), (499, 500), (500, 501), (501, 502), (502, 503), (503, 504), (504, 505), (505, 506), (506, 507), (507, 508), (508, 509), (509, 510), (510, 511), (511, 512), (512, 513), (513, 514), (514, 516), (516, 517), (517, 518), (518, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 532), (532, 533), (533, 534), (534, 535), (535, 536), (536, 537), (537, 538), (538, 539), (539, 540), (540, 541), (541, 542), (542, 543), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 552), (552, 553), (553, 554), (554, 555), (555, 557), (557, 558), (0, 0)],</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (503, 504), (504, 505), (505, 506), (506, 507), (507, 508), (508, 509), (509, 510), (510, 511), (511, 512), (512, 513), (513, 514), (514, 516), (516, 517), (517, 518), (518, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 532), (532, 533), (533, 534), (534, 535), (535, 536), (536, 537), (537, 538), (538, 539), (539, 540), (540, 541), (541, 542), (542, 543), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 552), (552, 553), (553, 554), (554, 555), (555, 557), (557, 558), (558, 559), (559, 560), (560, 561), (561, 562), (562, 563), (563, 564), (564, 565), (565, 566), (566, 567), (567, 568), (568, 569), (569, 570), (570, 571), (571, 572), (572, 573), (573, 574), (574, 575), (575, 576), (576, 577), (577, 578), (578, 579), (579, 580), (580, 581), (581, 582), (582, 583), (583, 584), (584, 585), (585, 586), (586, 587), (587, 588), (588, 589), (589, 590), (590, 591), (591, 592), (592, 593), (593, 594), (594, 595), (595, 596), (596, 597), (597, 598), (598, 599), (599, 600), (600, 601), (601, 603), (603, 604), (604, 606), (606, 607), (607, 608), (608, 609), (609, 610), (610, 611), (611, 612), (612, 613), (613, 614), (614, 615), (615, 616), (616, 617), (617, 618), (618, 619), (619, 620), (620, 621), (621, 622), (622, 623), (623, 624), (624, 625), (625, 626), (626, 627), (627, 631), (631, 632), (632, 633), (633, 637), (637, 638), (638, 639), (639, 640), (640, 641), (641, 642), (642, 643), (643, 644), (644, 645), (645, 646), (646, 647), (647, 648), (648, 649), (649, 650), (650, 651), (651, 652), (652, 653), (653, 657), (657, 658), (658, 659), (659, 660), (660, 662), (662, 663), (663, 664), (664, 665), (665, 666), (666, 667), (667, 668), (668, 669), (669, 670), (670, 671), (671, 672), (672, 673), (673, 674), (674, 675), (675, 676), (676, 677), (677, 678), (678, 679), (679, 680), (680, 681), (681, 682), (682, 683), (683, 684), (684, 685), (685, 686), (686, 687), (687, 688), (688, 689), (689, 690), (690, 691), (691, 692), (692, 693), (693, 694), (694, 695), (695, 696), (696, 697), (697, 698), (698, 699), (699, 700), (700, 701), (701, 702), (702, 703), (703, 704), (704, 705), (705, 706), (706, 707), (707, 708), (708, 709), (709, 710), (710, 714), (714, 715), (715, 716), (716, 717), (717, 719), (719, 720), (720, 721), (721, 722), (722, 723), (723, 724), (724, 725), (725, 726), (726, 727), (727, 728), (728, 729), (729, 730), (730, 731), (731, 732), (732, 733), (733, 734), (734, 735), (735, 736), (736, 737), (737, 738), (738, 739), (739, 740), (740, 741), (741, 742), (742, 743), (743, 744), (744, 745), (745, 746), (746, 747), (747, 748), (748, 749), (749, 750), (750, 751), (751, 752), (752, 753), (753, 754), (754, 755), (755, 756), (756, 757), (757, 758), (758, 759), (759, 760), (760, 761), (761, 762), (762, 763), (763, 767), (767, 768), (768, 769), (769, 770), (770, 772), (772, 773), (773, 774), (774, 775), (775, 776), (776, 777), (777, 778), (778, 779), (779, 780), (780, 781), (781, 782), (782, 783), (783, 785), (785, 786), (786, 787), (787, 788), (788, 789), (789, 790), (790, 791), (791, 792), (792, 793), (793, 795), (795, 796), (796, 797), (797, 798), (798, 799), (799, 800), (800, 801), (801, 802), (802, 803), (803, 804), (804, 805), (805, 806), (806, 807), (807, 808), (808, 809), (809, 810), (810, 811), (811, 812), (0, 0)],</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (756, 757), (757, 758), (758, 759), (759, 760), (760, 761), (761, 762), (762, 763), (763, 767), (767, 768), (768, 769), (769, 770), (770, 772), (772, 773), (773, 774), (774, 775), (775, 776), (776, 777), (777, 778), (778, 779), (779, 780), (780, 781), (781, 782), (782, 783), (783, 785), (785, 786), (786, 787), (787, 788), (788, 789), (789, 790), (790, 791), (791, 792), (792, 793), (793, 795), (795, 796), (796, 797), (797, 798), (798, 799), (799, 800), (800, 801), (801, 802), (802, 803), (803, 804), (804, 805), (805, 806), (806, 807), (807, 808), (808, 809), (809, 810), (810, 811), (811, 812), (812, 813), (813, 814), (814, 815), (0, 0)]]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;overflow_to_sample_mapping&#x27;: [0, 0, 0, 0]&#125;</span><br></pre></td></tr></table></figure><p><strong>四份分割，问题后面的文章起始位置都是在原文中对应的坐标</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">offset[context_start][<span class="number">0</span>] &gt; end_char <span class="keyword">or</span> offset[context_end][<span class="number">1</span>] &lt; start_char</span><br></pre></td></tr></table></figure><p>如果train_data中原始答案坐标，大于offset（四个样本）的范围，那么答案就不在这个offset[i]中</p><ul><li>context_start，context_end是代表这个offset起始和结束的位置，由sequence_ids()产生【第一句话为0，其他的为1】</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">idx = context_start</span><br><span class="line">            <span class="keyword">while</span> idx &lt;= context_end <span class="keyword">and</span> offset[idx][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line">            start_positions.append(idx - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            idx = context_end</span><br><span class="line">            <span class="keyword">while</span> idx &gt;= context_start <span class="keyword">and</span> offset[idx][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">                idx -= <span class="number">1</span></span><br><span class="line">            end_positions.append(idx + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>最后，根据 context_end（idx &#x3D; context_start）是sequence_ids内的idx，其对应的就是【0,1】的值，</p><p>用两个while </p><p>&#x3D;&#x3D;idx &lt;&#x3D; context_end and offset[idx][0] &lt;&#x3D; start_char&#x3D;&#x3D;作为条件</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/03%20Pipeline%20%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/03%20Pipeline%20%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="title-03-Pipeline-句子相似度"><a href="#title-03-Pipeline-句子相似度" class="headerlink" title="title:03 Pipeline 句子相似度"></a>title:03 Pipeline 句子相似度</h2><p><strong>主要进行训练框架优化</strong></p><ul><li>端到端 ML 实施（训练、验证、预测、评估）</li><li>轻松适应您自己的数据集</li><li>促进其他基于 BERT 的模型（BERT、ALBERT、…）的快速实验</li><li>使用有限的计算资源进行快速训练（混合精度、梯度累积……）</li><li>多 GPU 执行</li><li>分类决策的阈值选择（不一定是 0.5）</li><li>冻结 BERT 层，只更新分类层权重或更新所有权重</li><li>种子设置，可复现结果</li></ul><h1 id="PipeLine"><a href="#PipeLine" class="headerlink" title="PipeLine"></a>PipeLine</h1><h3 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, maxlen, with_labels=<span class="literal">True</span>, bert_model=<span class="string">&#x27;albert-base-v2&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">        self.data = data  <span class="comment"># pandas dataframe</span></span><br><span class="line">        <span class="comment">#Initialize the tokenizer</span></span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  </span><br><span class="line"></span><br><span class="line">        self.maxlen = maxlen</span><br><span class="line">        self.with_labels = with_labels </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#根据索引索取DataFrame中句子1余句子2</span></span><br><span class="line">        sent1 = <span class="built_in">str</span>(self.data.loc[index, <span class="string">&#x27;sentence1&#x27;</span>])</span><br><span class="line">        sent2 = <span class="built_in">str</span>(self.data.loc[index, <span class="string">&#x27;sentence2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对句子对分词，得到input_ids、attention_mask和token_type_ids</span></span><br><span class="line">        encoded_pair = self.tokenizer(sent1, sent2, </span><br><span class="line">                                      padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 填充到最大长度</span></span><br><span class="line">                                      truncation=<span class="literal">True</span>,  <span class="comment"># 根据最大长度进行截断</span></span><br><span class="line">                                      max_length=self.maxlen,  </span><br><span class="line">                                      return_tensors=<span class="string">&#x27;pt&#x27;</span>)  <span class="comment"># 返回torch.Tensor张量</span></span><br><span class="line">        </span><br><span class="line">        token_ids = encoded_pair[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment"># tensor token ids</span></span><br><span class="line">        attn_masks = encoded_pair[<span class="string">&#x27;attention_mask&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment"># padded values对应为 &quot;0&quot; ，其他token为1</span></span><br><span class="line">        token_type_ids = encoded_pair[<span class="string">&#x27;token_type_ids&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment">#第一个句子的值为0，第二个句子的值为1 # 只有一句全为0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.with_labels:  <span class="comment"># True if the dataset has labels</span></span><br><span class="line">            label = self.data.loc[index, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">            <span class="keyword">return</span> token_ids, attn_masks, token_type_ids, label  </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> token_ids, attn_masks, token_type_ids</span><br></pre></td></tr></table></figure><p>建议，进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample = <span class="built_in">next</span>(<span class="built_in">iter</span>(DataLoader(tr_dataset, batch_size=<span class="number">2</span>)))</span><br><span class="line">sample</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr_model = SentencePairClassifier(freeze_bert=True)</span><br><span class="line">tr_model(sample[0], sample[1], sample[2])</span><br></pre></td></tr></table></figure><p>就是方便最后的维度转换，squeeze、flatten、view；甚至可以用reshape方法</p><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentencePairClassifier</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert_model=<span class="string">&quot;albert-base-v2&quot;</span>, freeze_bert=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SentencePairClassifier, self).__init__()</span><br><span class="line">        <span class="comment">#  初始化预训练模型Bert xxx</span></span><br><span class="line">        self.bert_layer = AutoModel.from_pretrained(bert_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  encoder 隐藏层大小</span></span><br><span class="line">        <span class="keyword">if</span> bert_model == <span class="string">&quot;albert-base-v2&quot;</span>:  <span class="comment"># 12M 参数</span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-large-v2&quot;</span>:  <span class="comment"># 18M 参数</span></span><br><span class="line">            hidden_size = <span class="number">1024</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-xlarge-v2&quot;</span>:  <span class="comment"># 60M 参数</span></span><br><span class="line">            hidden_size = <span class="number">2048</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-xxlarge-v2&quot;</span>:  <span class="comment"># 235M 参数</span></span><br><span class="line">            hidden_size = <span class="number">4096</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="comment"># 110M 参数</span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;roberta-base&quot;</span>: <span class="comment"># </span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 固定Bert层 更新分类输出层</span></span><br><span class="line">        <span class="keyword">if</span> freeze_bert:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> self.bert_layer.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">False</span></span><br><span class="line">                </span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 分类输出</span></span><br><span class="line">        self.cls_layer = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">    @autocast()  </span><span class="comment"># 混合精度训练</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attn_masks, token_type_ids</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">            -input_ids : Tensor  containing token ids</span></span><br><span class="line"><span class="string">            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values</span></span><br><span class="line"><span class="string">            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入给Bert，获取上下文表示</span></span><br><span class="line">        <span class="comment"># cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)</span></span><br><span class="line">        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)</span><br><span class="line">        <span class="comment"># last_hidden_state,pooler_output,all_hidden_states 12层</span></span><br><span class="line">        <span class="comment"># 将last layer hidden-state of the [CLS] 输入到 classifier layer</span></span><br><span class="line">        <span class="comment"># - last_hidden_state 的向量平均</span></span><br><span class="line">        <span class="comment"># - 取all_hidden_states最后四层，然后做平均 weighted 平均</span></span><br><span class="line">        <span class="comment"># - last_hidden_state+lstm</span></span><br><span class="line">        <span class="comment"># 获取输出</span></span><br><span class="line">        logits = self.cls_layer(self.dropout(outputs[<span class="string">&#x27;pooler_output&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h3 id="固定随机种子"><a href="#固定随机种子" class="headerlink" title="固定随机种子"></a>固定随机种子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 固定随机种子，保证结果复现</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br></pre></td></tr></table></figure><h3 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!mkdir models <span class="comment">#可以在之前补充绝对路径</span></span><br><span class="line">!mkdir results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate</span>):</span><br><span class="line"></span><br><span class="line">    best_loss = np.Inf</span><br><span class="line">    best_ep = <span class="number">1</span></span><br><span class="line">    nb_iterations = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    print_every = nb_iterations // <span class="number">5</span>  <span class="comment"># 打印频率</span></span><br><span class="line">    iters = []</span><br><span class="line">    train_losses = []</span><br><span class="line">    val_losses = []</span><br><span class="line"></span><br><span class="line">    scaler = GradScaler()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line"></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> it, (seq, attn_masks, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_loader)):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 转为cuda张量</span></span><br><span class="line">            seq, attn_masks, token_type_ids, labels = \</span><br><span class="line">                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># 混合精度加速训练</span></span><br><span class="line">            <span class="keyword">with</span> autocast():</span><br><span class="line">                <span class="comment"># Obtaining the logits from the model</span></span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Computing loss</span></span><br><span class="line">                loss = criterion(logits.squeeze(-<span class="number">1</span>), labels.<span class="built_in">float</span>())</span><br><span class="line">                loss = loss / iters_to_accumulate  <span class="comment"># Normalize the loss because it is averaged</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backpropagating the gradients</span></span><br><span class="line">            <span class="comment"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class="line">            scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % iters_to_accumulate == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Optimization step</span></span><br><span class="line">                <span class="comment"># scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.</span></span><br><span class="line">                <span class="comment"># If these gradients do not contain infs or NaNs, opti.step() is then called,</span></span><br><span class="line">                <span class="comment"># otherwise, opti.step() is skipped.</span></span><br><span class="line">                scaler.step(opti)</span><br><span class="line">                <span class="comment"># Updates the scale for next iteration.</span></span><br><span class="line">                scaler.update()</span><br><span class="line">                <span class="comment"># 根据迭代次数调整学习率。</span></span><br><span class="line">                lr_scheduler.step()</span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                opti.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % print_every == <span class="number">0</span>:  <span class="comment"># Print training loss information</span></span><br><span class="line">                <span class="built_in">print</span>()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;it+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;nb_iterations&#125;</span> of epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete. \</span></span><br><span class="line"><span class="string">                Loss : <span class="subst">&#123;running_loss / print_every&#125;</span> &quot;</span>)</span><br><span class="line"></span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        val_loss = evaluate_loss(net, device, criterion, val_loader)  <span class="comment"># Compute validation loss</span></span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete! Validation Loss : <span class="subst">&#123;val_loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;</span>.<span class="built_in">format</span>(best_loss, val_loss))</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">            net_copy = copy.deepcopy(net)  <span class="comment"># # 保存最优模型</span></span><br><span class="line">            best_loss = val_loss</span><br><span class="line">            best_ep = ep + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    path_to_model=<span class="string">f&#x27;models/<span class="subst">&#123;bert_model&#125;</span>_lr_<span class="subst">&#123;lr&#125;</span>_val_loss_<span class="subst">&#123;<span class="built_in">round</span>(best_loss, <span class="number">5</span>)&#125;</span>_ep_<span class="subst">&#123;best_ep&#125;</span>.pt&#x27;</span></span><br><span class="line">    torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model has been saved in &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_model))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> loss</span><br><span class="line">    torch.cuda.empty_cache() <span class="comment"># 清空显存</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, device, criterion, dataloader</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    评估输出</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    mean_loss = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> it, (seq, attn_masks, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(dataloader)):</span><br><span class="line">            seq, attn_masks, token_type_ids, labels = \</span><br><span class="line">                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)</span><br><span class="line">            logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">            mean_loss += criterion(logits.squeeze(-<span class="number">1</span>), labels.<span class="built_in">float</span>()).item()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mean_loss / count</span><br></pre></td></tr></table></figure><ol><li><p>注意autocast和累计梯度 这两种加速计算的方法</p></li><li><p>evaluate的时候要注意数据的维度，标签的类型</p></li></ol><h3 id="超参数-amp-开始训练"><a href="#超参数-amp-开始训练" class="headerlink" title="超参数 &amp; 开始训练"></a>超参数 &amp; 开始训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bert_model = <span class="string">&quot;albert-base-v2&quot;</span>  <span class="comment"># &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;</span></span><br><span class="line">freeze_bert = <span class="literal">False</span>  <span class="comment"># 是否冻结Bert</span></span><br><span class="line">maxlen = <span class="number">128</span>  <span class="comment"># 最大长度</span></span><br><span class="line">bs = <span class="number">16</span>  <span class="comment"># batch size</span></span><br><span class="line">iters_to_accumulate = <span class="number">2</span>  <span class="comment"># 梯度累加</span></span><br><span class="line">lr = <span class="number">2e-5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># 训练轮数</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  固定随机种子 便于复现</span></span><br><span class="line">set_seed(<span class="number">1</span>) <span class="comment"># 2022 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集与验证集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading training data...&quot;</span>)</span><br><span class="line">train_set = CustomDataset(df_train, maxlen, bert_model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading validation data...&quot;</span>)</span><br><span class="line">val_set = CustomDataset(df_val, maxlen, bert_model)</span><br><span class="line"><span class="comment"># 常见训练集与验证集DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  <span class="comment"># if multiple GPUs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    net = nn.DataParallel(net)</span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">opti = AdamW(net.parameters(), lr=lr, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line">num_warmup_steps = <span class="number">0</span> <span class="comment"># The number of steps for the warmup phase.</span></span><br><span class="line">num_training_steps = epochs * <span class="built_in">len</span>(train_loader)  <span class="comment"># The total number of training steps</span></span><br><span class="line">t_total = (<span class="built_in">len</span>(train_loader) // iters_to_accumulate) * epochs  <span class="comment"># Necessary to take into account Gradient accumulation</span></span><br><span class="line">lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)</span><br><span class="line"></span><br><span class="line">train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)</span><br></pre></td></tr></table></figure><ol><li>注意多gpu训练 <code>torch.cuda.device_count() &gt; 1</code>, <code>net = nn.DataParallel(net)</code>的使用</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_probs_from_logits</span>(<span class="params">logits</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Converts a tensor of logits into an array of probabilities by applying the sigmoid function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    probs = torch.sigmoid(logits.unsqueeze(-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> probs.detach().cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_prediction</span>(<span class="params">net, device, dataloader, with_labels=<span class="literal">True</span>, result_file=<span class="string">&quot;results/output.txt&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Predict the probabilities on a dataset with or without labels and print the result in a file</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    w = <span class="built_in">open</span>(result_file, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    probs_all = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> with_labels:</span><br><span class="line">            <span class="keyword">for</span> seq, attn_masks, token_type_ids, _ <span class="keyword">in</span> tqdm(dataloader):<span class="comment"># 训练集、验证集</span></span><br><span class="line">                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)</span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">                probs = get_probs_from_logits(logits.squeeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                probs_all += probs.tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> seq, attn_masks, token_type_ids <span class="keyword">in</span> tqdm(dataloader): <span class="comment"># 没有标签的测试集</span></span><br><span class="line">                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)</span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">                probs = get_probs_from_logits(logits.squeeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                probs_all += probs.tolist()</span><br><span class="line"></span><br><span class="line">    w.writelines(<span class="built_in">str</span>(prob)+<span class="string">&#x27;\n&#x27;</span> <span class="keyword">for</span> prob <span class="keyword">in</span> probs_all)</span><br><span class="line">    w.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">path_to_model = <span class="string">&#x27;./model&#x27;</span>  </span><br><span class="line"><span class="comment"># path_to_model = &#x27;/content/models/...&#x27;  # You can add here your trained model</span></span><br><span class="line"></span><br><span class="line">path_to_output_file = <span class="string">&#x27;./results&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading test data...&quot;</span>)</span><br><span class="line">test_set = CustomDataset(df_test, maxlen, bert_model)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model = SentencePairClassifier(bert_model)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  <span class="comment"># if multiple GPUs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loading the weights of the model...&quot;</span>)</span><br><span class="line">model.load_state_dict(torch.load(path_to_model))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicting on test data...&quot;</span>)</span><br><span class="line">test_prediction(net=model, device=device, dataloader=test_loader, with_labels=<span class="literal">True</span>,  <span class="comment"># set the with_labels parameter to False if your want to get predictions on a dataset without labels</span></span><br><span class="line">                result_file=path_to_output_file)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predictions are available in : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_output_file))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">path_to_output_file = <span class="string">&#x27;results/output.txt&#x27;</span>  <span class="comment"># 预测结果概率文件</span></span><br><span class="line"></span><br><span class="line">labels_test = df_test[<span class="string">&#x27;label&#x27;</span>]  <span class="comment"># true labels</span></span><br><span class="line"></span><br><span class="line">probs_test = pd.read_csv(path_to_output_file, header=<span class="literal">None</span>)[<span class="number">0</span>]  <span class="comment"># 预测概率</span></span><br><span class="line">threshold = <span class="number">0.6</span>   <span class="comment"># you can adjust this threshold for your own dataset</span></span><br><span class="line">preds_test=(probs_test&gt;=threshold).astype(<span class="string">&#x27;uint8&#x27;</span>) <span class="comment"># predicted labels using the above fixed threshold</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># metric = load_metric(&quot;glue&quot;, &quot;mrpc&quot;)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>02 Bert Toxic 评论</title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/02%20Bert%20Toxic%20%E8%AF%84%E8%AE%BA%20/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/02%20Bert%20Toxic%20%E8%AF%84%E8%AE%BA%20/</url>
      
        <content type="html"><![CDATA[<h1 id="package"><a href="#package" class="headerlink" title="package"></a>package</h1><p><strong>tqdm</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><p><strong>Kfold</strong> 废了 没用的东西 <strong>nlp中还是去练prompt好了</strong></p><p>kfold引用的是下标，所以在dataloader中加载时，将下标赋值给sampler进行采样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"></span><br><span class="line">k_folds = <span class="number">5</span></span><br><span class="line">kfold = KFold(n_splits=k_folds, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>dataset的返回有大概两种</p><ol><li><p>返回数据，可以直接处理的那种</p></li><li><p>返回路径，即列表是存储 指向数据的路径的字符串，</p><ol><li>在dataloader( cutmix( dataset ) )进行二次处理，</li><li>dataloader( tokenizer( dataset ) ) 相同，&#x3D;&#x3D;大概吧，还没试过，tokenizer返回的是打包的元组&#x3D;&#x3D;  <strong>不行，换kfold了</strong></li></ol></li><li><p>与kfold 或者 特类notebook中的tqdm 使用要注意细节</p></li></ol><h1 id="Bert-模型选择"><a href="#Bert-模型选择" class="headerlink" title="Bert 模型选择"></a>Bert 模型选择</h1><ol><li>bert的选择很关键，注意要看到文档说明的返回数据的形状</li><li>损失函数的选择也很重要，BCE中要使用view( -1, num_class) 进行转换。</li></ol><h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a><strong>导包</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle <span class="keyword">as</span> reset</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, accuracy_score</span><br></pre></td></tr></table></figure><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a><strong>预处理</strong></h2><h3 id="x3D-x3D-train、valid-划分-x3D-x3D"><a href="#x3D-x3D-train、valid-划分-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;train、valid 划分&#x3D;&#x3D;"></a>&#x3D;&#x3D;train、valid 划分&#x3D;&#x3D;</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot_key</span>(<span class="params">file_path</span>):</span><br><span class="line">    data_df = pd.read_csv(file_path, error_bad_lines=<span class="literal">False</span>,engine=<span class="string">&#x27;python&#x27;</span>)</span><br><span class="line">    colunms_name = data_df.columns.to_list()</span><br><span class="line">    onehot_labels = train_df[colunms_name].values.tolist()</span><br><span class="line">    <span class="comment">#onehot_labels = [data_df.iloc[i, -6:].to_list() for i in tqdm(range(len(data_df)))]</span></span><br><span class="line">    data_df[<span class="string">&#x27;label&#x27;</span>] = onehot_labels</span><br><span class="line">    data_for_bert = data_df[[<span class="string">&#x27;comment_text&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_for_bert</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据集index打乱后进行 82分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_valid_split</span>(<span class="params">data_df, test_size, shuffle=<span class="literal">True</span>, random_state=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        data_df = reset(data_df, random_state=random_state)</span><br><span class="line"></span><br><span class="line">    train = data_df[<span class="built_in">int</span>(<span class="built_in">len</span>(data_df)*test_size):].reset_index(drop = <span class="literal">True</span>)</span><br><span class="line">    valid  = data_df[:<span class="built_in">int</span>(<span class="built_in">len</span>(data_df)*test_size)].reset_index(drop = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train, valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">num, file_path, batch_num</span>):</span><br><span class="line">    data_df = one_hot_key(file_path)</span><br><span class="line">    <span class="comment"># 进行 fold次划分返回 input_ids, token_type, mask</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">        train, valid = train_valid_split(data_df, test_size=(<span class="number">1</span>/fold))</span><br><span class="line">        train_set = mydataset(train)</span><br><span class="line">        valid_set = mydataset(valid)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> train_loader, valid_loader <span class="comment"># 做yield一次一次返回</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>补充一个对含有字符串的处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">df_to_content</span>(<span class="params">file_path</span>): </span><br><span class="line">    colunms_name = data_df.columns.to_list()</span><br><span class="line">    onehot_labels = <span class="built_in">list</span>(data_df[colunms_name].applymap(<span class="built_in">str</span>).values.tolist())</span><br><span class="line">    data_df[<span class="string">&#x27;content&#x27;</span>] = onehot_labels</span><br><span class="line">    data_df[<span class="string">&#x27;content&#x27;</span>] = data_df[<span class="string">&#x27;content&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27; &#x27;</span>.join(x)) </span><br><span class="line">    <span class="keyword">return</span> train_df</span><br></pre></td></tr></table></figure><h3 id="Dataset-1"><a href="#Dataset-1" class="headerlink" title="Dataset"></a><strong>Dataset</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">mydataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, with_label=<span class="literal">True</span></span>):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.with_label = with_label</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        self.feature = self.data.comment_text[idx]</span><br><span class="line">        self.label = self.data.label[idx]</span><br><span class="line">        </span><br><span class="line">        inputs = tokenizer(self.feature, return_tensors=<span class="string">&quot;pt&quot;</span>, </span><br><span class="line">                           padding=<span class="string">&#x27;max_length&#x27;</span>, max_length=<span class="number">160</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        input_ids= inputs.input_ids.squeeze(<span class="number">0</span>)</span><br><span class="line">        token_type = inputs.token_type_ids.squeeze(<span class="number">0</span>)</span><br><span class="line">        mask = inputs.attention_mask.squeeze(<span class="number">0</span>)</span><br><span class="line">        label = torch.Tensor(self.label)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.with_label:</span><br><span class="line">            <span class="keyword">return</span> input_ids, token_type, mask, label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> input_ids, token_type, mask</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a><strong>训练</strong></h2><h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>, num_labels=<span class="number">6</span>) </span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line">loss_func = nn.BCEWithLogitsLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">path = <span class="string">&#x27;/content/train.csv&#x27;</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">batch_num = <span class="number">64</span></span><br><span class="line">best_score = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">!mkdir model</span><br><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> train_set, valid_set <span class="keyword">in</span> train_epoch(epochs, path, batch_num):</span><br><span class="line">    </span><br><span class="line">    train_loader = DataLoader(train_set, batch_size=batch_num)</span><br><span class="line">    valid_loader = DataLoader(valid_set, batch_size=batch_num)</span><br><span class="line">    train_loss, valid_loss = [], []</span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(train_loader):</span><br><span class="line">        input_ids, token_type, mask, label = batch</span><br><span class="line">        input_ids, token_type, mask, label = input_ids.to(device), token_type.to(device), </span><br><span class="line">         mask.to(device), label.to(device)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        outputs = model(input_ids, token_type, mask)</span><br><span class="line">        logits = outputs[<span class="number">0</span>]</span><br><span class="line">        loss = loss_func(logits.view(-<span class="number">1</span>,<span class="number">6</span>),label.view(-<span class="number">1</span>,<span class="number">6</span>))</span><br><span class="line">        train_loss.append(loss.item())</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">    train_ave_loss = ( <span class="built_in">sum</span>(train_loss)/ <span class="built_in">len</span>(train_loss))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># valid</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(valid_loader):</span><br><span class="line">        input_ids, token_type, mask, label = <span class="built_in">tuple</span>(t.to(device) <span class="keyword">for</span> t <span class="keyword">in</span> batch)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model(input_ids, token_type, mask)</span><br><span class="line">            b_logit_pred = outputs[<span class="number">0</span>]</span><br><span class="line">            pred_label = torch.sigmoid(b_logit_pred)</span><br><span class="line">            </span><br><span class="line">            b_logit_pred = b_logit_pred.detach().cpu().numpy()</span><br><span class="line">            pred_label = pred_label.to(<span class="string">&#x27;cpu&#x27;</span>).numpy()</span><br><span class="line">            label = label.to(<span class="string">&#x27;cpu&#x27;</span>).numpy()</span><br><span class="line"></span><br><span class="line">            <span class="comment">#tokenized_texts.append(b_input_ids) #这是啥</span></span><br><span class="line">            <span class="comment">#logit_preds.append(b_logit_pred)</span></span><br><span class="line">            true_labels.append(label)</span><br><span class="line">            pred_labels.append(pred_label)</span><br><span class="line">            </span><br><span class="line">    pred_labels = [item <span class="keyword">for</span> sublist <span class="keyword">in</span> pred_labels <span class="keyword">for</span> item <span class="keyword">in</span> sublist]</span><br><span class="line">    true_labels = [item <span class="keyword">for</span> sublist <span class="keyword">in</span> true_labels <span class="keyword">for</span> item <span class="keyword">in</span> sublist]</span><br><span class="line"></span><br><span class="line">    threshold = <span class="number">0.50</span></span><br><span class="line">    pred_bools = [pl&gt;threshold <span class="keyword">for</span> pl <span class="keyword">in</span> pred_labels] <span class="comment">#大于0.5的会被设置为True</span></span><br><span class="line">    true_bools = [tl==<span class="number">1</span> <span class="keyword">for</span> tl <span class="keyword">in</span> true_labels]</span><br><span class="line">    val_f1_accuracy = f1_score(true_bools,pred_bools,average=<span class="string">&#x27;micro&#x27;</span>)*<span class="number">100</span></span><br><span class="line">    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*<span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;F1 Validation Accuracy: &#x27;</span>, val_f1_accuracy)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Flat Validation Accuracy: &#x27;</span>, val_flat_accuracy)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">global</span> best_score     </span><br><span class="line">        <span class="keyword">if</span> best_score &lt; val_f1_accuracy + val_flat_accuracy:</span><br><span class="line">            best_score = val_f1_accuracy + val_flat_accuracy</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">f&#x27;/model/Score:<span class="subst">&#123;best_score&#125;</span>.pth&#x27;</span>)</span><br><span class="line">            <span class="comment">#model.config.to_json_file()</span></span><br></pre></td></tr></table></figure><p><del>不太明白为什么要阀值输出，做了以下优化</del></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sample = np.random.random((<span class="number">3</span>,<span class="number">4</span>)).tolist()</span><br><span class="line">label = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]]).tolist()</span><br><span class="line">df = pd.DataFrame(sample)</span><br><span class="line">result = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">0</span>]):</span><br><span class="line">    length = <span class="built_in">sum</span>(label[i])</span><br><span class="line">    result.append(df.iloc[i,:].sort_values(ascending=<span class="literal">False</span>).index.to_list()[:length])</span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pred_df = pd.DataFrame(pred_labels.tolist())</span><br><span class="line">result = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">0</span>]):</span><br><span class="line">    length = <span class="built_in">sum</span>(label[i])</span><br><span class="line">    result.append(pred_df[i,:].sort_values(ascending=<span class="literal">False</span>).index.to_list()[:length])</span><br><span class="line"></span><br><span class="line"><span class="comment"># result即为结果</span></span><br></pre></td></tr></table></figure><p><strong>是因为预测的时候，没有标签数量啦，笨蛋</strong></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a><strong>测试</strong></h2><p>测试的时候就不能shuffle了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#preprocesing 之后直接丢给 fine_tune好的 bert</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">&#x27;model_path.pth&#x27;</span>).to(device)</span><br><span class="line"></span><br><span class="line">pred_label_ids = torch.argmax(outputs[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">name_class = tokenizer.convert_ids_to_tokens(pred_label_ids)</span><br></pre></td></tr></table></figure><h2 id="F1：阈值搜索"><a href="#F1：阈值搜索" class="headerlink" title="F1：阈值搜索"></a>F1：阈值搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">macro_thresholds = np.array(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>))/<span class="number">10</span></span><br><span class="line">f1_results, flat_acc_results = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> th <span class="keyword">in</span> macro_thresholds:</span><br><span class="line">    pred_bools = [pl&gt;th <span class="keyword">for</span> pl <span class="keyword">in</span> pred_labels]</span><br><span class="line">    test_f1_accuracy = f1_score(true_bools,pred_bools,average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    test_flat_accuracy = accuracy_score(true_bools, pred_bools)</span><br><span class="line">    f1_results.append(test_f1_accuracy)</span><br><span class="line">    flat_acc_results.append(test_flat_accuracy)</span><br><span class="line"></span><br><span class="line">best_macro_th = macro_thresholds[np.argmax(f1_results)] <span class="comment">#best macro threshold value</span></span><br><span class="line"></span><br><span class="line">micro_thresholds = (np.array(<span class="built_in">range</span>(<span class="number">10</span>))/<span class="number">100</span>)+best_macro_th <span class="comment">#calculating micro threshold values</span></span><br><span class="line"></span><br><span class="line">f1_results, flat_acc_results = [], []</span><br><span class="line"><span class="keyword">for</span> th <span class="keyword">in</span> micro_thresholds:</span><br><span class="line">    pred_bools = [pl&gt;th <span class="keyword">for</span> pl <span class="keyword">in</span> pred_labels]</span><br><span class="line">    test_f1_accuracy = f1_score(true_bools,pred_bools,average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    test_flat_accuracy = accuracy_score(true_bools, pred_bools)</span><br><span class="line">    f1_results.append(test_f1_accuracy)</span><br><span class="line">    flat_acc_results.append(test_flat_accuracy)</span><br><span class="line"></span><br><span class="line">best_f1_idx = np.argmax(f1_results) <span class="comment">#best threshold value</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Printing and saving classification report</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Best Threshold: &#x27;</span>, micro_thresholds[best_f1_idx])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test F1 Accuracy: &#x27;</span>, f1_results[best_f1_idx])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test Flat Accuracy: &#x27;</span>, flat_acc_results[best_f1_idx], <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">best_pred_bools = [pl&gt;micro_thresholds[best_f1_idx] <span class="keyword">for</span> pl <span class="keyword">in</span> pred_labels]</span><br><span class="line">clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)</span><br><span class="line">pickle.dump(clf_report_optimized, <span class="built_in">open</span>(<span class="string">&#x27;classification_report_optimized.txt&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(clf_report_optimized)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>01 叶子分类Tricks</title>
      <link href="/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/01%20%E5%8F%B6%E5%AD%90%E5%88%86%E7%B1%BBTricks%20/"/>
      <url>/2022/11/21/kaggle%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0/01%20%E5%8F%B6%E5%AD%90%E5%88%86%E7%B1%BBTricks%20/</url>
      
        <content type="html"><![CDATA[<h1 id="Library"><a href="#Library" class="headerlink" title="Library"></a>Library</h1><h2 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a>Sklearn</h2><ol><li>预处理函数preprocessing</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessing有很多个子函数，比如正态化数据，做one-hot-key等</span></span><br><span class="line">sex = pd.Series([<span class="string">&quot;male&quot;</span>, <span class="string">&quot;female&quot;</span>, <span class="string">&quot;female&quot;</span>, <span class="string">&quot;male&quot;</span>, <span class="string">&quot;male&quot;</span>])</span><br><span class="line"></span><br><span class="line">encoder = preprocessing.LabelEncoder()       <span class="comment">#获取一个LabelEncoder</span></span><br><span class="line"><span class="comment">#可用df[&#x27;label&#x27;].unique().to_list()获得标签类别</span></span><br><span class="line">model = encoder.fit([<span class="string">&quot;male&quot;</span>, <span class="string">&quot;female&quot;</span>])      <span class="comment">#训练LabelEncoder, 把male编码为0，female编码为1</span></span><br><span class="line">sex = model.transform(sex)                   <span class="comment">#使用训练好的LabelEncoder对原数据进行编码</span></span><br><span class="line"><span class="comment"># 得到[1 0 0 1 1]</span></span><br></pre></td></tr></table></figure><h2 id="CutMix"><a href="#CutMix" class="headerlink" title="CutMix"></a>CutMix</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cutmix.cutmix <span class="keyword">import</span> CutMix</span><br><span class="line"><span class="keyword">from</span> cutmix.utils <span class="keyword">import</span> CutMixCrossEntropyLoss</span><br><span class="line"></span><br><span class="line">train_loss_function = CutMixCrossEntropyLoss(<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#其内部实现如下:</span></span><br><span class="line">  <span class="comment"># Compute the accuracy for current batch.</span></span><br><span class="line">      <span class="comment"># acc = (logits.argmax(dim=-1) == labels).float().mean()</span></span><br><span class="line">      <span class="comment"># Record the loss and accuracy</span></span><br><span class="line">    </span><br><span class="line">trainloader = torch.utils.data.DataLoader(</span><br><span class="line">                      CutMix(TrainValidData(train_val_path, img_path, transform = train_transform),                               num_class=<span class="number">176</span>, beta=<span class="number">1.0</span>, prob=<span class="number">0.5</span>, num_mix=<span class="number">2</span>), </span><br><span class="line">                      batch_size=<span class="number">128</span>, sampler=train_subsampler, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_transform、train_subsampler变量如下:</span></span><br><span class="line">train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)</span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    <span class="comment"># 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。</span></span><br><span class="line">    <span class="comment"># 然后，缩放图像以创建224 x 224的新图像</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.08</span>, <span class="number">1.0</span>), ratio=(<span class="number">3.0</span> / <span class="number">4.0</span>, <span class="number">4.0</span> / <span class="number">3.0</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    <span class="comment"># 随机更改亮度，对比度和饱和度</span></span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">    <span class="comment"># 添加随机噪声</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 标准化图像的每个通道</span></span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br></pre></td></tr></table></figure><h1 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h1><h2 id="Lr-scheduler"><a href="#Lr-scheduler" class="headerlink" title="Lr_scheduler"></a>Lr_scheduler</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> CosineAnnealingLR</span><br><span class="line"></span><br><span class="line">scheduler = CosineAnnealingLR(optimizer,T_max=<span class="number">10</span>)</span><br><span class="line">scheduler.step()</span><br></pre></td></tr></table></figure><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>图片增强: 随机剪裁、翻转、对比度和颜色调整、随机噪声</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    <span class="comment"># 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。</span></span><br><span class="line">    <span class="comment"># 然后，缩放图像以创建224 x 224的新图像</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.08</span>, <span class="number">1.0</span>), ratio=(<span class="number">3.0</span> / <span class="number">4.0</span>, <span class="number">4.0</span> / <span class="number">3.0</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    <span class="comment"># 随机更改亮度，对比度和饱和度</span></span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">    <span class="comment"># 添加随机噪声</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 标准化图像的每个通道</span></span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line">val_test_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    <span class="comment"># 从图像中心裁切224x224大小的图片</span></span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br></pre></td></tr></table></figure><h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>将数据通过多个不同的模型，以其最终在验证集上的正确率做权重分配，投票得出最终的submission</p><p>或者直接平均，要注意每个模型对数据的要求和敏感度。</p><h2 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h2><p>不光训练的时候K折验证，提交submission也要K折，以获得最平滑平均的正确率</p><p>示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kf = KFold(n_splits=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kf.get_n_splits(X)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(kf)</span><br><span class="line">KFold(n_splits=<span class="number">2</span>, random_state=<span class="literal">None</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kf.split(X):</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&quot;TRAIN:&quot;</span>, train_index, <span class="string">&quot;TEST:&quot;</span>, test_index)</span><br><span class="line"><span class="meta">... </span>    X_train, X_test = X[train_index], X[test_index]</span><br><span class="line"><span class="meta">... </span>    y_train, y_test = y[train_index], y[test_index]</span><br><span class="line">TRAIN: [<span class="number">2</span> <span class="number">3</span>] TEST: [<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line">TRAIN: [<span class="number">0</span> <span class="number">1</span>] TEST: [<span class="number">2</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p><strong>kfold.split()将返回一个训练索引集合，一个验证索引集合</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"></span><br><span class="line">k_folds = <span class="number">5</span></span><br><span class="line">kfold = KFold(n_splits=k_folds, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#trainin：</span></span><br><span class="line"><span class="keyword">for</span> fold, (train_ids,valid_ids) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(train_val_dataset)):</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Code-Trick"><a href="#Code-Trick" class="headerlink" title="Code Trick"></a>Code Trick</h1><ol><li>固定随机种子seed</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seed_everything</span>(<span class="params">seed</span>):</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># LHY版本</span></span><br><span class="line">myseed = <span class="number">6666</span>  <span class="comment"># set a random seed for reproducibility</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">np.random.seed(myseed)      </span><br><span class="line">torch.manual_seed(myseed)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed_all(myseed)</span><br></pre></td></tr></table></figure><ol start="2"><li>查看模型信息summary</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">summary(model)</span><br></pre></td></tr></table></figure><ol start="3"><li>模型的参数冷冻</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否要冻住模型的前面一些层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_parameter_requires_grad</span>(<span class="params">model, feature_extracting</span>):</span><br><span class="line">    <span class="keyword">if</span> feature_extracting:</span><br><span class="line">        model = model</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span>   <span class="comment"># 因为是全局变量，设置为false就会冻住</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNeSt模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnest_model</span>(<span class="params">num_classes, feature_extract = <span class="literal">False</span></span>):</span><br><span class="line">    model_ft = resnest50(pretrained=<span class="literal">True</span>)</span><br><span class="line">    set_parameter_requires_grad(model_ft, feature_extract)</span><br><span class="line">    num_ftrs = model_ft.fc.in_features</span><br><span class="line">    model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, num_classes))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model_ft</span><br></pre></td></tr></table></figure><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><a href="https://www.kaggle.com/code/sheepwang/leaf-classification-eda-model">查看博主的论文inference</a></p><ol><li>EDA</li><li>Train trick</li></ol><h3 id="具体代码"><a href="#具体代码" class="headerlink" title="具体代码"></a>具体代码</h3><h4 id="training-epoch"><a href="#training-epoch" class="headerlink" title="training epoch"></a><strong>training epoch</strong></h4><p><strong>可以换成LHY的不进步多少个循环再停止</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------------------------------&#x27;</span>)</span><br><span class="line"><span class="comment">#使用sklearn分割数据集 #注意此处使用的特殊的数据集，kfold本质是对index处理</span></span><br><span class="line"><span class="keyword">for</span> fold, (train_ids,valid_ids) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(train_val_dataset)):</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;FOLD <span class="subst">&#123;fold&#125;</span>&#x27;</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;--------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#采样加入噪音</span></span><br><span class="line">  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)</span><br><span class="line">  valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#因为分割后返回的只是数据集的索引编号，所以需腰引入dataloader</span></span><br><span class="line">  trainloader = torch.utils.data.DataLoader(</span><br><span class="line">                      CutMix(TrainValidData(train_val_path, img_path, transform = train_transform), num_class=<span class="number">176</span>, beta=<span class="number">1.0</span>, prob=<span class="number">0.5</span>, num_mix=<span class="number">2</span>), </span><br><span class="line">                      batch_size=<span class="number">32</span>, sampler=train_subsampler, num_workers=<span class="number">0</span>)</span><br><span class="line">  validloader = torch.utils.data.DataLoader(</span><br><span class="line">                      TrainValidData(train_val_path, img_path, transform = val_test_transform),</span><br><span class="line">                      batch_size=<span class="number">32</span>, sampler=valid_subsampler, num_workers=<span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#放到gpu</span></span><br><span class="line">  model = resnest_model(<span class="number">176</span>)</span><br><span class="line">  model = model.to(device)</span><br><span class="line">  model.device = device</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#优化器和优化方案</span></span><br><span class="line">  optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay= weight_decay)</span><br><span class="line">  scheduler = CosineAnnealingLR(optimizer,T_max=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#训练循环</span></span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num_epochs):</span><br><span class="line">    </span><br><span class="line">    model.train()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Starting epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    train_losses = []</span><br><span class="line">    train_accs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(trainloader):</span><br><span class="line">  <span class="comment">#GPU</span></span><br><span class="line">      imgs, labels = batch</span><br><span class="line">      imgs = imgs.to(device)</span><br><span class="line">      labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">      logits = model(imgs)</span><br><span class="line">      loss = train_loss_function(logits,labels)</span><br><span class="line">      optimizer.zero_grad()</span><br><span class="line">      loss.backward()</span><br><span class="line">      optimizer.step()</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># Compute the accuracy for current batch. 注意这里是每个batch的loss</span></span><br><span class="line">      <span class="comment"># acc = (logits.argmax(dim=-1) == labels).float().mean()</span></span><br><span class="line">      <span class="comment"># Record the loss and accuracy.</span></span><br><span class="line">        </span><br><span class="line">      train_losses.append(loss.item())</span><br><span class="line">      <span class="comment"># train_accs.append(acc)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;第%d个epoch的学习率：%f&quot;</span> % (epoch+<span class="number">1</span>,optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    scheduler.step()</span><br><span class="line"><span class="comment"># 总loss / batch数</span></span><br><span class="line">    train_loss = np.<span class="built_in">sum</span>(train_losses) / <span class="built_in">len</span>(train_losses)</span><br><span class="line">    <span class="comment"># train_acc = np.sum(train_accs) / len(train_accs)</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[ Train | <span class="subst">&#123;epoch + <span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epochs:03d&#125;</span> ] loss = <span class="subst">&#123;train_loss:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train process (all epochs) is complete</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Training process has finished. Saving trained model.&#x27;</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Starting validation&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="validation"><a href="#validation" class="headerlink" title="validation"></a>validation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这一段代码跟上面是连着的</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 按K折存储模型</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;saving model with loss &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(train_loss))</span><br><span class="line">  save_path = <span class="string">f&#x27;./model-fold-<span class="subst">&#123;fold&#125;</span>.pth&#x27;</span></span><br><span class="line">  torch.save(model.state_dict(),save_path)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#开始验证</span></span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br><span class="line">  valid_losses = []</span><br><span class="line">  valid_accs = []</span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(validloader):</span><br><span class="line">      imgs, labels = batch</span><br><span class="line">      logits = model(imgs.to(device))</span><br><span class="line">      <span class="comment">#这里的loss-function用的是nn.CrossEntropyLoss()</span></span><br><span class="line">      loss = valid_loss_function(logits,labels.to(device)) </span><br><span class="line">      acc = (logits.argmax(dim=-<span class="number">1</span>) == labels.to(device)).<span class="built_in">float</span>().mean()</span><br><span class="line">      valid_losses.append(loss.item())        </span><br><span class="line">      valid_accs.append(acc)</span><br><span class="line"></span><br><span class="line">    valid_loss = np.<span class="built_in">sum</span>(valid_losses)/<span class="built_in">len</span>(valid_losses)</span><br><span class="line">    valid_acc = np.<span class="built_in">sum</span>(valid_accs)/<span class="built_in">len</span>(valid_accs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[ Valid | <span class="subst">&#123;epoch + <span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epochs:03d&#125;</span> ] loss = <span class="subst">&#123;valid_loss:<span class="number">.5</span>f&#125;</span>, </span></span><br><span class="line"><span class="string">          acc =<span class="subst">&#123;valid_acc:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy for fold %d: %d&#x27;</span> % (fold, valid_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------------------------------------&#x27;</span>)</span><br><span class="line">    results[fold] = valid_acc</span><br><span class="line">          </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;K-FOLD CROSS VALIDATION RESULTS FOR <span class="subst">&#123;k_folds&#125;</span> FOLDS&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------------------------&#x27;</span>)</span><br><span class="line">total_summation = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> results.items():</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;Fold <span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;value&#125;</span> &#x27;</span>)</span><br><span class="line">  total_summation += value</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Average: <span class="subst">&#123;total_summation/<span class="built_in">len</span>(results.items())&#125;</span> &#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="test-submission"><a href="#test-submission" class="headerlink" title="test submission"></a>test submission</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加入TTA数据增强包 不知道增强了什么</span></span><br><span class="line"><span class="keyword">import</span> ttach <span class="keyword">as</span> tta</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入测试数据集</span></span><br><span class="line">testloader = torch.utils.data.DataLoader(</span><br><span class="line">                      TestData(test_path, img_path, transform = val_test_transform),</span><br><span class="line">                      batch_size=<span class="number">32</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##模型预设</span></span><br><span class="line">model = resnest_model(<span class="number">176</span>)</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按K折载入模型</span></span><br><span class="line"><span class="keyword">for</span> test_fold <span class="keyword">in</span> <span class="built_in">range</span>(k_folds):</span><br><span class="line">  model_path = <span class="string">f&#x27;./model-fold-<span class="subst">&#123;test_fold&#125;</span>.pth&#x27;</span></span><br><span class="line">  saveFileName = <span class="string">f&#x27;./submission-fold-<span class="subst">&#123;test_fold&#125;</span>.csv&#x27;</span></span><br><span class="line">  model.load_state_dict(torch.load(model_path))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Some modules like Dropout or BatchNorm affect if the model is in training mode.</span></span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br><span class="line">  <span class="comment"># Test-Time Augmentation</span></span><br><span class="line">  tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(<span class="number">200</span>,<span class="number">200</span>)) </span><br><span class="line"></span><br><span class="line">  predictions = []</span><br><span class="line">  <span class="comment"># Iterate the testing set by batches.</span></span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(testloader):</span><br><span class="line">      </span><br><span class="line">      imgs = batch</span><br><span class="line">      <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">          logits = tta_model(imgs.to(device))</span><br><span class="line">      </span><br><span class="line">      predictions.extend(logits.argmax(dim=-<span class="number">1</span>).cpu().numpy().tolist())</span><br><span class="line"></span><br><span class="line">  preds = []</span><br><span class="line">  <span class="comment">#将序号转换成对应的叶子名称</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> predictions:</span><br><span class="line">      preds.append(num_to_class[i])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#存储预测结果</span></span><br><span class="line">  test_data = pd.read_csv(test_path)</span><br><span class="line">  test_data[<span class="string">&#x27;label&#x27;</span>] = pd.Series(preds)</span><br><span class="line">  submission = pd.concat([test_data[<span class="string">&#x27;image&#x27;</span>], test_data[<span class="string">&#x27;label&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">  submission.to_csv(saveFileName, index=<span class="literal">False</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="class-dataset"><a href="#class-dataset" class="headerlink" title="class dataset()"></a>class dataset()</h4><p>train&amp;valid</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 继承pytorch的dataset，创建自己的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainValidData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_path, file_path, resize_height=<span class="number">224</span>, resize_width=<span class="number">224</span>, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_path (string): csv 文件路径</span></span><br><span class="line"><span class="string">            img_path (string): 图像文件所在路径</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 需要调整后的照片尺寸，我这里每张图片的大小尺寸不一致#</span></span><br><span class="line">        self.resize_height = resize_height</span><br><span class="line">        self.resize_width = resize_width</span><br><span class="line"></span><br><span class="line">        self.file_path = file_path</span><br><span class="line">        self.to_tensor = transforms.ToTensor() <span class="comment">#将数据转换成tensor形式</span></span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取 csv 文件</span></span><br><span class="line">        <span class="comment"># 利用pandas读取csv文件</span></span><br><span class="line">        self.data_info = pd.read_csv(csv_path, header=<span class="literal">None</span>)  <span class="comment">#header=None是去掉表头部分</span></span><br><span class="line">        <span class="comment"># 文件第一列包含图像文件名称</span></span><br><span class="line">        self.image_arr = np.asarray(self.data_info.iloc[<span class="number">1</span>:,<span class="number">0</span>]) <span class="comment">#self.data_info.iloc[1:,0]表示读取第一列，从第二行开始一直读取到最后一行</span></span><br><span class="line">        <span class="comment"># 第二列是图像的 label</span></span><br><span class="line">        self.label_arr = np.asarray(self.data_info.iloc[<span class="number">1</span>:,<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 计算 length</span></span><br><span class="line">        self.data_len = <span class="built_in">len</span>(self.data_info.index) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># 从 image_arr中得到索引对应的文件名</span></span><br><span class="line">        single_image_name = self.image_arr[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取图像文件</span></span><br><span class="line">        img_as_img = Image.<span class="built_in">open</span>(self.file_path + single_image_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#如果需要将RGB三通道的图片转换成灰度图片可参考下面两行</span></span><br><span class="line">        <span class="comment"># if img_as_img.mode != &#x27;L&#x27;:</span></span><br><span class="line">        <span class="comment">#     img_as_img = img_as_img.convert(&#x27;L&#x27;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#设置好需要转换的变量，还可以包括一系列的nomarlize等等操作</span></span><br><span class="line">        transform = transforms.Compose([</span><br><span class="line">            transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">            transforms.ToTensor()</span><br><span class="line">        ])</span><br><span class="line">        img_as_img = transform(img_as_img)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到图像的 label</span></span><br><span class="line">        label = self.label_arr[index]</span><br><span class="line">        number_label = class_to_num[label]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (img_as_img, number_label)  <span class="comment">#返回每一个index对应的图片数据和对应的label</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_len</span><br></pre></td></tr></table></figure><p>test</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 继承pytorch的dataset，创建自己的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TestData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_path, file_path, resize_height=<span class="number">224</span>, resize_width=<span class="number">224</span>, transform = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_path (string): csv 文件路径</span></span><br><span class="line"><span class="string">            img_path (string): 图像文件所在路径</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 需要调整后的照片尺寸，我这里每张图片的大小尺寸不一致#</span></span><br><span class="line">        self.resize_height = resize_height</span><br><span class="line">        self.resize_width = resize_width</span><br><span class="line"></span><br><span class="line">        self.file_path = file_path</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.to_tensor = transforms.ToTensor() <span class="comment">#将数据转换成tensor形式</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取 csv 文件</span></span><br><span class="line">        <span class="comment"># 利用pandas读取csv文件</span></span><br><span class="line">        self.data_info = pd.read_csv(csv_path, header=<span class="literal">None</span>)  <span class="comment">#header=None是去掉表头部分</span></span><br><span class="line">        <span class="comment"># 文件第一列包含图像文件名称</span></span><br><span class="line">        self.image_arr = np.asarray(self.data_info.iloc[<span class="number">1</span>:,<span class="number">0</span>]) <span class="comment">#self.data_info.iloc[1:,0]表示读取第一列，从第二行开始一直读取到最后一行</span></span><br><span class="line">        <span class="comment"># 计算 length</span></span><br><span class="line">        self.data_len = <span class="built_in">len</span>(self.data_info.index) - <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># 从 image_arr中得到索引对应的文件名</span></span><br><span class="line">        single_image_name = self.image_arr[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取图像文件</span></span><br><span class="line">        img_as_img = Image.<span class="built_in">open</span>(self.file_path + single_image_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#如果需要将RGB三通道的图片转换成灰度图片可参考下面两行</span></span><br><span class="line">        <span class="comment"># if img_as_img.mode != &#x27;L&#x27;:</span></span><br><span class="line">        <span class="comment">#     img_as_img = img_as_img.convert(&#x27;L&#x27;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#设置好需要转换的变量，还可以包括一系列的nomarlize等等操作</span></span><br><span class="line">        transform = transforms.Compose([</span><br><span class="line">            transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">            transforms.ToTensor()</span><br><span class="line">        ])</span><br><span class="line">        img_as_img = transform(img_as_img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img_as_img</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_len</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/11/19/hello-world/"/>
      <url>/2022/11/19/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
