<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pipeline 句意相似度</title>
      <link href="/posts/10656.html"/>
      <url>/posts/10656.html</url>
      
        <content type="html"><![CDATA[<p><strong>主要进行训练框架优化</strong></p><ul><li>端到端 ML 实施（训练、验证、预测、评估）</li><li>轻松适应您自己的数据集</li><li>促进其他基于 BERT 的模型（BERT、ALBERT、…）的快速实验</li><li>使用有限的计算资源进行快速训练（混合精度、梯度累积……）</li><li>多 GPU 执行</li><li>分类决策的阈值选择（不一定是 0.5）</li><li>冻结 BERT 层，只更新分类层权重或更新所有权重</li><li>种子设置，可复现结果</li></ul><h1 id="PipeLine"><a href="#PipeLine" class="headerlink" title="PipeLine"></a>PipeLine</h1><h3 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, maxlen, with_labels=<span class="literal">True</span>, bert_model=<span class="string">&#x27;albert-base-v2&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">        self.data = data  <span class="comment"># pandas dataframe</span></span><br><span class="line">        <span class="comment">#Initialize the tokenizer</span></span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  </span><br><span class="line"></span><br><span class="line">        self.maxlen = maxlen</span><br><span class="line">        self.with_labels = with_labels </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#根据索引索取DataFrame中句子1余句子2</span></span><br><span class="line">        sent1 = <span class="built_in">str</span>(self.data.loc[index, <span class="string">&#x27;sentence1&#x27;</span>])</span><br><span class="line">        sent2 = <span class="built_in">str</span>(self.data.loc[index, <span class="string">&#x27;sentence2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对句子对分词，得到input_ids、attention_mask和token_type_ids</span></span><br><span class="line">        encoded_pair = self.tokenizer(sent1, sent2, </span><br><span class="line">                                      padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 填充到最大长度</span></span><br><span class="line">                                      truncation=<span class="literal">True</span>,  <span class="comment"># 根据最大长度进行截断</span></span><br><span class="line">                                      max_length=self.maxlen,  </span><br><span class="line">                                      return_tensors=<span class="string">&#x27;pt&#x27;</span>)  <span class="comment"># 返回torch.Tensor张量</span></span><br><span class="line">        </span><br><span class="line">        token_ids = encoded_pair[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment"># tensor token ids</span></span><br><span class="line">        attn_masks = encoded_pair[<span class="string">&#x27;attention_mask&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment"># padded values对应为 &quot;0&quot; ，其他token为1</span></span><br><span class="line">        token_type_ids = encoded_pair[<span class="string">&#x27;token_type_ids&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment">#第一个句子的值为0，第二个句子的值为1 # 只有一句全为0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.with_labels:  <span class="comment"># True if the dataset has labels</span></span><br><span class="line">            label = self.data.loc[index, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">            <span class="keyword">return</span> token_ids, attn_masks, token_type_ids, label  </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> token_ids, attn_masks, token_type_ids</span><br></pre></td></tr></table></figure><p>建议，进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample = <span class="built_in">next</span>(<span class="built_in">iter</span>(DataLoader(tr_dataset, batch_size=<span class="number">2</span>)))</span><br><span class="line">sample</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr_model = SentencePairClassifier(freeze_bert=True)</span><br><span class="line">tr_model(sample[0], sample[1], sample[2])</span><br></pre></td></tr></table></figure><p>就是方便最后的维度转换，squeeze、flatten、view；甚至可以用reshape方法</p><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentencePairClassifier</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert_model=<span class="string">&quot;albert-base-v2&quot;</span>, freeze_bert=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SentencePairClassifier, self).__init__()</span><br><span class="line">        <span class="comment">#  初始化预训练模型Bert xxx</span></span><br><span class="line">        self.bert_layer = AutoModel.from_pretrained(bert_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  encoder 隐藏层大小</span></span><br><span class="line">        <span class="keyword">if</span> bert_model == <span class="string">&quot;albert-base-v2&quot;</span>:  <span class="comment"># 12M 参数</span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-large-v2&quot;</span>:  <span class="comment"># 18M 参数</span></span><br><span class="line">            hidden_size = <span class="number">1024</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-xlarge-v2&quot;</span>:  <span class="comment"># 60M 参数</span></span><br><span class="line">            hidden_size = <span class="number">2048</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-xxlarge-v2&quot;</span>:  <span class="comment"># 235M 参数</span></span><br><span class="line">            hidden_size = <span class="number">4096</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="comment"># 110M 参数</span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;roberta-base&quot;</span>: <span class="comment"># </span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 固定Bert层 更新分类输出层</span></span><br><span class="line">        <span class="keyword">if</span> freeze_bert:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> self.bert_layer.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">False</span></span><br><span class="line">                </span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 分类输出</span></span><br><span class="line">        self.cls_layer = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">    @autocast()  </span><span class="comment"># 混合精度训练</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attn_masks, token_type_ids</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">            -input_ids : Tensor  containing token ids</span></span><br><span class="line"><span class="string">            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values</span></span><br><span class="line"><span class="string">            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入给Bert，获取上下文表示</span></span><br><span class="line">        <span class="comment"># cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)</span></span><br><span class="line">        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)</span><br><span class="line">        <span class="comment"># last_hidden_state,pooler_output,all_hidden_states 12层</span></span><br><span class="line">        <span class="comment"># 将last layer hidden-state of the [CLS] 输入到 classifier layer</span></span><br><span class="line">        <span class="comment"># - last_hidden_state 的向量平均</span></span><br><span class="line">        <span class="comment"># - 取all_hidden_states最后四层，然后做平均 weighted 平均</span></span><br><span class="line">        <span class="comment"># - last_hidden_state+lstm</span></span><br><span class="line">        <span class="comment"># 获取输出</span></span><br><span class="line">        logits = self.cls_layer(self.dropout(outputs[<span class="string">&#x27;pooler_output&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h3 id="固定随机种子"><a href="#固定随机种子" class="headerlink" title="固定随机种子"></a>固定随机种子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 固定随机种子，保证结果复现</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br></pre></td></tr></table></figure><h3 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!mkdir models <span class="comment">#可以在之前补充绝对路径</span></span><br><span class="line">!mkdir results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate</span>):</span><br><span class="line"></span><br><span class="line">    best_loss = np.Inf</span><br><span class="line">    best_ep = <span class="number">1</span></span><br><span class="line">    nb_iterations = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    print_every = nb_iterations // <span class="number">5</span>  <span class="comment"># 打印频率</span></span><br><span class="line">    iters = []</span><br><span class="line">    train_losses = []</span><br><span class="line">    val_losses = []</span><br><span class="line"></span><br><span class="line">    scaler = GradScaler()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line"></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> it, (seq, attn_masks, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_loader)):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 转为cuda张量</span></span><br><span class="line">            seq, attn_masks, token_type_ids, labels = \</span><br><span class="line">                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># 混合精度加速训练</span></span><br><span class="line">            <span class="keyword">with</span> autocast():</span><br><span class="line">                <span class="comment"># Obtaining the logits from the model</span></span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Computing loss</span></span><br><span class="line">                loss = criterion(logits.squeeze(-<span class="number">1</span>), labels.<span class="built_in">float</span>())</span><br><span class="line">                loss = loss / iters_to_accumulate  <span class="comment"># Normalize the loss because it is averaged</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backpropagating the gradients</span></span><br><span class="line">            <span class="comment"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class="line">            scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % iters_to_accumulate == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Optimization step</span></span><br><span class="line">                <span class="comment"># scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.</span></span><br><span class="line">                <span class="comment"># If these gradients do not contain infs or NaNs, opti.step() is then called,</span></span><br><span class="line">                <span class="comment"># otherwise, opti.step() is skipped.</span></span><br><span class="line">                scaler.step(opti)</span><br><span class="line">                <span class="comment"># Updates the scale for next iteration.</span></span><br><span class="line">                scaler.update()</span><br><span class="line">                <span class="comment"># 根据迭代次数调整学习率。</span></span><br><span class="line">                lr_scheduler.step()</span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                opti.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % print_every == <span class="number">0</span>:  <span class="comment"># Print training loss information</span></span><br><span class="line">                <span class="built_in">print</span>()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;it+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;nb_iterations&#125;</span> of epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete. \</span></span><br><span class="line"><span class="string">                Loss : <span class="subst">&#123;running_loss / print_every&#125;</span> &quot;</span>)</span><br><span class="line"></span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        val_loss = evaluate_loss(net, device, criterion, val_loader)  <span class="comment"># Compute validation loss</span></span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete! Validation Loss : <span class="subst">&#123;val_loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;</span>.<span class="built_in">format</span>(best_loss, val_loss))</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">            net_copy = copy.deepcopy(net)  <span class="comment"># # 保存最优模型</span></span><br><span class="line">            best_loss = val_loss</span><br><span class="line">            best_ep = ep + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    path_to_model=<span class="string">f&#x27;models/<span class="subst">&#123;bert_model&#125;</span>_lr_<span class="subst">&#123;lr&#125;</span>_val_loss_<span class="subst">&#123;<span class="built_in">round</span>(best_loss, <span class="number">5</span>)&#125;</span>_ep_<span class="subst">&#123;best_ep&#125;</span>.pt&#x27;</span></span><br><span class="line">    torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model has been saved in &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_model))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> loss</span><br><span class="line">    torch.cuda.empty_cache() <span class="comment"># 清空显存</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, device, criterion, dataloader</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    评估输出</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    mean_loss = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> it, (seq, attn_masks, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(dataloader)):</span><br><span class="line">            seq, attn_masks, token_type_ids, labels = \</span><br><span class="line">                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)</span><br><span class="line">            logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">            mean_loss += criterion(logits.squeeze(-<span class="number">1</span>), labels.<span class="built_in">float</span>()).item()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mean_loss / count</span><br></pre></td></tr></table></figure><ol><li><p>注意autocast和累计梯度 这两种加速计算的方法</p></li><li><p>evaluate的时候要注意数据的维度，标签的类型</p></li></ol><h3 id="超参数-amp-开始训练"><a href="#超参数-amp-开始训练" class="headerlink" title="超参数 &amp; 开始训练"></a>超参数 &amp; 开始训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bert_model = <span class="string">&quot;albert-base-v2&quot;</span>  <span class="comment"># &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;</span></span><br><span class="line">freeze_bert = <span class="literal">False</span>  <span class="comment"># 是否冻结Bert</span></span><br><span class="line">maxlen = <span class="number">128</span>  <span class="comment"># 最大长度</span></span><br><span class="line">bs = <span class="number">16</span>  <span class="comment"># batch size</span></span><br><span class="line">iters_to_accumulate = <span class="number">2</span>  <span class="comment"># 梯度累加</span></span><br><span class="line">lr = <span class="number">2e-5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># 训练轮数</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  固定随机种子 便于复现</span></span><br><span class="line">set_seed(<span class="number">1</span>) <span class="comment"># 2022 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集与验证集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading training data...&quot;</span>)</span><br><span class="line">train_set = CustomDataset(df_train, maxlen, bert_model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading validation data...&quot;</span>)</span><br><span class="line">val_set = CustomDataset(df_val, maxlen, bert_model)</span><br><span class="line"><span class="comment"># 常见训练集与验证集DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  <span class="comment"># if multiple GPUs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    net = nn.DataParallel(net)</span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">opti = AdamW(net.parameters(), lr=lr, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line">num_warmup_steps = <span class="number">0</span> <span class="comment"># The number of steps for the warmup phase.</span></span><br><span class="line">num_training_steps = epochs * <span class="built_in">len</span>(train_loader)  <span class="comment"># The total number of training steps</span></span><br><span class="line">t_total = (<span class="built_in">len</span>(train_loader) // iters_to_accumulate) * epochs  <span class="comment"># Necessary to take into account Gradient accumulation</span></span><br><span class="line">lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)</span><br><span class="line"></span><br><span class="line">train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)</span><br></pre></td></tr></table></figure><ol><li>注意多gpu训练 <code>torch.cuda.device_count() &gt; 1</code>, <code>net = nn.DataParallel(net)</code>的使用</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_probs_from_logits</span>(<span class="params">logits</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Converts a tensor of logits into an array of probabilities by applying the sigmoid function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    probs = torch.sigmoid(logits.unsqueeze(-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> probs.detach().cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_prediction</span>(<span class="params">net, device, dataloader, with_labels=<span class="literal">True</span>, result_file=<span class="string">&quot;results/output.txt&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Predict the probabilities on a dataset with or without labels and print the result in a file</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    w = <span class="built_in">open</span>(result_file, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    probs_all = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> with_labels:</span><br><span class="line">            <span class="keyword">for</span> seq, attn_masks, token_type_ids, _ <span class="keyword">in</span> tqdm(dataloader):<span class="comment"># 训练集、验证集</span></span><br><span class="line">                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)</span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">                probs = get_probs_from_logits(logits.squeeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                probs_all += probs.tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> seq, attn_masks, token_type_ids <span class="keyword">in</span> tqdm(dataloader): <span class="comment"># 没有标签的测试集</span></span><br><span class="line">                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)</span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">                probs = get_probs_from_logits(logits.squeeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                probs_all += probs.tolist()</span><br><span class="line"></span><br><span class="line">    w.writelines(<span class="built_in">str</span>(prob)+<span class="string">&#x27;\n&#x27;</span> <span class="keyword">for</span> prob <span class="keyword">in</span> probs_all)</span><br><span class="line">    w.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">path_to_model = <span class="string">&#x27;./model&#x27;</span>  </span><br><span class="line"><span class="comment"># path_to_model = &#x27;/content/models/...&#x27;  # You can add here your trained model</span></span><br><span class="line"></span><br><span class="line">path_to_output_file = <span class="string">&#x27;./results&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading test data...&quot;</span>)</span><br><span class="line">test_set = CustomDataset(df_test, maxlen, bert_model)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model = SentencePairClassifier(bert_model)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  <span class="comment"># if multiple GPUs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loading the weights of the model...&quot;</span>)</span><br><span class="line">model.load_state_dict(torch.load(path_to_model))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicting on test data...&quot;</span>)</span><br><span class="line">test_prediction(net=model, device=device, dataloader=test_loader, with_labels=<span class="literal">True</span>,  <span class="comment"># set the with_labels parameter to False if your want to get predictions on a dataset without labels</span></span><br><span class="line">                result_file=path_to_output_file)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predictions are available in : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_output_file))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">path_to_output_file = <span class="string">&#x27;results/output.txt&#x27;</span>  <span class="comment"># 预测结果概率文件</span></span><br><span class="line"></span><br><span class="line">labels_test = df_test[<span class="string">&#x27;label&#x27;</span>]  <span class="comment"># true labels</span></span><br><span class="line"></span><br><span class="line">probs_test = pd.read_csv(path_to_output_file, header=<span class="literal">None</span>)[<span class="number">0</span>]  <span class="comment"># 预测概率</span></span><br><span class="line">threshold = <span class="number">0.6</span>   <span class="comment"># you can adjust this threshold for your own dataset</span></span><br><span class="line">preds_test=(probs_test&gt;=threshold).astype(<span class="string">&#x27;uint8&#x27;</span>) <span class="comment"># predicted labels using the above fixed threshold</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># metric = load_metric(&quot;glue&quot;, &quot;mrpc&quot;)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 句意相似度 </tag>
            
            <tag> Pipeline </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weight &amp; Bias</title>
      <link href="/posts/26087.html"/>
      <url>/posts/26087.html</url>
      
        <content type="html"><![CDATA[<h1 id="torch-inference-mode"><a href="#torch-inference-mode" class="headerlink" title="torch.inference_mode()"></a><code>torch.inference_mode()</code></h1><p>with no_gradient的一种加速  <a href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html">参考文档</a></p><h1 id="nn-MarginRankingLoss"><a href="#nn-MarginRankingLoss" class="headerlink" title=" nn.MarginRankingLoss()"></a><code> nn.MarginRankingLoss()</code></h1><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html">文档</a> margin &#x3D; 0  x1大于x2 则去-y，viceversa 取 y</p><p>*<em>loss(<em>x</em>1,<em>x</em>2,<em>y</em>)&#x3D;max(0,−</em>y<em>∗(<em>x</em>1−</em>x<em>2)+margin)</em>*</p><p>这里最后的loss是平均后的</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MarginRankingLoss()</span><br><span class="line">input1 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">input2 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>).sign()</span><br><span class="line">output = loss(input1, input2, target)</span><br><span class="line">output.backward()</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">input1, input2, target, output</span><br><span class="line"></span><br><span class="line">(tensor([ <span class="number">0.0277</span>, -<span class="number">0.3806</span>,  <span class="number">1.0405</span>], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> tensor([-<span class="number">0.9075</span>,  <span class="number">0.3271</span>,  <span class="number">0.1156</span>], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> tensor([ <span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]),</span><br><span class="line"> tensor(<span class="number">0.3083</span>, grad_fn=&lt;MeanBackward0&gt;))</span><br><span class="line"> </span><br><span class="line">input1 - input2 , (input1 - input2) * (-target)</span><br><span class="line"></span><br><span class="line">(tensor([ <span class="number">0.9352</span>, -<span class="number">0.7077</span>,  <span class="number">0.9249</span>], grad_fn=&lt;SubBackward0&gt;),</span><br><span class="line"> tensor([-<span class="number">0.9352</span>, -<span class="number">0.7077</span>,  <span class="number">0.9249</span>], grad_fn=&lt;MulBackward0&gt;),</span><br><span class="line"> </span><br><span class="line">loss = <span class="number">0.9249</span>/<span class="number">3</span> </span><br><span class="line"></span><br><span class="line">```</span><br></pre></td></tr></table></figure><h1 id="gc-collect"><a href="#gc-collect" class="headerlink" title="gc.collect()"></a><code>gc.collect()</code></h1><p>清除内存</p><h1 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a><code>defaultdict</code></h1><p>获得创建key不给value也不报错的dict</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">history = defaultdict(list)</span><br><span class="line"></span><br><span class="line">history[&#x27;Train Loss&#x27;].append(1.1)</span><br></pre></td></tr></table></figure><h1 id="StratifiedKFold"><a href="#StratifiedKFold" class="headerlink" title="StratifiedKFold()"></a><code>StratifiedKFold()</code></h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold, KFold</span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=CONFIG[<span class="string">&#x27;n_fold&#x27;</span>], shuffle=<span class="literal">True</span>, random_state=CONFIG[<span class="string">&#x27;seed&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fold, ( _, val_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf.split(X=df, y=df.worker)):</span><br><span class="line">    df.loc[val_ , <span class="string">&quot;kfold&quot;</span>] = <span class="built_in">int</span>(fold)</span><br><span class="line">    </span><br><span class="line">df[<span class="string">&quot;kfold&quot;</span>] = df[<span class="string">&quot;kfold&quot;</span>].astype(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure><p>第五行 将X分k折，y标签为样本对应index，fold 在 0~5</p><p>得到df[“kfold”] 列包含 属于第几折的 valid数据</p><p>通过下面的函数直接选择<strong>非本折的数据作为train</strong>，其他的就是valid</p><p><code>df_train = df[df.kfold != fold].reset_index(drop=True) df_valid = df[df.kfold == fold].reset_index(drop=True)</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_loaders</span>(<span class="params">fold</span>):</span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG[<span class="string">&#x27;tokenizer&#x27;</span>], max_length=CONFIG[<span class="string">&#x27;max_length&#x27;</span>])</span><br><span class="line">    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG[<span class="string">&#x27;tokenizer&#x27;</span>], max_length=CONFIG[<span class="string">&#x27;max_length&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(train_dataset, batch_size=CONFIG[<span class="string">&#x27;train_batch_size&#x27;</span>], </span><br><span class="line">                              num_workers=<span class="number">2</span>, shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[<span class="string">&#x27;valid_batch_size&#x27;</span>], </span><br><span class="line">                              num_workers=<span class="number">2</span>, shuffle=<span class="literal">False</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_loader, valid_loade</span><br></pre></td></tr></table></figure><h1 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><code>tqdm</code></h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bar = tqdm(enumerate(dataloader), total=len(dataloader))</span><br></pre></td></tr></table></figure><p>单个epoch下面对bar做如下设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,</span><br><span class="line">                        LR=optimizer.param_groups[0][&#x27;lr&#x27;])  </span><br></pre></td></tr></table></figure><h1 id="Weights-amp-Biases-W-amp-B"><a href="#Weights-amp-Biases-W-amp-B" class="headerlink" title="Weights &amp; Biases (W&amp;B) "></a><code>Weights &amp; Biases (W&amp;B) </code></h1><ul><li><p>hash 一个项目id</p></li><li><p>train valid 定义一个 1个epoch 的函数 返回 分别其中的loss</p></li><li><p>wandb.log({“Train Loss”: train_epoch_loss}) 使用 log 方式记录 损失函数</p></li><li><pre><code class="py">run = wandb.init(project=&#39;Jigsaw&#39;,                      config=CONFIG,                     job_type=&#39;Train&#39;,                     group=CONFIG[&#39;group&#39;],                     tags=[&#39;roberta-base&#39;, f&#39;&#123;HASH_NAME&#125;&#39;, &#39;margin-loss&#39;],                     name=f&#39;&#123;HASH_NAME&#125;-fold-&#123;fold&#125;&#39;,                     anonymous=&#39;must&#39;)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TRAIN PART</span><br></pre></td></tr></table></figure>run.finish()<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">显示如下</span><br><span class="line">&#x27;hash--------name&#x27;</span><br><span class="line">Syncing run k5nu8k69390a-fold-0 to Weights &amp; Biases (docs).</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre></li></ul><h1 id="流程训练提炼"><a href="#流程训练提炼" class="headerlink" title="流程训练提炼"></a>流程训练提炼</h1><ul><li>for fold in range(0, CONFIG[‘n_fold’])</li><li>wandb.init</li><li>prepare_loaders、fetch_scheduler</li><li>run_training<ul><li>train_one_epoch、valid_one_epoch —-&gt; to got model, loss for wandb</li></ul></li></ul><p>中间掺杂 W&amp;B 的数据实时载入分析即可</p><p>df[‘y’].value_counts(normalize&#x3D;True) to got the percentage of each values</p><p><a href="https://www.kaggle.com/code/debarshichanda/pytorch-w-b-jigsaw-starter">原文链接</a></p>]]></content>
      
      
      <categories>
          
          <category> Trick </category>
          
      </categories>
      
      
        <tags>
            
            <tag> wandb </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV 02 Vit 叶子图片分类</title>
      <link href="/posts/28702.html"/>
      <url>/posts/28702.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Vit——Vision Transformer</p><p>这里通过<a href="https://www.kaggle.com/competitions/classify-leaves/data">kaggle的叶子分类任务</a>来使用预训练(Pre-train)模型Vit来提升我们的任务表示</p><h1 id="1-观察模型-amp-处理数据"><a href="#1-观察模型-amp-处理数据" class="headerlink" title="1.观察模型&amp;处理数据"></a>1.观察模型&amp;处理数据</h1><h2 id="1-1-模型探索"><a href="#1-1-模型探索" class="headerlink" title="1.1 模型探索"></a>1.1 模型探索</h2><p>无论是基于python的特性(适配各个领域的包)，还是NLP里大行其道的Pre-train范式，拥有快速了解一个包的特性以适用于我们工作的能力，将极大的提升我们工作的效率和结果。所以下面我们来快速体验一下<a href="https://huggingface.co/google/vit-base-patch16-224">HuggingFace给出的模型范例</a>，并针对我们的任务进行相应的数据处理。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ViTFeatureExtractor, ViTForImageClassification</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(requests.get(url, stream=<span class="literal">True</span>).raw)</span><br><span class="line"></span><br><span class="line">feature_extractor = ViTFeatureExtractor.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line">model = ViTForImageClassification.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line"></span><br><span class="line">inputs = feature_extractor(images=image, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">logits = outputs.logits</span><br><span class="line"><span class="comment"># model predicts one of the 1000 ImageNet classes</span></span><br><span class="line">predicted_class_idx = logits.argmax(-<span class="number">1</span>).item()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上面的代码可以自行运行</p><h3 id="1-1-1-示例解读"><a href="#1-1-1-示例解读" class="headerlink" title="1.1.1 示例解读"></a>1.1.1 示例解读</h3><ul><li><p>上十行代码: 首先通过requests库拿到一张图片并用image生成图片形式，下面两行加载了Vit16的特征提取器和HF特供的图片分类适配模型</p></li><li><p>下面我们看看 特征提取后的输入(inputs)</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># inputs 输出如下</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;pixel_values&#x27;: tensor([[[[ 0.1137,  0.1686,  0.1843,  ..., -0.1922, -0.1843, -0.1843],</span></span><br><span class="line"><span class="string">          [ 0.1373,  0.1686,  0.1843,  ..., -0.1922, -0.1922, -0.2078],</span></span><br><span class="line"><span class="string">          [ 0.1137,  0.1529,  0.1608,  ..., -0.2314, -0.2235, -0.2157],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.8353,  0.7882,  0.7333,  ...,  0.7020,  0.6471,  0.6157],</span></span><br><span class="line"><span class="string">          [ 0.8275,  0.7961,  0.7725,  ...,  0.5843,  0.4667,  0.3961],</span></span><br><span class="line"><span class="string">          [ 0.8196,  0.7569,  0.7569,  ...,  0.0745, -0.0510, -0.1922]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-0.8039, -0.8118, -0.8118,  ..., -0.8902, -0.8902, -0.8980],</span></span><br><span class="line"><span class="string">          [-0.7882, -0.7882, -0.7882,  ..., -0.8745, -0.8745, -0.8824],</span></span><br><span class="line"><span class="string">          [-0.8118, -0.8039, -0.7882,  ..., -0.8902, -0.8902, -0.8902],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [-0.2706, -0.3176, -0.3647,  ..., -0.4275, -0.4588, -0.4824],</span></span><br><span class="line"><span class="string">          [-0.2706, -0.2941, -0.3412,  ..., -0.4824, -0.5451, -0.5765],</span></span><br><span class="line"><span class="string">          [-0.2784, -0.3412, -0.3490,  ..., -0.7333, -0.7804, -0.8353]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-0.5451, -0.4667, -0.4824,  ..., -0.7412, -0.6941, -0.7176],</span></span><br><span class="line"><span class="string">          [-0.5529, -0.5137, -0.4902,  ..., -0.7412, -0.7098, -0.7412],</span></span><br><span class="line"><span class="string">          [-0.5216, -0.4824, -0.4667,  ..., -0.7490, -0.7490, -0.7647],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.5686,  0.5529,  0.4510,  ...,  0.4431,  0.3882,  0.3255],</span></span><br><span class="line"><span class="string">          [ 0.5451,  0.4902,  0.5137,  ...,  0.3020,  0.2078,  0.1294],</span></span><br><span class="line"><span class="string">          [ 0.5686,  0.5608,  0.5137,  ..., -0.2000, -0.4275, -0.5294]]]])&#125;</span></span><br><span class="line"><span class="string">          &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">inputs[<span class="string">&#x27;pixel_values&#x27;</span>].size()</span><br><span class="line"><span class="comment"># torch.Size([1, 3, 224, 224])</span></span><br></pre></td></tr></table></figure><p>可以看到是一个字典类型的tensor数据，其维度为(b, C, W, H)</p><p><strong>因此我们喂给模型的数据也得是四维的结构</strong></p></li><li><p>接下来看看模型吐出来的结果</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># outputs 输入如下</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">MaskedLMOutput(loss=tensor(0.4776, grad_fn=&lt;DivBackward0&gt;), logits=tensor([[[[-0.0630, -0.0475, -0.1557,  ...,  0.0950,  0.0216, -0.0084],</span></span><br><span class="line"><span class="string">          [-0.1219, -0.0329, -0.0849,  ..., -0.0152, -0.0143, -0.0663],</span></span><br><span class="line"><span class="string">          [-0.1063, -0.0925, -0.0350,  ...,  0.0238, -0.0206, -0.2159],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.2204,  0.0593, -0.2771,  ...,  0.0819,  0.0535, -0.1783],</span></span><br><span class="line"><span class="string">          [-0.0302, -0.1537, -0.1370,  ..., -0.1245, -0.1181, -0.0070],</span></span><br><span class="line"><span class="string">          [ 0.0875,  0.0626, -0.0693,  ...,  0.1331,  0.1088, -0.0835]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.1977, -0.2163,  0.0469,  ...,  0.0802, -0.0414,  0.0552],</span></span><br><span class="line"><span class="string">          [ 0.1125, -0.0369,  0.0175,  ...,  0.0598, -0.0843,  0.0774],</span></span><br><span class="line"><span class="string">          [ 0.1559, -0.0994, -0.0055,  ..., -0.0215,  0.2452, -0.0603],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.0603,  0.1887,  0.2060,  ...,  0.0415, -0.0383,  0.0990],</span></span><br><span class="line"><span class="string">          [ 0.2106,  0.0992, -0.1562,  ..., -0.1254, -0.0603,  0.0685],</span></span><br><span class="line"><span class="string">          [ 0.0256,  0.1578,  0.0304,  ..., -0.0894,  0.0659,  0.1493]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-0.0348, -0.0362, -0.1617,  ...,  0.0527,  0.1927,  0.1431],</span></span><br><span class="line"><span class="string">          [-0.0447,  0.0137, -0.0798,  ...,  0.1057, -0.0299, -0.0742],</span></span><br><span class="line"><span class="string">          [-0.0725,  0.1473, -0.0118,  ..., -0.1284,  0.0010, -0.0773],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [-0.0315,  0.1065, -0.1130,  ...,  0.0091, -0.0650,  0.0688],</span></span><br><span class="line"><span class="string">          [ 0.0314,  0.1034, -0.0964,  ...,  0.0144,  0.0532, -0.0415],</span></span><br><span class="line"><span class="string">          [-0.0205,  0.0046, -0.0987,  ...,  0.1317, -0.0065, -0.1617]]]],</span></span><br><span class="line"><span class="string">       grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>可以看到有loss、logits、hidden_states、attentions，而我们的范例只取了logits作为结果输出。这里并不是说其他的部分没用，是只取适配下游任务的输出即可。<a href="https://arxiv.org/abs/2010.11929">详情可研究Vit的论文</a></p></li><li><p>最后通过<code>argmax</code>函数和<code>model.config.id2label</code>得出标签相对应的文字</p><p>argmax就是返回最大值的位置下标、model.config.id2label配置了对应标签的名称，也知道了最后的classifier层是1000维的</p></li></ul><h3 id="1-1-2-小结"><a href="#1-1-2-小结" class="headerlink" title="1.1.2 小结"></a>1.1.2 小结</h3><p>通过以上探索，我们可以得出：</p><ol><li>输入的维度为(batch_size, 3, 224, 224)</li><li>最后的classifier需由1000改成我们叶子的类别数</li></ol><h2 id="1-2-数据处理"><a href="#1-2-数据处理" class="headerlink" title="1.2 数据处理"></a>1.2 数据处理</h2><p>接下来我们将探索数据的特性，并修改以适应我们的模型</p><h3 id="1-2-1-EDA"><a href="#1-2-1-EDA" class="headerlink" title="1.2.1 EDA"></a>1.2.1 EDA</h3><p>即Exploratory Data Analysis</p><p>首先导入所需包</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入各种包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastprogress.fastprogress <span class="keyword">import</span> master_bar, progress_bar</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> CosineAnnealingLR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (AdamW, get_scheduler)</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ViTFeatureExtractor, ViTForImageClassification</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看初始数据</p><p><code>train_df = pd.read_csv(&#39;/kaggle/input/classify-leaves/train.csv&#39;)</code></p><p>![](.&#x2F;img&#x2F;leaves classifier df 01.png)</p><p>使用下面代码给分类配上序号</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">num_map</span>(<span class="params">file_path</span>):</span><br><span class="line">    data_df = pd.read_csv(<span class="string">&#x27;/kaggle/input/classify-leaves/train.csv&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    categories = data_df.label.unique().tolist()</span><br><span class="line">    categories_zip = <span class="built_in">list</span>(<span class="built_in">zip</span>( <span class="built_in">range</span>(<span class="built_in">len</span>(categories)) , categories))</span><br><span class="line">    categories_dict = &#123;v:k <span class="keyword">for</span> k, v <span class="keyword">in</span> categories_zip&#125;</span><br><span class="line">    </span><br><span class="line">    data_df[<span class="string">&#x27;num_label&#x27;</span>] = data_df.label.<span class="built_in">map</span>(categories_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_df</span><br><span class="line">show_df = num_map(<span class="string">&#x27;/kaggle/input/classify-leaves/train.csv&#x27;</span>)</span><br><span class="line">show_df.to_csv(<span class="string">&#x27;train_valid_dataset.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><img src="./img/leaves classifier df 02.png" style="zoom:80%;" /><h3 id="1-2-2-图片数据查看"><a href="#1-2-2-图片数据查看" class="headerlink" title="1.2.2 图片数据查看"></a>1.2.2 图片数据查看</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">&#x27;/kaggle/input/classify-leaves/&#x27;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(path+train_df.image[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.figure(&quot;Image&quot;) # 图像窗口名称</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># 关掉坐标轴为 off</span></span><br><span class="line">plt.title(<span class="string">&#x27;image&#x27;</span>) <span class="comment"># 图像题目</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>![](.&#x2F;img&#x2F;leaves classifier df 03 .png)</p><p>这里我们做一下维度转换 即 [0, 1, 2]  换成 [2, 1, 0], 并只取某一个通道 看看</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.asarray(img).shape</span></span><br><span class="line"><span class="comment"># 可以看到图片反了，正确的顺序是.transpose([2, 0, 1])</span></span><br><span class="line">img_trans = np.asarray(img).transpose([<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">plt.imshow(img_trans[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>![](.&#x2F;img&#x2F;leaves classifier df 04 .png)</p><h1 id="2-Preprocessing"><a href="#2-Preprocessing" class="headerlink" title="2.Preprocessing"></a>2.Preprocessing</h1><p>接下来我们分别要做 数据增强、数据类定义、数据加载器测试</p><h3 id="2-1-1-先来算个平均值标准差"><a href="#2-1-1-先来算个平均值标准差" class="headerlink" title="2.1.1 先来算个平均值标准差"></a>2.1.1 先来算个平均值标准差</h3><p>这里算的mean跟std是为了Normalize我们的数据使训练更稳定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_list</span>(<span class="params">img_dir, isclasses=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将图像的名称列表</span></span><br><span class="line"><span class="string">    args: img_dir:存放图片的目录</span></span><br><span class="line"><span class="string">          isclasses:图片是否按类别存放标志</span></span><br><span class="line"><span class="string">    return: 图片文件名称列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 路径下图像是否按类别分类存放</span></span><br><span class="line">    <span class="keyword">if</span> isclasses:</span><br><span class="line">        img_file = os.listdir(img_dir)</span><br><span class="line">        <span class="keyword">for</span> class_name <span class="keyword">in</span> img_file:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(os.path.join(img_dir, class_name)):     </span><br><span class="line">                class_img_list = os.listdir(os.path.join(img_dir, class_name))</span><br><span class="line">                img_list.extend(class_img_list)         </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_list = os.listdir(img_dir)</span><br><span class="line">    <span class="built_in">print</span>(img_list)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;image numbers: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_list)))</span><br><span class="line">    <span class="keyword">return</span> img_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_pixel_mean</span>(<span class="params">img_dir, img_list, img_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;求数据集图像的R、G、B均值</span></span><br><span class="line"><span class="string">    args: img_dir:</span></span><br><span class="line"><span class="string">          img_list:</span></span><br><span class="line"><span class="string">          img_size:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    R_sum = <span class="number">0</span></span><br><span class="line">    G_sum = <span class="number">0</span></span><br><span class="line">    B_sum = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环读取所有图片</span></span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> img_list:</span><br><span class="line">        img_path = os.path.join(img_dir, img_name)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(img_path):</span><br><span class="line">            image = cv2.imread(img_path)</span><br><span class="line">            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">            image = cv2.resize(image, (img_size, img_size))      <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">            R_sum += image[:, :, <span class="number">0</span>].mean()</span><br><span class="line">            G_sum += image[:, :, <span class="number">1</span>].mean()</span><br><span class="line">            B_sum += image[:, :, <span class="number">2</span>].mean()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    R_mean = R_sum / count</span><br><span class="line">    G_mean = G_sum / count</span><br><span class="line">    B_mean = B_sum / count</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;R_mean:&#123;&#125;, G_mean:&#123;&#125;, B_mean:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(R_mean,G_mean,B_mean))</span><br><span class="line">    RGB_mean = [R_mean, G_mean, B_mean]</span><br><span class="line">    <span class="keyword">return</span> RGB_mean</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_pixel_std</span>(<span class="params">img_dir, img_mean, img_list, img_size</span>):</span><br><span class="line">    R_squared_mean = <span class="number">0</span></span><br><span class="line">    G_squared_mean = <span class="number">0</span></span><br><span class="line">    B_squared_mean = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    image_mean = np.array(img_mean)</span><br><span class="line">    <span class="comment"># 循环读取所有图片</span></span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> img_list:</span><br><span class="line">        img_path = os.path.join(img_dir, img_name)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(img_path):</span><br><span class="line">            image = cv2.imread(img_path)    <span class="comment"># 读取图片</span></span><br><span class="line">            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">            image = cv2.resize(image, (img_size, img_size))      <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">            image = image - image_mean    <span class="comment"># 零均值</span></span><br><span class="line">            <span class="comment"># 求单张图片的方差</span></span><br><span class="line">            R_squared_mean += np.mean(np.square(image[:, :, <span class="number">0</span>]).flatten())</span><br><span class="line">            G_squared_mean += np.mean(np.square(image[:, :, <span class="number">1</span>]).flatten())</span><br><span class="line">            B_squared_mean += np.mean(np.square(image[:, :, <span class="number">2</span>]).flatten())</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    R_std = math.sqrt(R_squared_mean / count)</span><br><span class="line">    G_std = math.sqrt(G_squared_mean / count)</span><br><span class="line">    B_std = math.sqrt(B_squared_mean / count)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;R_std:&#123;&#125;, G_std:&#123;&#125;, B_std:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(R_std, G_std, B_std))</span><br><span class="line">    RGB_std = [R_std, G_std, B_std]</span><br><span class="line">    <span class="keyword">return</span> RGB_std</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    image_dir = <span class="string">&#x27;/图片文件路径&#x27;</span></span><br><span class="line">    image_list = get_image_list(image_dir, isclasses=<span class="literal">False</span>)</span><br><span class="line">    RGB_mean = get_image_pixel_mean(image_dir, image_list, img_size=<span class="number">224</span>)</span><br><span class="line">    get_image_pixel_std(image_dir, RGB_mean, image_list, img_size=<span class="number">224</span>)```</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-1-2-数据增强"><a href="#2-1-2-数据增强" class="headerlink" title="2.1.2 数据增强"></a>2.1.2 数据增强</h3><p><code>transforms.Compose</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])</span><br></pre></td></tr></table></figure><p><a href="https://pytorch.org/vision/stable/transforms.html#compositions-of-transforms">见官网</a></p><h3 id="2-2-1-Dataset"><a href="#2-2-1-Dataset" class="headerlink" title="2.2.1 Dataset"></a>2.2.1 Dataset</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">imgdataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, os_path, file_path, transform </span>):</span><br><span class="line">        self.os_path = os_path</span><br><span class="line">        self.data = pd.read_csv(file_path)</span><br><span class="line">        self.transform = transform</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img = Image.<span class="built_in">open</span>(self.os_path + self.data.image[idx])</span><br><span class="line">        label = self.data.num_label[idx]</span><br><span class="line">        </span><br><span class="line">        self.transform != <span class="literal">None</span>:</span><br><span class="line">        img = self.transform(img)</span><br><span class="line">        label = torch.tensor(label)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="2-2-2-模型测试"><a href="#2-2-2-模型测试" class="headerlink" title="2.2.2 模型测试"></a>2.2.2 模型测试</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = imgdataset(<span class="string">&#x27;/kaggle/input/classify-leaves/&#x27;</span>, </span><br><span class="line">                           <span class="string">&#x27;/kaggle/working/processed_train.csv&#x27;</span>,</span><br><span class="line">                          transform = transform)</span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size = <span class="number">1</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">samples = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line">samples[<span class="number">0</span>], samples[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">``` 输入如下</span><br><span class="line">(tensor([[[[<span class="number">0.7608</span>, <span class="number">0.7608</span>, <span class="number">0.7608</span>,  ..., <span class="number">0.8353</span>, <span class="number">0.8353</span>, <span class="number">0.8392</span>],</span><br><span class="line">           [<span class="number">0.7608</span>, <span class="number">0.7608</span>, <span class="number">0.7608</span>,  ..., <span class="number">0.8392</span>, <span class="number">0.8353</span>, <span class="number">0.8431</span>],</span><br><span class="line">           [<span class="number">0.7608</span>, <span class="number">0.7608</span>, <span class="number">0.7608</span>,  ..., <span class="number">0.8392</span>, <span class="number">0.8392</span>, <span class="number">0.8431</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>]],</span><br><span class="line"> </span><br><span class="line">          [[<span class="number">0.8118</span>, <span class="number">0.8118</span>, <span class="number">0.8118</span>,  ..., <span class="number">0.8588</span>, <span class="number">0.8588</span>, <span class="number">0.8627</span>],</span><br><span class="line">           [<span class="number">0.8118</span>, <span class="number">0.8118</span>, <span class="number">0.8118</span>,  ..., <span class="number">0.8627</span>, <span class="number">0.8588</span>, <span class="number">0.8667</span>],</span><br><span class="line">           [<span class="number">0.8118</span>, <span class="number">0.8118</span>, <span class="number">0.8118</span>,  ..., <span class="number">0.8627</span>, <span class="number">0.8627</span>, <span class="number">0.8667</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>]],</span><br><span class="line"> </span><br><span class="line">          [[<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.8510</span>, <span class="number">0.8510</span>, <span class="number">0.8549</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.8549</span>, <span class="number">0.8510</span>, <span class="number">0.8588</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.8549</span>, <span class="number">0.8549</span>, <span class="number">0.8588</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>]]]]),</span><br><span class="line"> tensor([<span class="number">55</span>])) ```</span><br></pre></td></tr></table></figure><p>这里可以直接看到transforms的ToTensor方式已经将我们的数据修改乘(C, W, H)形式（原来的是 C在最后）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_ot = model(samples[<span class="number">0</span>])</span><br><span class="line">test_pred = test_ot.logits.argmax(-<span class="number">1</span>)</span><br><span class="line">test_pred, test_ot.logits</span><br></pre></td></tr></table></figure><h1 id="3-训练循环"><a href="#3-训练循环" class="headerlink" title="3. 训练循环"></a>3. 训练循环</h1><h2 id="3-1-plot"><a href="#3-1-plot" class="headerlink" title="3.1 plot"></a>3.1 plot</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_loss_update</span>(<span class="params">epoch, mb, train_loss, valid_loss</span>):</span><br><span class="line"></span><br><span class="line">    x = <span class="built_in">range</span>(<span class="number">1</span>, epoch+<span class="number">1</span>)</span><br><span class="line">    y = np.concatenate((train_loss, valid_loss))</span><br><span class="line">    graphs = [[x,train_loss], [x,valid_loss]]</span><br><span class="line">    x_margin = <span class="number">0.2</span></span><br><span class="line">    y_margin = <span class="number">0.05</span></span><br><span class="line">    x_bounds = [<span class="number">1</span>-x_margin, epochs+x_margin]</span><br><span class="line">    <span class="comment"># y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]  #边界换成0-1看看</span></span><br><span class="line">y_bounds = [<span class="number">0</span>-y_margin, <span class="number">1</span>+y_margin]</span><br><span class="line">    </span><br><span class="line">    mb.update_graph(graphs, x_bounds, y_bounds)</span><br></pre></td></tr></table></figure><p>上面是一个 在训练过程中绘制ACC的包 <a href="https://github.com/fastai/fastprogress">fastprogress</a></p><h2 id="3-2-train-valid"><a href="#3-2-train-valid" class="headerlink" title="3.2 train_valid"></a>3.2 train_valid</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>(<span class="params">net, device, criterion, opti, lr, lr_scheduler, batch_size, </span></span><br><span class="line"><span class="params">               train_loader, val_loader, epochs, model_name</span>):</span><br><span class="line">    </span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    train_acc, valid_acc = [], []</span><br><span class="line">    mb = master_bar(<span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> mb:</span><br><span class="line">        train_correct, valid_correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># train_part</span></span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> progress_bar(train_loader, parent=mb):</span><br><span class="line"></span><br><span class="line">            x, y = <span class="built_in">tuple</span>(k.to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch_data)</span><br><span class="line">            outputs = net(x).logits</span><br><span class="line">            loss = criterion(outputs, y)</span><br><span class="line">            train_correct += (outputs.argmax(dim=-<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            opti.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">            opti.step()</span><br><span class="line">        lr_now = lr_scheduler.get_lr()[<span class="number">0</span>]</span><br><span class="line">        train_acc.append(train_correct/(<span class="built_in">len</span>(train_loader)*batch_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># valid_part</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> progress_bar(val_loader, parent=mb):</span><br><span class="line"></span><br><span class="line">                x, y = <span class="built_in">tuple</span>(k.to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch_data)</span><br><span class="line">                outputs = net(x).logits</span><br><span class="line">                loss = criterion(outputs, y)</span><br><span class="line">                valid_correct += (outputs.argmax(dim=-<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            valid_acc.append(valid_correct/(<span class="built_in">len</span>(val_loader)*batch_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plot</span></span><br><span class="line">        plot_loss_update(epochs, mb, train_acc, valid_acc)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print info</span></span><br><span class="line">        train_loss_now = train_acc[-<span class="number">1</span>]</span><br><span class="line">        valid_loss_now = valid_acc[-<span class="number">1</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> complete! Train Acc : </span></span><br><span class="line"><span class="string">              <span class="subst">&#123;train_loss_now*<span class="number">100</span>:<span class="number">.5</span>f&#125;</span>% with lr <span class="subst">&#123;lr_now:<span class="number">.4</span>f&#125;</span>&quot;</span>, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> complete! Validation Acc : <span class="subst">&#123;valid_loss_now*<span class="number">100</span>:<span class="number">.5</span>f&#125;</span>%&quot;</span>, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> valid_loss_now</span><br></pre></td></tr></table></figure><p>上面我们定义两个数组保存ACC的值，以绘制图形</p><h2 id="3-3-kfold-save"><a href="#3-3-kfold-save" class="headerlink" title="3.3 kfold_save"></a>3.3 kfold_save</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kfold_loop</span>(<span class="params">data, save_path, config</span>):</span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> fold, (train_ids,valid_ids) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(data)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;FOLD <span class="subst">&#123;fold&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># config 数据配置</span></span><br><span class="line">        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)</span><br><span class="line">        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)</span><br><span class="line">        </span><br><span class="line">        config[<span class="string">&#x27;train_loader&#x27;</span>] = torch.utils.data.DataLoader(data, batch_size=<span class="number">32</span>, </span><br><span class="line">                                                 sampler=train_subsampler, num_workers=<span class="number">2</span>)</span><br><span class="line">        config[<span class="string">&#x27;val_loader&#x27;</span>] = torch.utils.data.DataLoader(data,batch_size=<span class="number">32</span>, </span><br><span class="line">                                                  sampler=valid_subsampler, num_workers=<span class="number">2</span>)</span><br><span class="line">        config[<span class="string">&#x27;opti&#x27;</span>]  = torch.optim.AdamW(model.parameters(), lr=lr)</span><br><span class="line">        config[<span class="string">&#x27;lr_scheduler&#x27;</span>] = CosineAnnealingLR(config[<span class="string">&#x27;opti&#x27;</span>],T_max=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        net.to(device)</span><br><span class="line">        valid_acc_now = train_loop(**config)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># save  保存最好的模型</span></span><br><span class="line">        <span class="keyword">if</span> valid_acc_now &gt; best_acc:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Best validation Acc improved from <span class="subst">&#123;best_acc:<span class="number">.5</span>f&#125;</span> to <span class="subst">&#123;valid_loss_now:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            net_copy = copy.deepcopy(net)</span><br><span class="line">            best_acc = valid_loss_now</span><br><span class="line"></span><br><span class="line">            save_path = config[<span class="string">&#x27;save_path&#x27;</span>]</span><br><span class="line">            path_to_model = <span class="string">f&#x27;<span class="subst">&#123;save_path&#125;</span>/<span class="subst">&#123;model_name&#125;</span>_lr_<span class="subst">&#123;lr_now:<span class="number">.5</span>f&#125;</span>_valid_acc_<span class="subst">&#123;best_acc:<span class="number">.5</span>f&#125;</span>.pt&#x27;</span></span><br><span class="line">            torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;The model has been saved in <span class="subst">&#123;path_to_model&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这里我们进行k折验证</p><h2 id="3-4-config"><a href="#3-4-config" class="headerlink" title="3.4 config"></a>3.4 config</h2><p>最后我们定义超参数，以及其他构件</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">seed = <span class="number">1222</span></span><br><span class="line">bs = <span class="number">32</span></span><br><span class="line">lr = <span class="number">3e-4</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">warm_steps = <span class="number">122</span>*epochs</span><br><span class="line">total_steps = <span class="number">458</span>*epochs</span><br><span class="line"></span><br><span class="line">set_seed(seed)</span><br><span class="line"></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.08</span>, <span class="number">1.0</span>), ratio=(<span class="number">3.0</span> / <span class="number">4.0</span>, <span class="number">4.0</span> / <span class="number">3.0</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">os_path = <span class="string">&#x27;/kaggle/input/classify-leaves/&#x27;</span></span><br><span class="line">file_path = <span class="string">&#x27;/kaggle/working/train_valid_dataset.csv&#x27;</span></span><br><span class="line">dataset = imgdataset(os_path, file_path, train_transform)</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = ViTForImageClassification.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line">model.classifier = nn.Linear(<span class="number">768</span>, <span class="number">176</span>)</span><br><span class="line"><span class="keyword">for</span> idx, para <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.parameters()): <span class="comment">#冻结部分参数</span></span><br><span class="line">    para.requires_grad = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">197</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">criter = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><p>打包配置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;net&#x27;</span>:model,</span><br><span class="line">    <span class="string">&#x27;device&#x27;</span>:device,</span><br><span class="line">    </span><br><span class="line">    <span class="string">&#x27;lr&#x27;</span>:lr,</span><br><span class="line">    <span class="string">&#x27;opti&#x27;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;lr_scheduler&#x27;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;criterion&#x27;</span>:criter,</span><br><span class="line">    </span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>:bs, </span><br><span class="line">    <span class="string">&#x27;train_loader&#x27;</span>:<span class="number">1</span>, </span><br><span class="line">    <span class="string">&#x27;val_loader&#x27;</span>:<span class="number">1</span>, </span><br><span class="line">    <span class="string">&#x27;epochs&#x27;</span>:epochs, </span><br><span class="line">    <span class="string">&#x27;model_name&#x27;</span>:<span class="string">&#x27;leaves_classifier_model&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="4-训练-amp-结果分析"><a href="#4-训练-amp-结果分析" class="headerlink" title="4. 训练 &amp; 结果分析"></a>4. 训练 &amp; 结果分析</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!mkdir model_save</span><br><span class="line">save_dir = os.getcwd()+<span class="string">&#x27;/model_save&#x27;</span></span><br><span class="line"></span><br><span class="line">kfold_loop(dataset, save_dir, config)</span><br></pre></td></tr></table></figure><blockquote><p>这里第一个fold 出了点问题，总之valid_acc应该是从6%到了23% 后面就是跟下图一样了</p></blockquote><p><img src="/./img/1669374036209.jpg"></p><p>这里我截取了两个fold进行数据查看 (1fold在p100上训练大概40分钟左右）</p><ul><li>随着模型在训练集上的准确率上升，模型在验证集上的准确率也跟train_acc逐步拟合，当然由于验证集的数据没有训练过，中间有一些抖动。但是模型最后还是学到东西了的。</li></ul><p><strong>小结</strong>：</p><p>由于硬件资源的限制，就不再进行训练(模型还是在继续提升的)，我们省略了模型融合和提交结果的验证，这里简单提下</p><ul><li>以投票方式的模型融合为例，Vit的投票结果占权重0.4，剩下的ResNeSt和ResNeXt各占0.25,  VGG占0.1，最后决定输出的标签</li><li>test上就是valid部分 只输出176维度里最大值的位置即可</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>此次我们学习了<strong>Pre-train</strong>的范式、<strong>K-fold验证</strong>、<strong>DataAugment</strong>。</p><ul><li>重点是理解‘拿来主义’，总之拿来就用</li><li>k折交叉验证只是验证的一种方式</li><li>数据增强则需要在理解数据集的基础上进行，是炼丹师必修的一门，当然也有非常多中增强数据的方式</li></ul><p>之后我们将进行对比学习的讲解</p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> HuggingFace </tag>
            
            <tag> Trick </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV 01 CNN MNIST识别</title>
      <link href="/posts/53023.html"/>
      <url>/posts/53023.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将通过CNN 让计算机识别MNIST数据集中的手写数字 以此来介绍Pytorch的基本使用方法：</p><ul><li>Pytorch中的数据类型——tensor</li><li>Pytorch中的数据集、数据加载器——Dataset、DataLoader</li><li>Pytorch中的基础类模型——torch.nn.Module</li></ul><p>以及程序设计上的一些小技巧。</p><h1 id="1-tensor"><a href="#1-tensor" class="headerlink" title="1. tensor"></a>1. tensor</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><p>tensor一词译为<strong>张量</strong>，一般我们所接触的矩阵是二维的，称为二阶张量、向量称为一阶张量、标量称为零阶张量。接下来我们通过Numpy库了解一下张量。（这里并非数学上严格的定义，感性理解一下即可）</p><h3 id="1-1-1-二阶张量-矩阵"><a href="#1-1-1-二阶张量-矩阵" class="headerlink" title="1.1.1 二阶张量 矩阵"></a>1.1.1 二阶张量 矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先我们举一个三行八列的矩阵</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">1</span>,<span class="number">25</span>).reshape(<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 1,  2,  3,  4,  5,  6,  7,  8],</span></span><br><span class="line"><span class="string">       [ 9, 10, 11, 12, 13, 14, 15, 16],</span></span><br><span class="line"><span class="string">       [17, 18, 19, 20, 21, 22, 23, 24]])&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">b = np.ones_like(a).T</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1]])&#x27;&#x27;&#x27;</span></span><br><span class="line">          </span><br><span class="line"><span class="comment"># 我们创建以上两个矩阵，接下来我们把他们做点乘</span></span><br><span class="line">a@b</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[ 36,  36,  36],</span></span><br><span class="line"><span class="string">           [100, 100, 100],</span></span><br><span class="line"><span class="string">           [164, 164, 164]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>上面我们创建了两个矩阵a为三行八列，b为八行三列，两者做点积得到一个三行三列的矩阵。</p><p>我们拉到列表的角度解释这个矩阵，我们将所有数据都包含在一个大列表之内，大列表里有三个小列表，每个列表内有八个元素，</p><p>即<strong>三个小列表代表三行，每个列表的八个元素代表八个维度</strong>。</p><blockquote><p>这里举个小栗子帮助理解一下维度：</p><p>我们在三年级二班给各位同学做信息登记，每个人所需要填写【姓名、年龄、身高、体重】四项内容，我们把每个人的信息记为一条数据，那么我们就可以说这条数据有四个特征，它的维度为四。</p><p>通常来讲我们把特征记为<strong>feature</strong>，称这条数据有四个特征。</p><p>现在整个班级的信息都填写好了应该是如何的形状，我们假设有32个人：</p><p>【【张三、7、130、 70】</p><p>​【李四、7、131、 71】</p><p>​  。。。</p><p>​【李小明、7、129、70】】 如何我们得到一个32行4列的矩阵，记为（32,4）</p><p>接下来我们把视角拉倒整个三年级，我们假设有7个班级：</p><p>那么我们得到的数据维度应该是（7,32,4），表示我们有7个班级的数据，每个班级的数据维度（32,4）。</p><p>此后我们都称这个“班级为<strong>batch</strong>“ ，至此我们从二维的矩阵上升到三维的张量。</p></blockquote><h3 id="1-1-2-张量"><a href="#1-1-2-张量" class="headerlink" title="1.1.2 张量"></a>1.1.2 张量</h3><p>下面我们将介绍常用的张量形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先介绍下一张图片通常的数据格式，我们使用Numpy来伪造一下数据</span></span><br><span class="line"></span><br><span class="line">np.random.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[[[-1.03301822, -0.26956785, -1.81246034, ..., -0.2025034 ,</span></span><br><span class="line"><span class="string">          -0.24770628,  0.45183312],</span></span><br><span class="line"><span class="string">         [ 1.09102807,  0.92955389,  0.07537901, ...,  0.69203358,</span></span><br><span class="line"><span class="string">          -0.17726632, -0.74610015],</span></span><br><span class="line"><span class="string">         [ 0.40508712, -1.2507095 ,  0.68702445, ..., -0.12526432,</span></span><br><span class="line"><span class="string">           0.0390443 ,  1.00993313],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 1.96843042, -2.43286678,  0.08543089, ..., -1.57232148,</span></span><br><span class="line"><span class="string">           0.92844287, -0.25532137],</span></span><br><span class="line"><span class="string">         [ 0.46919141, -0.13700029,  1.78645959, ...,  0.01334257,</span></span><br><span class="line"><span class="string">           1.31030895, -0.22523819],</span></span><br><span class="line"><span class="string">         [ 0.63897933,  0.54846445, -0.64030391, ...,  0.92298892,</span></span><br><span class="line"><span class="string">          -0.50840421,  1.34232325]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.01892086,  0.1456131 , -0.08903806, ...,  1.68250139,</span></span><br><span class="line"><span class="string">           1.2097305 , -0.2680935 ],</span></span><br><span class="line"><span class="string">         [ 0.92759263,  0.22665021,  1.28734004, ...,  0.09925943,</span></span><br><span class="line"><span class="string">           1.30039407,  3.34710594],</span></span><br><span class="line"><span class="string">         [ 0.53486942, -0.56230181, -1.92117215, ...,  1.33047469,</span></span><br><span class="line"><span class="string">          -1.19211895, -0.03081918],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 0.2539067 , -2.13160564,  0.27519544, ..., -0.62223126,</span></span><br><span class="line"><span class="string">           0.5818296 ,  0.07102949],</span></span><br><span class="line"><span class="string">         [-0.7524386 , -0.71244818,  0.88997093, ...,  0.16566338,</span></span><br><span class="line"><span class="string">           0.80577231, -3.35350436],</span></span><br><span class="line"><span class="string">         [ 0.99558393, -2.32335969, -2.87512549, ...,  1.16290939,</span></span><br><span class="line"><span class="string">           2.24089232,  0.22083378]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.35970859,  0.7961136 ,  0.09896652, ...,  1.82609401,</span></span><br><span class="line"><span class="string">          -0.49607535,  0.23424012],</span></span><br><span class="line"><span class="string">         [-0.22283053, -1.35535905, -0.55896315, ...,  1.68093489,</span></span><br><span class="line"><span class="string">           0.80969216,  0.63538616],</span></span><br><span class="line"><span class="string">         [-0.88285682,  0.59389887, -1.05559301, ..., -0.00719476,</span></span><br><span class="line"><span class="string">          -0.25654492, -1.40716977],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 0.44508688, -0.05650302, -2.97674436, ...,  1.25730001,</span></span><br><span class="line"><span class="string">          -1.66409024,  0.96057644],</span></span><br><span class="line"><span class="string">         [-1.3237267 , -0.27798159, -1.8947621 , ...,  1.96216661,</span></span><br><span class="line"><span class="string">          -0.10569547, -0.8446272 ],</span></span><br><span class="line"><span class="string">         [ 0.22525617,  0.75040916,  0.72823974, ..., -1.93525763,</span></span><br><span class="line"><span class="string">          -0.74464397,  0.55771249]]]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>上面就是一张图片**W(width)<strong>为28，</strong>H(heigh)**为28，有RGB三个通道，batch为1的图片(这个batch里面只有一张图片)的数据表示形式。</p><p>当然上面的数据太过复杂，我们以下面W和H都为4的数据继续讲解一下各个数据的意义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[[[ 0.43748082, -0.65000689,  0.13972451, -0.40213376],</span></span><br><span class="line"><span class="string">            [ 0.09342289, -0.83655856,  0.51844492,  0.96505144],</span></span><br><span class="line"><span class="string">            [ 0.68421876,  1.05527391, -0.30821748, -1.89826909],</span></span><br><span class="line"><span class="string">            [-0.36654524,  0.22642376,  0.16545107,  0.00401234]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [[-0.13032482,  0.68182741, -0.52511016,  0.75875314],</span></span><br><span class="line"><span class="string">             [-1.39072336, -0.22848391, -1.64733525,  0.3339502 ],</span></span><br><span class="line"><span class="string">             [-1.06568103, -0.58455172, -0.02874822, -0.64499225],</span></span><br><span class="line"><span class="string">             [-0.23380602, -0.74809941, -0.71214339, -0.44950305]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [[-1.51112191,  0.49145194, -0.01839728, -1.52788219],</span></span><br><span class="line"><span class="string">             [ 0.93370593,  0.96444176, -0.67434299, -1.8492484 ],</span></span><br><span class="line"><span class="string">             [ 0.51140855, -0.58682968, -1.16261225, -0.65782238],</span></span><br><span class="line"><span class="string">             [ 0.8643421 ,  0.79983446, -0.92330871, -2.45649675]]]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><p>一号位<strong>batch</strong>&#x3D;1表示只有一张图片</p><ul><li>若batch&#x3D;3，我们下面降到的模型依次取本批次内【0】号(3,4,4)、【1】(3,4,4)、【2】(3,4,4)进行处理</li></ul></li><li><p>二号位<strong>Channel</strong>&#x3D;3 表示有三个通道分别是RGB</p><ul><li>如上面 0.43748082….0.00401234，就表示在R通道内，这张图片的颜色数据</li><li>G和B通道同理，让三者叠加就可以表示颜色的明暗，从而勾勒画面，渲染颜色</li></ul></li><li><p>最后的两位就表示长宽，每个元素表示像素点的明暗程度。如R通道的第一个元素0.43748082就表示这个像素点有多红</p><p>(红也有程度对吧)</p></li></ul><h2 id="1-2-Pytorch中tensor的API"><a href="#1-2-Pytorch中tensor的API" class="headerlink" title="1.2 Pytorch中tensor的API"></a>1.2 Pytorch中tensor的API</h2><p>Pytorch中tensor号称是跟Numpy及其相似的操作方式，有Numpy的学习基础的话几乎不用付出学习成本来适应tensor。但是实际情况就经常出现各种警告。无论如何，tensor可以享受到GPU的加速运算，总的来说也够友好，下面我们将介绍其常用的API</p><p>首先是随机函数，基本跟Numpy是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Random Tensor:</span></span><br><span class="line"><span class="string"> tensor([[0.7453, 0.7993, 0.8484],</span></span><br><span class="line"><span class="string">        [0.3592, 0.3243, 0.7226]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ones Tensor:</span></span><br><span class="line"><span class="string"> tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Zeros Tensor:</span></span><br><span class="line"><span class="string"> tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>接下来介绍tensor对象的一些属性，其中device默认就是使用cpu，表示我们数据在cpu上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8165, 0.1909, 0.6631, 0.3062],</span></span><br><span class="line"><span class="string">           [0.0178, 0.5158, 0.0267, 0.9819],</span></span><br><span class="line"><span class="string">           [0.6103, 0.7354, 0.7933, 0.2770]]) &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">tensor.shape   <span class="comment"># 将返回 torch.Size([3, 4])</span></span><br><span class="line">tensor.dtype<span class="comment"># 将返回 torch.float32</span></span><br><span class="line">tensor.device<span class="comment"># 将返回 cpu</span></span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span><span class="comment"># 这句话经常来指定数据处理的设备</span></span><br></pre></td></tr></table></figure><p>torch.cat也是一个常用的函数，用来链接数据。</p><p>下面以第一行为例，cat函数将<code>[0.8165, 0.1909, 0.6631, 0.3062]</code>与<code>[0.8165, 0.1909, 0.6631, 0.3062]</code>、<code>[0.8165, 0.1909, 0.6631, 0.3062,]</code>连接，这是因为dim&#x3D;1表示在第一维度，其视角内的可操作单位为<code>0.8165, 0.1909, 0.6631, 0.3062</code>这些元素，dim&#x3D;0则可操作的基本单位为tensor(这里的tensor表示上面的三行四列的实例张量)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8165, 0.1909, 0.6631, 0.3062, 0.8165, 0.1909, 0.6631, 0.3062, 0.8165,</span></span><br><span class="line"><span class="string">         0.1909, 0.6631, 0.3062],</span></span><br><span class="line"><span class="string">        [0.0178, 0.5158, 0.0267, 0.9819, 0.0178, 0.5158, 0.0267, 0.9819, 0.0178,</span></span><br><span class="line"><span class="string">         0.5158, 0.0267, 0.9819],</span></span><br><span class="line"><span class="string">        [0.6103, 0.7354, 0.7933, 0.2770, 0.6103, 0.7354, 0.7933, 0.2770, 0.6103,</span></span><br><span class="line"><span class="string">         0.7354, 0.7933, 0.2770]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>另外介绍一下常用的类型转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">n = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">t.to_list()<span class="comment"># 将tensor类型转换为list</span></span><br><span class="line">t.numpy()<span class="comment"># 转换为Numpy类型</span></span><br><span class="line">torch.from_numpy(n)<span class="comment"># 从Numpy转换为tensor</span></span><br></pre></td></tr></table></figure><p>最后最常用的就是下面两句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">torch.tensor(data) <span class="comment"># 返回tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])可使用dtype=torch.float32换成浮点数</span></span><br><span class="line">torch.Tensor(data)<span class="comment"># 返回 tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])</span></span><br></pre></td></tr></table></figure><h1 id="2-Dataset-DataLoader"><a href="#2-Dataset-DataLoader" class="headerlink" title="2. Dataset DataLoader"></a>2. Dataset DataLoader</h1><p> 都说数据科学家一般的时间都花在数据处理上，一点不假。前面花了这么大篇幅讲价tensor，接下来我们将介绍Pytorch中存储数据的</p><p>Dataset和数据加载器DataLoader</p><h2 id="2-1-你的数据类"><a href="#2-1-你的数据类" class="headerlink" title="2.1 你的数据类"></a>2.1 你的数据类</h2><p>虽然我们使用的MNIST数据集已经可以直接通过Pytorch的API调用，如下</p><p><code>from torchvision import datasets</code></p><p><code>datasets.MNIST(root=&#39;../dataset/mnist/&#39;, train=True, download=True, transform=transform)</code></p><blockquote><p>root表示存储或者加载数据的路径</p><p>train表示是否只加载训练部分的数据集，不设定默认加载全部数据集</p><p>download字面意思</p><p>transform指代这批数据使用什么转换形式，一般来说是一种数据增强方式，以后会专门介绍</p></blockquote><p>我们还是来具体解释下通常要自定义使用的dataset。</p><p>定义符合你要求的数据集有三步必须操作：</p><ul><li>定义你自己的数据集类并继承自<code>torch.utils.data.Dataset</code></li><li>需要包含<code>__len__</code>方法返回长度</li><li>需要包含<code>__getitem__</code>方法，按照下标取得数据</li></ul><p>以上配置也都是为了配合DataLoader的使用，下面我们定义一个dataset类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TitanicDataSets</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,flag</span>):</span><br><span class="line">        xy = preprocess(pd.read_csv(<span class="string">&quot;Titanic.csv&quot;</span>), flag=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> flag == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.x_data = xy.iloc[:, :-<span class="number">1</span>][:<span class="number">800</span>]</span><br><span class="line">            self.y_data = xy.iloc[:, -<span class="number">1</span>][:<span class="number">800</span>]</span><br><span class="line">            self.<span class="built_in">len</span> = self.x_data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> flag == <span class="string">&quot;valid&quot;</span>:</span><br><span class="line">            self.x_data = xy[<span class="number">0</span>][<span class="number">800</span>:<span class="number">892</span>]</span><br><span class="line">            self.y_data = xy[<span class="number">1</span>][<span class="number">800</span>:<span class="number">892</span>]</span><br><span class="line">            self.<span class="built_in">len</span> = self.x_data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br></pre></td></tr></table></figure><p>上面我们引入kaggle著名的<a href="https://www.kaggle.com/competitions/titanic/data">泰坦尼克号幸存者预测比赛使用的数据集</a>，其中x_data获得的是前八百行乘客的信息，y_data记录的就是是否存活。</p><blockquote><p>一般来说我们也<strong>将数据集分为train和valid两部分</strong>，因为最后我们需要预测的数据集并不会有是否存活的标签，所以<strong>通过训练模型参数以拟合train部分的数据，以valid为本次训练的结果导向以修正模型参数</strong>，最终预测，就是我们的目的。</p></blockquote><p>如上面所示，Python中的语句就是这么简洁明了，我们在初始部分读取数据集，然后根据传入的flag决定是处理train还是valid的部分数据，最后我们赋予这个类像列表那样的获取下标和切片能力(<code>__getitem__</code>方法)、以及返回长度的能力(<code>__len__</code>方法)</p><h2 id="2-2-数据加载器"><a href="#2-2-数据加载器" class="headerlink" title="2.2 数据加载器"></a>2.2 数据加载器</h2><p>Pytorch的数据加载器DataLoader简单易用，下面介绍它部分常用参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = TitanicDataSets(flag=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>dataset 表示它所处理的数据，一般是你定义的dataset类，或者具有下标取值，和返回长度的数据类型也可以</li><li>batch_size 表示一词传给模型多少条数据</li><li>shuffle 表示是否打乱</li><li>num_workers 表示使用你cpu的几个核进行读取</li></ul><p>可以使用下面的语句查看dataloader返回给你的数据形状</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">samples = next(iter(train_loader))</span><br><span class="line">samples[:2]# 查看本批次(batch)的前两个样本([0]号，[1]号)</span><br></pre></td></tr></table></figure><h1 id="3-模型"><a href="#3-模型" class="headerlink" title="3. 模型"></a>3. 模型</h1><p>对于模型，以我的理解，数据虽然是死的，但是理解它的方式是活的；模型是活的，但是组合它的方式并没有那么灵活。这里之所以说是组合，说点题外话，是因为如今预训练模型大行其道，大模型在各个任务上不断刷新纪录(SOTA)，小型机构很难有力量去训练这种大模型，于是在大模型上修修改改以适应下游任务的方式，只能使用这种像是Transformer的方式不断变形组合，总感觉缺了点活力。(奠定Pre-train的Bert就是在Transformer基础上提出来的)。</p><h2 id="3-1-模型定义"><a href="#3-1-模型定义" class="headerlink" title="3.1 模型定义"></a>3.1 模型定义</h2><p>下面开始定义我们的CNN模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># flatten data from (n,1,28,28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># -1 此处自动算出的是320</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><ul><li>初始化部分就是定义这个模型的各种方法</li><li>forward向前传播，应用初始化部分定义的函数</li></ul><p>因为MNIST数据集是黑白图片所以只有一个通道(以灰度grey刻画图像即可)</p><h2 id="3-2-模型功能细节"><a href="#3-2-模型功能细节" class="headerlink" title="3.2 模型功能细节"></a>3.2 模型功能细节</h2><p><code>torch.nn.Conv2d</code>即convolution（卷积层）</p><ul><li><p>第一个参数表示进入卷积层数据的channel数</p></li><li><p>第二个参数表示完成卷积后数据的channel数</p></li><li><p>padding&#x3D;1 即在图形周围填充一圈为0的数据(一般来说是有些图形在某些情况下不padding将会取不到原本边界上的值)</p></li><li><p>kernel_size表示卷积核大小(3,3)，如图所示（5,5）的图形padding之后变为（7,7），其经过卷积核映射成（3,3）的形状</p></li></ul><p><img src="/../../article_img/Convolution_arithmetic_-_Padding_strides.gif"></p><blockquote><p>卷积核进行的操作是elementwise multiplication，就是元素与核上对应元素相乘之后加起来就可以了</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d">这里也请各位查看Pytoch文档查看更多参数的详细解释</a></p></blockquote><p><code>torch.nn.MaxPool2d</code></p><p>也就是在一个设定的核的窗口内取最大值</p><p><strong>注意maxpool就是为了取得此区域最大值作为特征输出给下一层的所以不会有overlap的地方</strong></p><p><img src="/../../article_img/maxpool.gif"></p><p><code>激活函数relu</code></p><p>直接上图，置于sigma函数、softmax函数、tanh函数之后会开文讲</p><p><img src="/../../article_img/1_DfMRHwxY1gyyDmrIAd-gjQ.png"></p><p><code>torch.nn.Linear</code></p><p>就是全连接层。</p><p>对于经过卷积、池化、激活的数据，维度为(batch_size, b, c, d)，</p><p>我们将其压缩为(batch_size, n) 最后送给全连接层做n到10的映射，最后变为(batch_size, 10)，以最大数的下标作为我们模型的预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如batch_size = 2</span></span><br><span class="line"></span><br><span class="line">tensor = torch.rand(<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[0.1022, 0.3252, 0.8618, 0.1433, 0.8307, 0.8538, 0.1535, 0.1760, 0.0021,0.9704],</span></span><br><span class="line"><span class="string"> [0.6809, 0.6555, 0.1134, 0.3555, 0.4866, 0.5923, 0.5204, 0.9048, 0.5630,0.4472]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">tensor.argmax(dim=-<span class="number">1</span>)   <span class="comment"># 获得 [9, 7] 第一个列表最大值的下标是9，第二个是7</span></span><br></pre></td></tr></table></figure><h1 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h1><h2 id="4-1-一般流程"><a href="#4-1-一般流程" class="headerlink" title="4.1 一般流程"></a>4.1 一般流程</h2><p>首先将模型实例化，并引入损失函数、优化器和设备。（关于损失函数和优化器之后也会开文讲）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Net()</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p>加载数据，这里函数名都是见名知意，很好理解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"> </span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>下面定义训练循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">       <span class="comment"># 注意这里因为是MNIST数据集，所以自动返回tensor类型，这才有to()方法</span></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        target = target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"> </span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"> </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span>*correct/total))</span><br></pre></td></tr></table></figure><p>以上模型准确率在98%</p><h2 id="4-2-看看准不准"><a href="#4-2-看看准不准" class="headerlink" title="4.2 看看准不准"></a>4.2 看看准不准</h2><p>以下内容使用jupyter notebook查看</p><p>单个查看(train_loader就是上面流程中定义的)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_pred</span>():</span><br><span class="line">    samples = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line">    x = samples[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    pred = model(x).argmax(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred.item())</span><br><span class="line"></span><br><span class="line">    plt.imshow(x.squeeze().numpy())</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_pred()</span><br></pre></td></tr></table></figure><p><img src="/../../article_img/Mnist_pred.png"></p><p>批量查看(train_loader就是上面流程中定义的)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_batch_pred</span>():</span><br><span class="line">    samples = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line">    x = samples[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, imgs <span class="keyword">in</span> <span class="built_in">enumerate</span>(x[:<span class="number">10</span>], <span class="number">0</span>):</span><br><span class="line">        npimg = imgs.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">10</span>, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(npimg, cmap=plt.cm.binary)</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    pred = model(x[:<span class="number">10</span>]).argmax(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred.numpy().tolist())</span><br><span class="line">    </span><br><span class="line">show_batch_pred()</span><br></pre></td></tr></table></figure><p><img src="/../../article_img/Mnist_batch.png"></p><p>可以看到还是错了一个的，倒数第三应该是4 (要不就是我看错了)</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>下面我们来总结一下训练一个模型的pipeline，我认为总结让我们的pipeline获得一定的泛化能力</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"> </span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># flatten data from (n,1,28,28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># -1 此处自动算出的是320</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> x </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练和验证</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        target = target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"> </span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"> </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span>*correct/total))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(epochs)):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 模块定义，一般这里会加入超参数的定义</span></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">main(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><ul><li><p>回顾整个流程: 准备数据—&gt; 定义模型—&gt; 训练循环设计—&gt; 超参数—&gt; 训练并分析结果。各个环节细节的设计请各位参照<a href="https://pytorch.org/docs/stable/index.html">Pytoch官方文档研究</a></p></li><li><p>以上就是整个数据到模型到结果的流程，下节我们将介绍VGG、ResNet50等预训练模型</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Pytorch 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python基础01 数据类型</title>
      <link href="/posts/12763.html"/>
      <url>/posts/12763.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文介绍Python中基本的数据类型：</p><ul><li>字符串、数字</li><li>列表</li><li>字典</li><li>集合</li><li>元组</li></ul><p>以及一些常用的处理小技巧。</p><h1 id="1-字符串、数字"><a href="#1-字符串、数字" class="headerlink" title="1. 字符串、数字"></a>1. 字符串、数字</h1><p>Python中字符串（str）的处理对于没有任何变成经验的同学可能有些苦恼，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">s1 = <span class="string">&#x27;1222&#x27;</span></span><br><span class="line">i = <span class="number">1222</span></span><br><span class="line"></span><br><span class="line">add_up = s1 + i <span class="comment"># 这段函数就会报错，因为无法将 int类型 与 str类型相加</span></span><br></pre></td></tr></table></figure><ul><li><p>int ：整数类型，将小数抹除</p><ul><li>float ：浮点数类型，因为二进制进位的关系数据并不准确的</li></ul></li><li><p>以上就是提醒各位，对数据处理的时候，一定要留心数据的类型</p></li></ul><h2 id="1-1-字符串中的序号"><a href="#1-1-字符串中的序号" class="headerlink" title="1.1 字符串中的序号"></a>1.1 字符串中的序号</h2><p>字符串的序号可以让你快速取得一串字符中任意位置的任意字符，有如下三种基本方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27;Attention Is A Talent&#x27;</span> <span class="comment"># 首先创建一个字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种 取单个字符</span></span><br><span class="line">s[<span class="number">0</span>] <span class="comment"># 将获得 &#x27;A&#x27;</span></span><br><span class="line">s[<span class="number">20</span>]<span class="comment"># 将获得 &#x27;t&#x27;  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种 取多个字符</span></span><br><span class="line">s[<span class="number">0</span>:<span class="number">2</span>]  <span class="comment"># 将获得 &#x27;At&#x27; </span></span><br><span class="line">s[:<span class="number">20</span>]<span class="comment"># 将获得 &#x27;Attention Is A Talen&#x27;</span></span><br><span class="line">s[::<span class="number">2</span>]  <span class="comment"># 将获得 &#x27;AtninI  aet&#x27;</span></span><br><span class="line">s[::<span class="number">1</span>]  <span class="comment"># 将获得 &#x27;Attention Is A Talent&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种 倒序</span></span><br><span class="line">s[-<span class="number">1</span>]   <span class="comment"># 将获得 &#x27;t&#x27; </span></span><br><span class="line">s[-<span class="number">21</span>:]<span class="comment"># 将获得 &#x27;Attention Is A Talent&#x27;</span></span><br></pre></td></tr></table></figure><p>有如下需要注意的几个地方：</p><ol><li><p>在python中我们使用引号包裹需要的字符串内容</p></li><li><p>字符串的序号是从 0 开始定义的，所以上面的s有21个字符，而<strong>方括号内</strong>的取值范围是[0,20], 细心你的肯定发现了，空格数也算进去了。没错<strong>空格也算一种特殊的字符</strong>。</p></li><li><p>第二种取值方式我们称之为<strong>切片</strong>，python中的切片方式等价数学上的**左闭右开~[x, y)**，上面字符s[:20]，是取得s[20]号位左边的全部值，但是不会包含s[20]。</p></li></ol><p>​当然是用s[0:20] 也是等价的。</p><p>​<strong>步长</strong> 即第二种方法的第三个式子，是用步长就是字面意思，每走n步取值。</p><p>​s[::2]就是 s[0]第一步， s[1]第二步(存储)，s[2]第三步，s[3]第四步(存储)….</p><ol start="4"><li><p>倒序是字符串的另一套序号，它有很多应用场景，比如定义一个很长的字符串你可能需要用s[1222222]才能取得这个值，但是是用s[-1]就很方便。</p><p>当然，需要注意<strong>倒序是从[-1]开始的</strong>。</p></li></ol><h2 id="1-2-特殊字符"><a href="#1-2-特殊字符" class="headerlink" title="1.2 特殊字符"></a>1.2 特殊字符</h2><p><code>&#39;\n&#39;(换行)    &#39;\b&#39;(回退)    &#39;\r&#39;(光标回到本行行首)  &#39;\t&#39;(相当于八个空格，两个table)</code></p><p>以上就是几个常见的特殊字符，其中特别需要注意的是路径中的斜杠如遇到 \nigger 计算机可能就无法明白你输入的是 ‘igger’，还是含有n的字符串。有以下两种处理方式：</p><ul><li><code>&#39;\\nigger&#39;</code> </li><li><code>r&#39;\nigger&#39;</code></li></ul><h2 id="1-3-字符串的运算以及常用函数"><a href="#1-3-字符串的运算以及常用函数" class="headerlink" title="1.3 字符串的运算以及常用函数"></a>1.3 字符串的运算以及常用函数</h2><p><strong>运算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1 + s2               <span class="comment"># 将两个字符串连接</span></span><br><span class="line">s1*n                  <span class="comment"># 将s1复制n次</span></span><br><span class="line">s1 <span class="keyword">in</span> s2              <span class="comment"># 如果s1是s2的字串 则返回 True 否 False</span></span><br></pre></td></tr></table></figure><p><strong>函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27; Attention,Is,A,Talent &#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split函数</span></span><br><span class="line">s.split(<span class="string">&#x27;,&#x27;</span>)<span class="comment"># 输出为 [&#x27; Attention&#x27;, &#x27;Is&#x27;, &#x27;A&#x27;, &#x27;Talent &#x27;]</span></span><br><span class="line">                   <span class="comment"># split函数以逗号为标志，返回一个分隔后的列表</span></span><br><span class="line"><span class="comment"># count函数                   </span></span><br><span class="line">s.count(<span class="string">&#x27;A&#x27;</span>)                    <span class="comment"># 统计A在s中的次数，本例中将返回int类型的2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># upper，lower函数</span></span><br><span class="line">s.upper() / s1.lower()          <span class="comment"># 将字符串转化为对应的大小写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replace函数</span></span><br><span class="line">s.replace(<span class="string">&#x27;tion&#x27;</span>, <span class="string">&#x27;&#x27;</span>)  <span class="comment"># 将字符串中的&#x27;tion&#x27;替换为&#x27;&#x27;，即没有东西，相当于删除</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># center函数</span></span><br><span class="line">s.center(<span class="number">30</span>, <span class="string">&#x27;=&#x27;</span>)               <span class="comment"># 将s放在中间，左右两侧平均填充等于号至总字符数为30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># strip函数</span></span><br><span class="line">s.strip(<span class="string">&#x27; &#x27;</span>)                    <span class="comment"># s两侧删除空格，以及其他不可读符号如&#x27;\n&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># join函数</span></span><br><span class="line"><span class="string">&#x27;,&#x27;</span>.join(s)                     <span class="comment"># s中每个字符间填充逗号</span></span><br><span class="line"><span class="comment"># 返回 &#x27;A,t,t,e,n,t,i,o,n,,,I,s,,,A,,,T,a,l,e,n,t&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># len函数</span></span><br><span class="line"><span class="built_in">len</span>(s)<span class="comment"># 返回s的长度</span></span><br></pre></td></tr></table></figure><ol><li>上面我们以逗号为分隔符使用split函数，但是注意<strong>中英文的逗号是有区别的</strong>，其他有些符号也一样，需要注意。</li><li>上述中的replace几乎可以代替center函数，但是注意<strong>strip只能处理字符串的两端</strong></li><li>如join函数返回的结果，再次提醒空格也算是字符</li><li>最后，<strong>上述操作产生都是一个新的对象</strong>，即调用s后返回的是原本的字符串，并不是函数作用后的结果。<br>需要<code>s = s.replace(&#39;tion&#39;, &#39;&#39;)  </code> 赋值才‘生效’。</li></ol><h2 id="1-4-数字函数"><a href="#1-4-数字函数" class="headerlink" title="1.4 数字函数"></a>1.4 数字函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pow</span>(x, n)    <span class="comment">#为x的n次方</span></span><br><span class="line"><span class="built_in">divmod</span>(<span class="number">10</span>, <span class="number">3</span>)    <span class="comment"># 输出为（3， 1）</span></span><br><span class="line"><span class="built_in">abs</span>()       <span class="comment">#返回值为绝对值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">int</span>(<span class="number">12.34</span>)   <span class="comment">#输出 12</span></span><br><span class="line"><span class="built_in">float</span>(<span class="number">12</span>), <span class="built_in">float</span>(<span class="string">&#x27;12.23&#x27;</span>)  <span class="comment">#输出为 12.0 和 12.23</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">round</span>(<span class="number">1.2345</span>， <span class="number">2</span>)   <span class="comment">#保留两位小数</span></span><br><span class="line"><span class="built_in">max</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)   <span class="comment">#返回值为3</span></span><br><span class="line"><span class="built_in">min</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)   <span class="comment">#返回值为1</span></span><br></pre></td></tr></table></figure><h2 id="1-5-格式化字符串"><a href="#1-5-格式化字符串" class="headerlink" title="1.5 格式化字符串"></a>1.5 格式化字符串</h2><p>在我们得到一个数据之后，经常需要对其做保留多少位小数、居中打印、靠右输出、等操作，我们一般叫使其格式化。在python中有三种格式化填充字符串的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># format方式</span></span><br><span class="line">a, b, c = <span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span>, <span class="number">0.12222</span></span><br><span class="line"><span class="string">&#x27;Attention &#123;a&#125; A &#123;b&#125; version &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(a, b, c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将得到如下输出 &#x27;Attention Is A Talent version0.12&#x27;</span></span><br></pre></td></tr></table></figure><p>format格式化将按照顺序填入上面字符串{}的空位，{:.2f}表示此处保留两位小数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f方式</span></span><br><span class="line">a, b, c = <span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span>, <span class="number">0.12222</span></span><br><span class="line"><span class="string">f&#x27;Attention <span class="subst">&#123;a&#125;</span> A <span class="subst">&#123;b&#125;</span> version <span class="subst">&#123;c:<span class="number">.2</span>f&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将得到如下输出 &#x27;Attention Is A Talent version0.12&#x27;</span></span><br></pre></td></tr></table></figure><p>f格式化就是对format方式的简化版</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %方式</span></span><br><span class="line">a, b, c = <span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span></span><br><span class="line"><span class="string">&#x27;Attention %s A %s &#x27;</span>% (<span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将得到如下输出 &#x27;Attention Is A Talent &#x27;</span></span><br></pre></td></tr></table></figure><p>这种方式很老了，推荐使用f方式，非常简洁。</p><p><strong>以下不是必看内容：format填充方式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chr</span>(Unicode)<span class="comment">#返回Unicode对应的字符</span></span><br><span class="line"><span class="built_in">ord</span>(<span class="string">&#x27;字&#x27;</span>)   <span class="comment">#返回对应的编码，如chr(ord(&#x27;a&#x27;)+i ) 即可遍历26字母</span></span><br></pre></td></tr></table></figure><p><strong>填充物若为chr(12222)，等特殊字符</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">chr</span>(<span class="number">12222</span>):^<span class="number">10</span>&#125;</span>&#x27;</span></span><br><span class="line"><span class="comment"># 输出为 &#x27;    ⾾     &#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">f&#x27;&#x27;</span>&#123;<span class="built_in">chr</span>(<span class="number">12222</span>):=^<span class="number">10</span>&#125;<span class="string">&#x27;</span></span><br><span class="line"><span class="string">&#x27;</span>====⾾=====<span class="string">&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">f&#x27;</span><span class="string">&#x27;&#123;chr(12222):=&gt;10&#125;&#x27;</span></span><br><span class="line"><span class="string">&#x27;=========⾾&#x27;</span></span><br></pre></td></tr></table></figure><p>如上{chr(12222):^10} 将chr(12222)对应的字符输出在中间，左右两侧填充空格。</p><p>也可用等号等其他符号填充，或者使用&gt;大于号使结果置右。</p><h1 id="2-列表"><a href="#2-列表" class="headerlink" title="2. 列表"></a>2. 列表</h1><p>列表跟上文中提到的字符串很像，或者说字符串是一种特殊的列表，其所有元素都是字符串。</p><p>python中的列表（list），在我的印象里几乎可以装任何的东西: 字符串、数字、甚至你定义的函数…</p><p><strong>列表是一种非常好用的数据类型，也是我们最常使用数据类型</strong>，以下概要对其简要介绍并补充几个<strong>判断符</strong>。</p><h2 id="2-1-概要"><a href="#2-1-概要" class="headerlink" title="2.1 概要"></a>2.1 概要</h2><p>列表同字符串也有<strong>正反序号下标，切片操作</strong>，不同的是列表可以含有各种类型的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="string">&#x27;Attention Is A Talent&#x27;</span>, <span class="number">100</span>, <span class="string">&#x27;% &#x27;</span>]</span><br><span class="line"></span><br><span class="line">ls[<span class="number">0</span>] == ls[-<span class="number">3</span>] <span class="comment"># 返回True</span></span><br><span class="line"><span class="string">&#x27;A&#x27;</span> <span class="keyword">in</span> ls<span class="comment"># 返回True</span></span><br><span class="line"><span class="built_in">str</span>(ls[<span class="number">1</span>]) + ls[<span class="number">2</span>] + ls[<span class="number">0</span>]<span class="comment"># &#x27;100% Attention Is A Talent&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><p>上面我们使用<strong>双等号</strong>作为判断符，等价询问python 是否 ls[0] &#x3D; ls[-3]</p><ul><li>还有 !&#x3D; 、&gt;&#x3D;、&lt;&#x3D;、等</li></ul></li><li><p>上面我们使用<strong>in</strong>作为判断词，等价询问python 是否 ‘A’ 在 ls</p><ul><li>还有 not in，or，and等</li></ul></li><li><p>第二点，我们使用了str()，它是一个函数，将对传给它的值做字符串化的处理</p><ul><li>int类型的 100 ——&gt; ‘100’ 即数字100变成字符串了</li></ul></li></ul><h2 id="2-2-列表的运算以及常用函数"><a href="#2-2-列表的运算以及常用函数" class="headerlink" title="2.2 列表的运算以及常用函数"></a>2.2 列表的运算以及常用函数</h2><p><strong>运算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="string">&#x27;Attention Is A Talent&#x27;</span>, <span class="number">100</span>, <span class="string">&#x27;% &#x27;</span>]</span><br><span class="line"></span><br><span class="line">ls * <span class="number">2</span> <span class="comment"># 将返回 [&#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;, &#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;]</span></span><br><span class="line"></span><br><span class="line">ls + ls[:<span class="number">1</span>]<span class="comment"># 将返回 [&#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;, &#x27;Attention Is A Talent&#x27;]</span></span><br></pre></td></tr></table></figure><ul><li>注意，<strong>列表的加法操作只能在列表跟列表之间</strong>。<ul><li>如上图使用 ls + ls[0] 将会报错</li></ul></li></ul><p><strong>函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面x表示单个元素、ls表示列表0号、ls1表示列表1号</span></span><br><span class="line"></span><br><span class="line">ls.append(x)<span class="comment"># 给ls尾添加x元素</span></span><br><span class="line">ls.remove(x)<span class="comment"># 将ls中出现的第一个x删除，如要删除所有x可以用（while+flag）或者set集合类型除重</span></span><br><span class="line">ls.extend(ls1)<span class="comment"># 将ls后面连接ls1</span></span><br><span class="line">ls.reverse()<span class="comment"># 将列表的元素逆置</span></span><br><span class="line">ls.insert(i,x)<span class="comment"># 在i位置 插入x</span></span><br><span class="line">ls.pop(i)<span class="comment"># i位置元素出栈，删除</span></span><br></pre></td></tr></table></figure><p>(这里加些列表复杂一点的方法，如果没有了解python中的字典、元组数据类型，可以在下文中了解后再看)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ls = [(<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;jerry&#x27;</span>, <span class="number">80</span>), (<span class="string">&#x27;mike&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;john&#x27;</span>, <span class="number">70</span>)]</span><br><span class="line">ls.sort()</span><br><span class="line">ls.sort(reverse = ture)<span class="comment"># 逆排序</span></span><br><span class="line">ls.sort(key= <span class="keyword">lambda</span> x:x[<span class="number">1</span>])<span class="comment"># 按值排序 </span></span><br><span class="line">-------------------------------</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(ls)<span class="comment"># 会自动把序列从小到大排序</span></span><br><span class="line"><span class="built_in">sorted</span>(ls, reverse = true)</span><br><span class="line"><span class="built_in">sorted</span>(ls, <span class="keyword">lambda</span> x:x[<span class="number">0</span>])<span class="comment"># 以lambda函数作为值排序</span></span><br><span class="line">-------------------------------</span><br><span class="line"></span><br><span class="line">seasons = [<span class="string">&#x27;Spring&#x27;</span>, <span class="string">&#x27;Summer&#x27;</span>, <span class="string">&#x27;Fall&#x27;</span>, <span class="string">&#x27;Winter&#x27;</span>]<span class="comment"># enumerate()的对象必须是可以迭代的类型(iterable)</span></span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">enumerate</span>(seasons))</span><br><span class="line">[(<span class="number">0</span>, <span class="string">&#x27;Spring&#x27;</span>), (<span class="number">1</span>, <span class="string">&#x27;Summer&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;Fall&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;Winter&#x27;</span>)]</span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">enumerate</span>(seasons, start=<span class="number">1</span>))</span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;Spring&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;Summer&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;Fall&#x27;</span>), (<span class="number">4</span>, <span class="string">&#x27;Winter&#x27;</span>)]</span><br></pre></td></tr></table></figure><ul><li><p>第一部分中为<strong>ls的sort方法</strong>配置参数</p><ul><li>reverse 表示逆置，如果对象没有‘大小’，则按照原来的顺序直接逆置</li><li>key 参数表示排序根据此值的大小，这里我们就是以每个元组的第二个值作为value排序</li></ul></li><li><p>第二部分是使用<strong>python中的sorted和sort函数</strong></p><ul><li><p>第一个参数表示传入的可迭代数据类型(就是列表这种含有很多元素，可以一个一个出来的数据类型)</p></li><li><p>第二个参数 同上面的key</p></li><li><blockquote><p><strong>sort 与 sorted 区别：</strong></p><p>sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。</p><p>list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。</p></blockquote></li></ul></li><li><p>第三部分使用了<strong>enumerate函数</strong>，这个函数主要是为元素添加下标，方便一些特殊场景处理</p><ul><li>enumerate函数返回一个含有位置下标的元组类型，为(index，element)形式</li></ul></li></ul><h2 id="2-3-列表应用的例子"><a href="#2-3-列表应用的例子" class="headerlink" title="2.3 列表应用的例子"></a>2.3 列表应用的例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="string">&#x27;Alice&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>]</span><br><span class="line"><span class="built_in">list</span>(lt) = ls[<span class="number">0</span>]<span class="comment"># [0]号为字符，导入list中会分割成[&#x27;a&#x27;, &#x27;l&#x27;, &#x27;i&#x27;, &#x27;c&#x27;, &#x27;e&#x27;]</span></span><br><span class="line">------------------------------------</span><br><span class="line">ls = [<span class="string">&#x27;Ali:ce&#x27;</span>, <span class="string">&#x27;Bo:b&#x27;</span>]</span><br><span class="line">lt = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ls:</span><br><span class="line">    elem = i.split(<span class="string">&#x27;:&#x27;</span>)[-<span class="number">1</span>]<span class="comment"># ls[i]为字符串使用split分割-&gt;[&#x27;Ali&#x27;,&#x27;ce&#x27;]取[-1]</span></span><br><span class="line">    lt.append(elem)<span class="comment"># 直接+=会变成[&#x27;c&#x27;,&#x27;e&#x27;],所以使用list的append,则直接将str的ce加入</span></span><br><span class="line"><span class="comment"># lt = [ce, b]  干净的字符串列表</span></span><br></pre></td></tr></table></figure><h1 id="3-字典"><a href="#3-字典" class="headerlink" title="3. 字典"></a>3. 字典</h1><p>Python字典（dict）是另一种<strong>可变容器模型</strong>,可存储任意类型对象。如字符串、数字、元组等其他容器模型<br><strong>因为字典是无序的所以不支持索引和切片</strong></p><p>注意：</p><ul><li>key不可以重复,否则只会保留第一个;</li><li>value值可以重复;</li><li>key可以是任意的数据类型,但不能出现可变的数据类型,保证key唯一;</li><li>key一般形式为字符串。</li></ul><h2 id="3-1-基本属性"><a href="#3-1-基本属性" class="headerlink" title="3.1 基本属性"></a>3.1 基本属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dic.keys()<span class="comment"># 返回字典中所有的key</span></span><br><span class="line">dic.values()<span class="comment"># 返回包含value的列表</span></span><br><span class="line">dic.items()<span class="comment"># 返回包含(键值,实值)元组的列表</span></span><br></pre></td></tr></table></figure><h2 id="3-2-基本函数"><a href="#3-2-基本函数" class="headerlink" title="3.2 基本函数"></a>3.2 基本函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dic.setdefault(k,value)</span><br><span class="line"><span class="comment">#如果key值存在,那么返回对应字典的value,不会用到自己设置的value;</span></span><br><span class="line"><span class="comment">#如果key值不存在.返回None,并且把新设置的key和value保存在字典中;</span></span><br><span class="line"><span class="comment">#如果key值不存在,但设置了value,则返回设置的value;</span></span><br><span class="line"></span><br><span class="line">dic.get(k,value)</span><br><span class="line"><span class="comment">#如果key值存在,那么返回对应字典的value,不会用到自己设置的value;</span></span><br><span class="line"><span class="comment">#如果key值不存在.返回None,但是不会把新设置的key和value保存在字典中;</span></span><br><span class="line"><span class="comment">#如果key值不存在,但设置了value,则返回设置的value;</span></span><br><span class="line"></span><br><span class="line">dic.items()</span><br><span class="line"><span class="comment">#打印字典中的所有元组</span></span><br></pre></td></tr></table></figure><p><strong>以下不是必看内容：dic.get(key, init_value)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ls = [(<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;jerry&#x27;</span>, <span class="number">80</span>), (<span class="string">&#x27;mike&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;john&#x27;</span>, <span class="number">70</span>)]</span><br><span class="line"></span><br><span class="line">ls1,dic = [], &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> ls:</span><br><span class="line">    ls1.append(i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> ls1:</span><br><span class="line">dic[i] = dic.get(i, <span class="number">0</span>) + <span class="number">1</span>         </span><br><span class="line">dic</span><br></pre></td></tr></table></figure><p>get第一个参数为对应的键，第二个参数为键对应的初始值。</p><p>get方法将键对应的值初始化为0，以后每见一次加一次，在文本统计时经常使用。</p><p>相对于dic[i]，dic.get(i)不会报错，而它每见一次加一次而不是覆盖，明显速度会比前者慢。(但是不是大量数据都差不多。)</p><h1 id="4-集合"><a href="#4-集合" class="headerlink" title="4.集合"></a>4.集合</h1><p>在Python中集合（set）元素之间无序，<strong>每个元素唯一</strong>，不存在相同元素<strong>集合元素不可更改</strong>，不能是可变数据类型</p><h2 id="4-1-创建和运算"><a href="#4-1-创建和运算" class="headerlink" title="4.1 创建和运算"></a>4.1 创建和运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">使用两种方式建立</span><br><span class="line">A = &#123;<span class="string">&quot;python&quot;</span>, <span class="number">123</span>, (<span class="string">&quot;python&quot;</span>,<span class="number">123</span>)&#125; <span class="comment"># 使用&#123;&#125;建立集合</span></span><br><span class="line">输出为&#123;<span class="number">123</span>, <span class="string">&#x27;python&#x27;</span>, (<span class="string">&#x27;python&#x27;</span>, <span class="number">123</span>)&#125;</span><br><span class="line">B = <span class="built_in">set</span>(<span class="string">&quot;pypy123&quot;</span>) <span class="comment"># 使用set()建立集合</span></span><br><span class="line">输出为&#123;<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;y&#x27;</span>&#125;</span><br><span class="line">C = &#123;<span class="string">&quot;python&quot;</span>, <span class="number">123</span>, <span class="string">&quot;python&quot;</span>,<span class="number">123</span>&#125;<span class="comment"># 去重</span></span><br><span class="line">输出为&#123;<span class="string">&#x27;python&#x27;</span>, <span class="number">123</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">S | T 并，返回一个新集合，包括在集合S和T中的所有元素 </span><br><span class="line">S - T 差，返回一个新集合，包括在集合S但不在T中的元素 </span><br><span class="line">S &amp; T 交，返回一个新集合，包括同时在集合S和T中的元素 </span><br><span class="line">S ^ T 补，返回一个新集合，包括集合S和T中的非相同元素 </span><br><span class="line">S &lt;= T 或 S &lt; T     返回<span class="literal">True</span>/<span class="literal">False</span>，判断S和T的子集关系 </span><br><span class="line">S &gt;= T 或 S &gt; T     返回<span class="literal">True</span>/<span class="literal">False</span>，判断S和T的包含关系</span><br></pre></td></tr></table></figure><h2 id="4-2-函数"><a href="#4-2-函数" class="headerlink" title="4.2 函数"></a>4.2 函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(s) <span class="comment">#返回序列s的长度，即元素个数</span></span><br><span class="line"><span class="built_in">min</span>(s) <span class="comment">#返回序列s的最小元素，s中元素需要可比较</span></span><br><span class="line"><span class="built_in">max</span>(s) <span class="comment">#返回序列s的最大元素，s中元素需要可比较</span></span><br><span class="line">s.index(x) / s.index(x, i, j)     <span class="comment">#返回序列s从i开始到j位置中第一次出现元素x的位置</span></span><br><span class="line">s.count(x) <span class="comment">#返回序列s中出现x的总次数</span></span><br></pre></td></tr></table></figure><p>其中len()、min()、max()函数是内置的通用函数</p><h1 id="5-元组"><a href="#5-元组" class="headerlink" title="5. 元组"></a>5. 元组</h1><p>python中的元组（tuple）是一种序列类型，一旦创建就<strong>不能被修改</strong> ，使用<strong>小括号 () 或 tuple() 创建</strong>，元素间用逗号 , 分隔 。</p><h2 id="5-1-创建和取值"><a href="#5-1-创建和取值" class="headerlink" title="5.1 创建和取值"></a>5.1 创建和取值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">creature = <span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;tiger&quot;</span>,<span class="string">&quot;human&quot;</span></span><br><span class="line">creature = (<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;tiger&#x27;</span>, <span class="string">&#x27;human&#x27;</span>) <span class="comment"># 可以使用括号和不带括号的两两种</span></span><br><span class="line">color = (<span class="number">0x001100</span>, <span class="string">&quot;blue&quot;</span>, creature)<span class="comment"># 输出会得到（，，（））</span></span><br></pre></td></tr></table></figure><p>元组的取值操作跟列表一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">creature = <span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;tiger&quot;</span>,<span class="string">&quot;human&quot;</span></span><br><span class="line">creature[::-<span class="number">1</span>]<span class="comment"># 输出为(&#x27;human&#x27;, &#x27;tiger&#x27;, &#x27;dog&#x27;, &#x27;cat&#x27;)</span></span><br><span class="line">color = (<span class="number">0x001100</span>, <span class="string">&quot;blue&quot;</span>, creature)</span><br><span class="line">color[-<span class="number">1</span>][<span class="number">2</span>]<span class="comment"># 输出为&#x27;tiger&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="5-2-函数"><a href="#5-2-函数" class="headerlink" title="5.2 函数"></a>5.2 函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span>(x) <span class="comment"># 将其他类型变量x转变为集合类型</span></span><br><span class="line">t.discard(x)     <span class="comment"># 移除S中元素x，如果x不在集合S中，不报错</span></span><br><span class="line">t.remove(x) <span class="comment"># 移除S中元素x，如果x不在集合S中，产生KeyError异常</span></span><br><span class="line">t.pop() <span class="comment"># 随机返回S的一个元素，更新S，若S为空产生KeyError异常</span></span><br><span class="line">t.add(x) <span class="comment"># 如果x不在集合S中，将x增加到S</span></span><br><span class="line">t.clear() <span class="comment"># 移除S中所有元素</span></span><br><span class="line">x <span class="keyword">in</span> t<span class="comment"># 判断S中元素x，x在集合S中，返回True，否则返回False</span></span><br><span class="line">x <span class="keyword">not</span> <span class="keyword">in</span> t <span class="comment"># 判断S中元素x，x不在集合S中，返回True，否则返回False</span></span><br><span class="line">t.copy() <span class="comment"># 返回集合S的一个副本</span></span><br><span class="line"><span class="built_in">len</span>(t) <span class="comment"># 返回集合S的元素个数</span></span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>同上文我们定义的字符串、列表、字典，我们按照<strong>见名知意</strong>的原则，将其赋值给s、ls、dic。这样做是为了程序的可读性。今后我们在写程序的时候会定义很多变量，变量一多了就容易搞混了，顺藤摸瓜找着效率又很低，所以给你的变量取一个好名字，是一个很划算的决定。</p><ul><li>举个例子<code>train_df = pandas.DataFrame(&#39;../train.csv&#39;)</code>中的 train_df表示这个是个训练数据，其数据类型为pandas的DataFrame结构。</li></ul></li><li><p>此外在操作数据的时候一定要注意数据类型，根据数据的特性选择合适的数据类型。</p></li><li><p>最后，python最好用的地方在于有各种各样的库供你选择，而掌握它基础的数据结构是第一步，下面我们将讲解函数以及类，最后介绍两个常用的库，介绍一些如何快速掌握一个库的通法。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer &amp; Self-Attention</title>
      <link href="/posts/4330.html"/>
      <url>/posts/4330.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://jalammar.github.io/illustrated-transformer/">阿三博客地址</a></p><p><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=6997c0a04f6a78d03d30de86e9b949d9">李沐老师 48分钟讲解 encoder-decoder中(KV–Q)的运算</a>: </p><p><strong>KQ相乘就是单个q对所有k的相似度作为attention score(给这个K值多少注意力)，与单个v做加权和(权值来自KQ)</strong></p><p>再通过<strong>注意力分数</strong>与<strong>V向量相乘</strong>，<strong>得到每个V应该多大的缩放</strong>， 进行相加后就得到了最终V应该是什么样子了</p><p><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=6997c0a04f6a78d03d30de86e9b949d9">李沐老师 56分 对multi-head输出和linear层相较于RNN的讲解</a>：</p><p>词向量经过Attention层抓取全局信息，汇聚之后，在每个点上都有了所需要的信息</p><p>(权重不同，每个输出的向量的重点在不同的position编码位置上)，因此只需要做linear transformation。</p><p><strong>bert中transformer参数计算</strong>:</p><p>embedding: vocab_size&#x3D;30522, max_position_embeddings&#x3D;512, token_type_embeddings&#x3D;2(就进行两句分别标记，多了截断)</p><p>​（30522+512+2）*768 &#x3D; 23835648 (23M)</p><p>self-attention: 768&#x2F;12 &#x3D; 64 (多头每头分64维度的向量) ，64*768(每个64映射回768)，QKV三个矩阵, </p><p>​  最后一层 786(64 *12的拼接)-&gt;768的线性变换</p><p>​(768&#x2F;12 * 768 <em>3 ) * 12 + (768</em>768) &#x3D; 2359296</p><p>​经过12个transformer</p><p>​2359296*12 &#x3D; 28311552 (28M)</p><p>feedfoward: 自注意力层之后 分别在 encoder 和 decoder 中有个一个全连接层</p><p>​维度从 768-&gt;4*768_768-&gt;768</p><p>​(768*4 * 768 )*2 &#x3D; 4718592</p><p>​(768*4 * 768 )*2  * 12 &#x3D; 56623104 (56M)</p><p>layernorm: 有伽马和贝塔两个参数，embedding层（768 * 2），12层的self-attention，</p><p>​768 * 2 + 768 * 2 * 2 * 12 &#x3D; 38400</p><p>总计: 23835648+28311552+56623104+38400 &#x3D; 108808704      (108M)</p><p>每一层的参数为:  多头注意力的参数 + 拼接线性变换的参数 + feed-forward的参数 + layer-norm的参数</p><p>768 * 768 &#x2F; 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2 &#x3D; 7080960  (7M)</p><h1 id="Encoder-编码阶段"><a href="#Encoder-编码阶段" class="headerlink" title="Encoder 编码阶段"></a>Encoder 编码阶段</h1><h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p>多头注意力机制将一个词向量留过八个 self-attention 头生成八个词向量 vector，</p><p>将八个词向量拼接，通过 fc 层进行 softmax 输出。</p><p>例如：</p><p>词向量为 (1,4) –&gt; </p><p>经过 QKV 矩阵(系数) 得到 (1,3) 八个 (1,3)*8 –&gt;</p><p>将输出拼接成 (8,3) 矩阵与全连接层的系数矩阵进行相乘再 softmax <strong>确定最后输出的</strong> 词向量 –&gt;</p><p>(1,4)</p><h3 id="注意-QKV矩阵怎么来的-attention分数-，最后为什么要拼接，以及FC层的系数"><a href="#注意-QKV矩阵怎么来的-attention分数-，最后为什么要拼接，以及FC层的系数" class="headerlink" title="注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数"></a>注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数</h3><ol><li><p>qk相乘得到，词向量与其他词的attention分数( q1*(k1,k2,k3) )</p></li><li><p>多头注意力机制让一份词向量产生了多份答案，将每一份注意力机制的产物拼接，</p><p>获得了词向量在不同注意力矩阵运算后的分数，进行拼接后，softmax输出<strong>最注意的词</strong>，即是注意力机制。</p></li><li><p><strong>多头注意力机制，将向量复制n份(n为多头头数)，投影到如512&#x2F;8 &#x3D; 64的64维的低维空间，最后将每一层的输出结果</strong></p><p><strong>此处为八层，8*64&#x3D;512 拼回512维的输出数据</strong></p><p><strong>由于Scale Dot Product 只是做乘法点积(向量变成qvk之后的attention运算)，没什么参数，因此重点学习的参数在Multi-Head的线性变换中，</strong></p><p><strong>即将 64*8的八份数据线性变换的下文中的W0，给模型八次机会希望能够学到什么，最后在拼接回来。</strong>&#x3D;&#x3D;</p></li></ol><p><strong>注意力机制流程</strong>：</p><p><strong>q –&gt; 查询向量</strong></p><p><strong>set( k，v)    k –&gt;关键字 v—-&gt; 值</strong></p><p><strong>如果 q对k的相似度很高，则输出v的概率也变高</strong></p><img src="https://jalammar.github.io/images/t/self-attention-output.png" alt="img" style="zoom: 80%;" /><p><strong>’多头’注意力机制</strong> </p><p>请注意并推演其<strong>词向量维度与系数矩阵带的行数</strong></p><p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="img"></p><h3 id="Scale-Dot-Product"><a href="#Scale-Dot-Product" class="headerlink" title="Scale Dot Product"></a>Scale Dot Product</h3><p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="img"></p><p><strong>step1</strong></p><p>QK做点积，则输出每一行，是q与所有k的相乘相加结果，</p><p>α1 &#x3D; （q11k11+q12k21+q13k31 ,  q11k12+q12k22+q13k32 )</p><p>α2同理。</p><p><strong>step2</strong></p><p>所以得到了query1对所有key的相似度，最后每一行做个softmax进行概率分布。</p><p>除以根号dk是为了平滑梯度，具体来说：<strong>当概率趋近于1的时候softmax函数的梯度很小</strong>，<strong>除以dk让数值接近函数中部，梯度会比较陡峭</strong></p><p><strong>step3</strong></p><p>将第二步的结果与V相乘得到最后的输出</p><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>位置编码是 将embedding好的词向量加上 position embedding vector 将<strong>信息融合，在注意力机制中进行计算</strong>。</p><p>(原文是使用sin cos将词向量份两部分进行编码， 本文中将交替使用sin cos，即单数sin 双数cos)</p><p>位置嵌入编码，主要是为了编辑定位<strong>词向量的位置</strong>以及<strong>词向量间的相对距离</strong></p><blockquote><p>pos为 词的种类数，为行标号</p><p>i 为特征维度</p><p>len(pos) * len(i)  表示为一position embedding 矩阵， 每一行为词的位置信息，每一列表示在特征上偏置，</p><p><strong>将位置信息 融入 词向量信息 使词获得 时间上的相对信息</strong></p></blockquote><p><a href="https://www.imagehub.cc/image/JqzAac"><img src="https://s1.imagehub.cc/images/2022/11/09/image638960e7f1096a03.png" alt="image638960e7f1096a03.png" border="0" /></a></p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png" alt="img" style="zoom: 80%;" /><p><img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png" alt="img"></p><h2 id="Residual-细节"><a href="#Residual-细节" class="headerlink" title="Residual 细节"></a>Residual 细节</h2><p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt="img"></p><h1 id="Decoder-解码阶段"><a href="#Decoder-解码阶段" class="headerlink" title="Decoder 解码阶段"></a>Decoder 解码阶段</h1><h3 id="Mask-Multi-head"><a href="#Mask-Multi-head" class="headerlink" title="Mask Multi-head"></a>Mask Multi-head</h3><p>与encoder不同的是，解码器在工作时会引入 <strong>Mask Multi-head 机制</strong>，将右侧的词盖住(设为负无穷或者别的)。</p><p>具体来说:</p><ol><li><p><strong>encoder 将生成的K和V矩阵</strong>传入 decoder 的 self-attention 模块中，而 <strong>decoder 将产生 mask 后的Q矩阵</strong>与其做attention。</p></li><li><p>mask做的事情</p><p><a href="https://www.imagehub.cc/image/JPWaZm"><img src="https://s1.imagehub.cc/images/2022/10/31/IMG_235920221004-111643.jpg" alt="IMG_235920221004-111643.jpg" border="0" /></a></p></li></ol><p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p><p>没太懂，这张图，这里<strong>可以实现并行化嘛</strong>？&#x3D;&#x3D;解释在Mask细节&#x3D;&#x3D;</p><p>按照小蛮的矩阵，右侧的是可以屏蔽的，但是扩展成n为的词向量怎么屏蔽呢？</p><p>在生成z矩阵屏蔽？ 不可能，已经参加计算了。</p><p>所以不能并行输出？ 只能一个一个吐？等下看下小蛮视频、huggingface教程也行</p><p><strong>时间维度</strong> </p><p>在时间序列的情况下，词向量表示为，t1时刻的vector，t2时刻的vector….</p><p>mask做的事情就是将后面(右边)的 tn个时刻都屏蔽掉，</p><p>而Qmatrix的形成 将vector含有了其之后词的信息(共享了系数矩阵)，所以将其右边屏蔽。</p><p>则剔除了后面词的信息，从而不进行考虑。</p><h3 id="Mask-细节"><a href="#Mask-细节" class="headerlink" title="Mask 细节"></a>Mask 细节</h3><p>mask就是为了阻止词知道后面的信息，具体来说就是QKV矩阵还相乘，但是引入-inf来阻止右边(后面的信息汇聚)</p><p><a href="https://www.imagehub.cc/image/JP7vvk"><img src="https://s1.imagehub.cc/images/2022/10/31/1a35106e841b162c68e41ceb2d8aafb.jpg" alt="1a35106e841b162c68e41ceb2d8aafb.jpg" border="0" style="zoom:50%;" /></a></p><p>同小蛮的流程，<strong>第一次点积：</strong>将Q和K矩阵相乘得到attention分数，</p><p>将右上角置零就会得到只含有本身信息和相对位置之前(左边)的信息，</p><p>且<strong>第二次点积:</strong> Mask(QK)与V相乘由下三角矩阵的性质，</p><p><a href="https://www.imagehub.cc/image/JPuD0r"><img src="https://s1.imagehub.cc/images/2022/10/31/752a1f3305f3573aacc1c7b92395faf.jpg" alt="752a1f3305f3573aacc1c7b92395faf.jpg" border="0" style="zoom:50%;" /></a></p><p>第一行(t1时刻)只考虑第一个值的输出</p><p>第一行(t2时刻)考虑第一个和第二个值的输出</p><p>….</p><p>这样就可以实现 tn时刻只考虑 t1 到 tn-1的输出</p><p>如此便可实现并行化。 (encode到decode还是串行的)</p><img src="https://z3.ax1x.com/2021/04/20/c7w7rD.png#shadow" alt="img" style="zoom:50%;" /><p><strong>注: mask去负无穷是因为 SoftMax中 e的指数形式只有在负无穷才为零，</strong></p><p><strong>这样相乘数据不会有一点影响，取其他值，都会影响softmax</strong></p><img src="https://z3.ax1x.com/2021/04/20/c7w48x.png#shadow" alt="img"  /><p><img src="https://s1.ax1x.com/2020/07/12/U3FCQ0.png#shadow" alt="img"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>特别注意理解 attention机制将词向量之间的联系， <strong>attention分数</strong></li><li>embedding方式为 <strong>词向量+位置编码向量</strong></li><li>引入了 <strong>Residual</strong></li><li><strong>encoder-decoder层的传入</strong>为KV矩阵，decoder生成Q矩阵</li><li><strong>Mask方式</strong></li></ol><h3 id="尚未明晰"><a href="#尚未明晰" class="headerlink" title="尚未明晰:"></a>尚未明晰:</h3><ol><li>multi-head 将输入的向量均分八等分？分别做 self-attention？ 减少参数加快运算，结果还差不多？</li><li>multi-head 处理的向量是在不同维度处理？ 比如 head1是词义，head2是位置等等。</li></ol><p>(来自小蛮视频)</p>]]></content>
      
      
      <categories>
          
          <category> Dive Into Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention机制</title>
      <link href="/posts/54367.html"/>
      <url>/posts/54367.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">博客地址</a></p><h2 id="传统Seq2Seq"><a href="#传统Seq2Seq" class="headerlink" title="传统Seq2Seq"></a>传统Seq2Seq</h2><p>​<a href="https://www.imagehub.cc/image/JJyQxx"><img src="https://s1.imagehub.cc/images/2022/10/29/image9776ac96fcdc3b7e.png" alt="image9776ac96fcdc3b7e.png" border="0" /></a></p><p><a href="https://jalammar.github.io/images/seq2seq_4.mp4">动画连接</a></p><p>左侧为 input 将句子一个一个投入到 encoder 中，</p><p>encoder整个处理其相关性得到 context，吐给 decoder，</p><p>decoder 进行一个一个解码输出，得到整个翻译后的句子。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>An attention model differs from a classic sequence-to-sequence model in two main ways:</p><ul><li>First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes <em>all</em> the hidden states to the decoder:</li></ul><p>​<strong>注意力机制将产生的隐藏层信息(时间步骤信息)，全部保留，一次性传给 Decoder。</strong></p><img src="https://img-blog.csdnimg.cn/20181119222424704.gif" alt="img" style="zoom: 80%;" /><ul><li><p>Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:</p><ol><li><p>Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence</p></li><li><p>Give each hidden state a score (let’s ignore how the scoring is done for now)</p></li><li><p>Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores</p></li></ol></li></ul><p>​<strong>decoder 将 encoder 输入的隐藏层的 vector 进行打分得到一个分数vector，</strong></p><p>​<strong>将分数 vector 做 softmax，得到一个权重 vector，</strong></p><p>​<strong>将权重 vector 与隐藏层 vector 相乘得到 注意力 vector，</strong></p><p>​<strong>最后把注意力 vector 进行相加就完成了。</strong></p><p><a href="https://www.imagehub.cc/image/JJ8xIK"><img src="https://s1.imagehub.cc/images/2022/10/29/imageb79497ae8b6c83ca.png" alt="imageb79497ae8b6c83ca.png" border="0" /></a></p><ul><li>注意: 将 encoder 的隐藏层信息传入 decoder之后，decoder 每一步都将使用其传入的隐藏层信息做 attention。</li></ul><p><img src="https://img-blog.csdnimg.cn/20181119222700993.gif" alt="img"></p><p>​<strong>由上图可以看到，输出时 Attention 机制就是将注意力放在分数最高的向量上，所以，称之为’注意力机制’</strong></p>]]></content>
      
      
      <categories>
          
          <category> Dive Into Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02 HuggingFace基础</title>
      <link href="/posts/45348.html"/>
      <url>/posts/45348.html</url>
      
        <content type="html"><![CDATA[<h3 id="Transformer分两块BERT-amp-GPT都很能打"><a href="#Transformer分两块BERT-amp-GPT都很能打" class="headerlink" title="Transformer分两块BERT&amp;GPT都很能打"></a>Transformer分两块BERT&amp;GPT都很能打</h3><ol><li><p>BERT用的是transformer的encoder</p><blockquote><p>BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。</p></blockquote></li><li><p>GPT用的是transformer的decoder</p><blockquote><p>在它之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。</p></blockquote></li></ol><h2 id="Bert的embedding"><a href="#Bert的embedding" class="headerlink" title="Bert的embedding"></a>Bert的embedding</h2><p>Embedding由三种Embedding求和而成：</p><ol><li><p>Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务</p><blockquote><p>BERT在第一句前会加一个[CLS]标志，<strong>最后一层该位对应向量可以作为整句话的语义表示</strong>，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更<strong>“公平”地融合文本中各个词的语义信息</strong>，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。</p></blockquote></li><li><p>Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</p></li><li><p>Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的</p></li></ol><h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><h3 id="tokenizer"><a href="#tokenizer" class="headerlink" title="tokenizer"></a>tokenizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># config模块</span></span><br><span class="line">MODEL_NAME=<span class="string">&quot;bert-base-chinese&quot;</span></span><br><span class="line">config = AutoConfig.from_pretrained(MODEL_NAME) <span class="comment">#c onfig可以配置模型信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizer模块</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)</span><br><span class="line"></span><br><span class="line">tokenizer.all_special_ids <span class="comment"># 查看特殊符号的id [100, 102, 0, 101, 103]</span></span><br><span class="line">tokenizer.all_special_tokens <span class="comment"># 查看token  [&#x27;[UNK]&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;]</span></span><br><span class="line"></span><br><span class="line">tokenizer.vocab_size <span class="comment"># 词汇表大小</span></span><br><span class="line">tokenizer.vocab <span class="comment"># 词汇对应的dict形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## tokening</span></span><br><span class="line">text=<span class="string">&quot;我在北京工作&quot;</span></span><br><span class="line">token_ids=tokenizer.encode(text)</span><br><span class="line">token_ids <span class="comment"># [101, 2769, 1762, 1266, 776, 2339, 868, 102]</span></span><br><span class="line">tokenizer.convert_ids_to_tokens(token_ids) <span class="comment"># [&#x27;[CLS]&#x27;, &#x27;我&#x27;, &#x27;在&#x27;, &#x27;北&#x27;, &#x27;京&#x27;, &#x27;工&#x27;, &#x27;作&#x27;, &#x27;[SEP]&#x27;]</span></span><br><span class="line">  <span class="comment"># convert_tokens_to_ids(tokens) 为对应方法</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">## padding 做向量填充</span></span><br><span class="line">token_ids=tokenizer.encode(text,padding=<span class="literal">True</span>,max_length=<span class="number">30</span>,add_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## encode_plus</span></span><br><span class="line">token_ids=tokenizer.encode_plus(</span><br><span class="line">    text,padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">    return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">    return_attention_mask=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="使用pre-train模型载入数据"><a href="#使用pre-train模型载入数据" class="headerlink" title="使用pre_train模型载入数据"></a>使用pre_train模型载入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model=AutoModel.from_pretrained(MODEL_NAME)</span><br><span class="line">outputs=model(token_ids[<span class="string">&#x27;input_ids&#x27;</span>],token_ids[<span class="string">&#x27;attention_mask&#x27;</span>])</span><br></pre></td></tr></table></figure><h3 id="数据集dataset定义"><a href="#数据集dataset定义" class="headerlink" title="数据集dataset定义"></a>数据集dataset定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,item</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        item 为数据索引，迭代取第item条数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        </span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,  <span class="comment">#转为tensor</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"><span class="comment">#print(encoding[&#x27;input_ids&#x27;])</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>:text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>:encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>:encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten(),</span><br><span class="line">            <span class="comment"># toeken_type_ids:0</span></span><br><span class="line">            <span class="string">&#x27;labels&#x27;</span>:torch.tensor(label,dtype=torch.long)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HuggingFace </tag>
            
            <tag> Bert </tag>
            
            <tag> Preprocessing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01 HuggingFace基础</title>
      <link href="/posts/45347.html"/>
      <url>/posts/45347.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Attention 原文</a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><ul><li><p>全面拥抱Transformer：NLP三大特征抽取器(CNN&#x2F;RNN&#x2F;TF)中，近两年新欢<strong>Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。</strong></p></li><li><p>像Wordvec出现之后一样，在人工智能领域种各种目标皆可向量化，也就是我们经常听到的“<strong>万物皆可Embedding</strong>”。而Transformer模型和Bert模型的出现，更是NLP领域划时代的产物：将<strong>transformer和双向语言模型进行融合</strong>，便得到NLP划时代的，也是当下在各自NLP下流任务中获得state-of-the-art的模型-BERT</p></li><li><p><strong>BERT起源于预训练的上下文表示学习</strong>，与之前的模型不同，BERT是一种深度双向的、无监督的语言表示，且仅使用纯文本语料库进行预训练的模型。<strong>上下文无关模型（如word2vec或GloVe）为词汇表中的每个单词生成一个词向量表示，因此容易出现单词的歧义问题</strong>。BERT考虑到单词出现时的上下文。例如，词“水分”的word2vec词向量在“植物需要吸收水分”和“财务报表里有水分”是相同的，但BERT根据上下文的不同提供不同的词向量，词向量与句子表达的句意有关。</p></li></ul><h3 id="Embedding："><a href="#Embedding：" class="headerlink" title="Embedding："></a>Embedding：</h3><p><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/bert_img/embedding.png" alt="img"></p><ul><li>首先类似 word2vec 的 token化，再进行片段标记( segment )，最后 ids 的位置编码(  position )</li><li>编码后一个 ’词‘ 有三个信息，token、段落位置信息、绝对位置信息( id: 1、2、3…)</li></ul><h4 id="Embedding解决的问题"><a href="#Embedding解决的问题" class="headerlink" title="Embedding解决的问题:"></a>Embedding解决的问题:</h4><ul><li>首先是之前用的 <strong>One-Hot Key</strong>，高维度，离散的，低信息密度的储存形式</li><li>其次是更好的 <strong>Contextual Similarity</strong>，上下文相关相似性。</li></ul><h2 id="Preview-Api"><a href="#Preview-Api" class="headerlink" title="Preview Api"></a>Preview Api</h2><h3 id="前置查看："><a href="#前置查看：" class="headerlink" title="前置查看："></a>前置查看：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>) <span class="comment"># 获取相应模型的tokenizer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForMaskedLM</span><br><span class="line">model = AutoModelForMaskedLM.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>) <span class="comment">#查看模型的分层</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="函数调用："><a href="#函数调用：" class="headerlink" title="函数调用："></a>函数调用：</h3><h4 id="字典大小，token化，ids化"><a href="#字典大小，token化，ids化" class="headerlink" title="字典大小，token化，ids化"></a>字典大小，token化，ids化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vocab = tokenizer.vocab</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;字典大小：&quot;</span>, <span class="built_in">len</span>(vocab)) <span class="comment"># 查看字典大小</span></span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(text)<span class="comment"># 将文字分词</span></span><br><span class="line">ids = tokenizer.convert_tokens_to_ids(tokens)<span class="comment"># 将文字转化为数字，进行编码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;, &#x27;等&#x27;, &#x27;到&#x27;, &#x27;潮&#x27;, &#x27;水&#x27;, &#x27;[MASK]&#x27;, &#x27;了&#x27;, &#x27;，&#x27;, &#x27;就&#x27;, &#x27;知&#x27;] ...</span></span><br><span class="line"><span class="string">[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ... </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="Mask模型的使用"><a href="#Mask模型的使用" class="headerlink" title="Mask模型的使用"></a>Mask模型的使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForMaskedLM</span><br><span class="line"><span class="comment"># 除了 tokens 以外我們還需要辨別句子的 segment ids</span></span><br><span class="line">tokens_tensor = torch.tensor([ids])  <span class="comment"># (1, seq_len)</span></span><br><span class="line">segments_tensors = torch.zeros_like(tokens_tensor)  <span class="comment"># (1, seq_len)</span></span><br><span class="line">maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 masked LM 估計 [MASK] 位置所代表的實際 token </span></span><br><span class="line">maskedLM_model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = maskedLM_model(tokens_tensor, segments_tensors)</span><br><span class="line">    predictions = outputs[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># (1, seq_len, num_hidden_units)</span></span><br><span class="line"><span class="keyword">del</span> maskedLM_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來</span></span><br><span class="line">masked_index = <span class="number">5</span></span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">probs, indices = torch.topk(torch.softmax(predictions[<span class="number">0</span>, masked_index], -<span class="number">1</span>), k)</span><br><span class="line">predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 top k 可能的字。一般我們就是取 top 1 当做预测值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;輸入 tokens ：&quot;</span>, tokens[:<span class="number">10</span>], <span class="string">&#x27;...&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> i, (t, p) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(predicted_tokens, probs), <span class="number">1</span>):</span><br><span class="line">    tokens[masked_index] = t</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Top &#123;&#125; (&#123;:2&#125;%)：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="built_in">int</span>(p.item() * <span class="number">100</span>), tokens[:<span class="number">10</span>]), <span class="string">&#x27;...&#x27;</span>)</span><br></pre></td></tr></table></figure><p>​輸入 tokens ： [‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘[MASK]’, ‘了’, ‘，’, ‘就’, ‘知’] …<br>​Top 1 (65%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘来’, ‘了’, ‘，’, ‘就’, ‘知’] …<br>​Top 2 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘过’, ‘了’, ‘，’, ‘就’, ‘知’] …<br>​Top 3 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘干’, ‘了’, ‘，’, ‘就’, ‘知’] …</p><h4 id="可视化模型-bertviz"><a href="#可视化模型-bertviz" class="headerlink" title="可视化模型: bertviz"></a>可视化模型: bertviz</h4><h2 id="Pandas预处理文本"><a href="#Pandas预处理文本" class="headerlink" title="Pandas预处理文本"></a>Pandas预处理文本</h2><ol><li>多使用自定义函数</li><li>nltk库的stopwords</li><li>textblob库的拼写检查、词干抽取、词性还原等</li></ol><h3 id="文本数据的基本体征提取"><a href="#文本数据的基本体征提取" class="headerlink" title="文本数据的基本体征提取"></a>文本数据的基本体征提取</h3><ul><li><p>词汇数量</p></li><li><p>字符数量</p></li><li><p>平均字长</p></li><li><p>停用词数量</p></li><li><p>特殊字符数量</p></li><li><p>数字数量</p></li><li><p>大写字母数量</p></li></ul><h3 id="文本数据的基本预处理"><a href="#文本数据的基本预处理" class="headerlink" title="文本数据的基本预处理"></a>文本数据的基本预处理</h3><ul><li>小写转换</li><li>去除标点符号</li><li>去除停用词</li><li>去除频现词</li><li>去除稀疏词</li><li>拼写校正</li><li>分词(tokenization)</li><li>词干提取(stemming)</li><li>词形还原(lemmatization)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HuggingFace </tag>
            
            <tag> Bert </tag>
            
            <tag> Preprocessing </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/light.js"/>
      <url>/js/light.js</url>
      
        <content type="html"><![CDATA[// 霓虹灯效果// 颜色数组var arr = ["#f14747", "#f1a247", "#f1ee47", "#b347f1", "#1edbff", "#ed709b", "#5636ed"];// 颜色索引var idx = 0;// 切换颜色function changeColor() {    // 仅夜间模式才启用    if (document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark') {        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = arr[idx] + " 0 0 10px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = arr[idx] + " 0 0 5px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = arr[idx] + " 0 0 12px";            document.getElementsByClassName("author-info__description")[0].style.textShadow = arr[idx] + " 0 0 12px";        } catch {                    }        idx++;        if (idx == 8) {            idx = 0;        }    } else {        // 白天模式恢复默认        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = "#1e1e1ee0 1px 1px 1px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = "";            document.getElementsByClassName("author-info__description")[0].style.textShadow = "";        } catch {                    }    }}// 开启计时器window.onload = setInterval(changeColor, 9900);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/categorybar.css"/>
      <url>/css/categorybar.css</url>
      
        <content type="html"><![CDATA[#categoryBar {  width: 100% !important;}ul.categoryBar-list {  margin: 5px 5px 0 5px !important;  padding: 0 !important;}li.categoryBar-list-item {  font-weight: bold;  display: inline-block;  height: 180px !important;  margin: 5px 0.5% 0 0.5% !important;  background-image: linear-gradient(rgba(0,0,0,0.4) 25%, rgba(16,16,16,0) 100%);  border-radius: 10px;  padding: 25px 0 25px 25px !important;  box-shadow: rgba(50,50,50,0.3) 50px 50px 50px 50px inset;  overflow: hidden;  background-size: 100% !important;  background-position: center !important;}li.categoryBar-list-item:hover {  background-size: 110% !important;  box-shadow: inset 500px 50px 50px 50px rgba(50,50,50,0.6);}li.categoryBar-list-item:hover span.categoryBar-list-descr {  transition: all 0.5s;  transform: translate(-100%, 0);}a.categoryBar-list-link {  color: #fff !important;  font-size: 20px !important;}a.categoryBar-list-link::before {  content: '|' !important;  color: #fff !important;  font-size: 20px !important;}a.categoryBar-list-link:after {  content: '';  position: relative;  width: 0;  bottom: 0;  display: block;  height: 3px;  border-radius: 3px;  background-color: #fff;}a.categoryBar-list-link:hover:after {  width: 90%;  left: 1%;  transition: all 0.5s;}span.categoryBar-list-count {  /* display: block !important; */  color: #fff !important;  font-size: 20px !important;}span.categoryBar-list-count::before {  content: '\f02d' !important;  padding-right: 15px !important;  display: inline-block;  font-weight: 600;  font-style: normal;  font-variant: normal;  font-family: 'Font Awesome 6 Free';  text-rendering: auto;  -webkit-font-smoothing: antialiased;}span.categoryBar-list-descr {  padding: 5px;  display: block !important;  color: #fff !important;  font-size: 20px !important;  position: relative;  right: -100%;}@media screen and (max-width: 650px) {  li.categoryBar-list-item {    width: 48% !important;    height: 150px !important;    margin: 5px 1% 0 1% !important;  }}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[/* 页脚与头图透明 */#footer {    background: transparent !important;  }  #page-header {    background: transparent !important;  }    /* 白天模式遮罩透明 */  #footer::before {    background: transparent !important;  }  /* #page-header::before {    background: transparent !important;  } */    /* 夜间模式遮罩透明 */  [data-theme="dark"] #footer::before {    background: transparent !important;  }  [data-theme="dark"] #page-header::before {    background: transparent !important;  }/* 一级菜单居中 *//* #nav .menus_items {    position: absolute !important;    width: fit-content !important;    left: 50% !important;    transform: translateX(-50%) !important;  }  子菜单横向展示 */  /* #nav .menus_items .menus_item:hover .menus_item_child {    display: flex !important;  }  /* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */  /* .menus_items .menus_item:nth-child(2) .menus_item_child {    left: -125px;  } */ *//* 夜间模式菜单栏发光字 */[data-theme="dark"] #nav .site-page,[data-theme="dark"] #nav .menus_items .menus_item .menus_item_child li a {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 手机端适配 */[data-theme="dark"] #sidebar #sidebar-menus .menus_items .site-page {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 侧边栏个人信息卡片动态渐变色 *//* #aside-content > .card-widget.card-info {   background-image: linear-gradient(  to right,   #ff8177 0%,  #ff867a 0%,  #ff8c7f 21%,  #f99185 52%,  #cf556c 78%,  #b12a5b 100%);;    box-shadow: 0 0 5px rgb(66, 68, 68);    position: relative;    background-size: 400% 400%;    -webkit-animation: Gradient 10s ease infinite;    -moz-animation: Gradient 10s ease infinite;    animation: Gradient 10s ease infinite !important;  }  @-webkit-keyframes Gradient {    0% {      background-position: 0% 50%;    }    50% {      background-position: 100% 50%;    }    100% {      background-position: 0% 50%;    }  }  @-moz-keyframes Gradient {    0% {      background-position: 0% 50%;    }    50% {      background-position: 100% 50%;    }    100% {      background-position: 0% 50%;    }  }  @keyframes Gradient {    0% {      background-position: 0% 50%;    }    50% {      background-position: 100% 50%;    }    100% {      background-position: 0% 50%;    }  }      /* 黑夜模式适配 */  [data-theme="dark"] #aside-content > .card-widget.card-info {    background: #191919dd;  }    /* 个人信息Follow me按钮 */  #aside-content > .card-widget.card-info > #card-info-btn {    background-color: #3eb8be;    border-radius: 8px;  }/* 翻页按钮居中 */#pagination {  width: 100%;  margin: auto;}/* 首页 卡片文章背景透明 */#recent-posts>.recent-post-item {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border-radius: 25px;  border: 1px solid rgb(169, 169, 169) }#aside-content .card-widget {  background: #ffffffee;   /* backdrop-filter: var(--backdrop-filter); */  border-radius: 18px;  transition: .3s;  border: 1px solid rgb(169, 169, 169) }div#post {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#page {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#archive {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#tag {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#category {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}#page-header.nav-fixed #nav {  background: rgba(255,255,255,.75);  /* backdrop-filter: var(--backdrop-filter) */}[data-theme=dark] #page-header.nav-fixed #nav {  background: rgba(0,0,0,.7)!important}[data-theme=dark] #recent-posts>.recent-post-item {  background: #191919dd  }[data-theme=dark] #aside-content .card-widget {  background: #191919dd}[data-theme=dark] div#post {  background: #191919dd}[data-theme=dark] div#tag {  background: #191919dd}[data-theme=dark] div#archive {  background: #191919dd}[data-theme=dark] div#page {  background: #191919dd}[data-theme=dark] div#category {  background: #191919dd}[data-theme=dark] #footer::before {  background: 0 0!important}[data-theme=dark] #page-header::before {  background: 0 0!important}.read-mode #aside-content .card-widget {  background: rgba(158,204,171,.5)!important}.read-mode div#post {  background: rgba(158,204,171,.5)!important}[data-theme=dark] .read-mode #aside-content .card-widget {  background: rgba(25,25,25,.9)!important;  color: #fff}[data-theme=dark] .read-mode div#post {  background: rgba(25,25,25,.9)!important;  color: #fff}/* 隐藏分类磁贴数字 */span.categoryBar-list-count {  display: none !important;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>文章分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>知识标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
