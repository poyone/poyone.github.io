<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Scrapy 01 tutorial</title>
      <link href="/posts/28501.html"/>
      <url>/posts/28501.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>如果vscode中你的终端不能识别scrapy可以在环境变量中加入scrapy.exe的路径</p></blockquote><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><p>安装好后，在目标文件夹内启动<code>scrapy startproject tutorial</code>命令，将会创建如下文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tutorial/</span><br><span class="line">    scrapy.cfg            <span class="comment"># deploy configuration file</span></span><br><span class="line"></span><br><span class="line">    tutorial/             <span class="comment"># project&#x27;s Python module, you&#x27;ll import your code from here</span></span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        items.py          <span class="comment"># project items definition file</span></span><br><span class="line"></span><br><span class="line">        middlewares.py    <span class="comment"># project middlewares file</span></span><br><span class="line"></span><br><span class="line">        pipelines.py      <span class="comment"># project pipelines file</span></span><br><span class="line"></span><br><span class="line">        settings.py       <span class="comment"># project settings file</span></span><br><span class="line"></span><br><span class="line">        spiders/          <span class="comment"># a directory where you&#x27;ll later put your spiders</span></span><br><span class="line">            __init__.py</span><br></pre></td></tr></table></figure><p>在<code>tutorial/spiders</code>目录下创建我们的第一个爬虫命名为<code>quotes_spider.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;https://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        page = response.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">2</span>]</span><br><span class="line">        filename = <span class="string">f&#x27;quotes-<span class="subst">&#123;page&#125;</span>.html&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">f&#x27;Saved file <span class="subst">&#123;filename&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>终端中启动爬虫<code>scrapy crawl quotes</code>会得到两个文件<em>quotes-1.html</em> and <em>quotes-2.html</em></p><h1 id="scrapy-shell"><a href="#scrapy-shell" class="headerlink" title="scrapy shell"></a>scrapy shell</h1><p>在解析他两之前，我们介绍 <a href="https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell">Scrapy shell</a>，用来调试我们输出 <code>scrapy shell &lt;url&gt;</code></p><blockquote><p>pip install ipython之后 在上级目录中找到<code>scrapy.cfg</code>文件在<code>setting</code>下加入</p><p><code>shell = bpython</code> 如果你的ipython不能用的话</p><p>输入exit可以退出</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell <span class="string">&quot;https://quotes.toscrape.com/page/1/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[ ... Scrapy log here ... ]</span></span><br><span class="line"><span class="string">2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://quotes.toscrape.com/page/1/&gt; (referer: None)</span></span><br><span class="line"><span class="string">[s] Available Scrapy objects:</span></span><br><span class="line"><span class="string">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span></span><br><span class="line"><span class="string">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;</span></span><br><span class="line"><span class="string">[s]   item       &#123;&#125;</span></span><br><span class="line"><span class="string">[s]   request    &lt;GET https://quotes.toscrape.com/page/1/&gt;</span></span><br><span class="line"><span class="string">[s]   response   &lt;200 https://quotes.toscrape.com/page/1/&gt;</span></span><br><span class="line"><span class="string">[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;</span></span><br><span class="line"><span class="string">[s]   spider     &lt;DefaultSpider &#x27;default&#x27; at 0x7fa91c8af990&gt;</span></span><br><span class="line"><span class="string">[s] Useful shortcuts:</span></span><br><span class="line"><span class="string">[s]   shelp()           Shell help (print this help)</span></span><br><span class="line"><span class="string">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span></span><br><span class="line"><span class="string">[s]   view(response)    View response in a browser&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>以上是返回的一些可以操作的对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;title&#x27;</span>)</span><br><span class="line"><span class="comment"># [&lt;Selector xpath=&#x27;descendant-or-self::title&#x27; data=&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;&gt;]</span></span><br></pre></td></tr></table></figure><p>如此可以实现交互式运行</p><h2 id="css语法"><a href="#css语法" class="headerlink" title="css语法"></a>css语法</h2><h3 id="text"><a href="#text" class="headerlink" title="::text"></a>::text</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;title::text&#x27;</span>).getall()</span><br><span class="line"><span class="comment"># [&#x27;Quotes to Scrape&#x27;]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;title&#x27;</span>).getall()</span><br><span class="line"><span class="comment"># [&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;]</span></span><br></pre></td></tr></table></figure><h3 id="get-x2F-getall"><a href="#get-x2F-getall" class="headerlink" title="get&#x2F;getall"></a>get&#x2F;getall</h3><p>返回一个，或者全部</p><h2 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;title::text&#x27;</span>).re(<span class="string">r&#x27;Quotes.*&#x27;</span>)</span><br><span class="line"><span class="comment"># [&#x27;Quotes to Scrape&#x27;]</span></span><br><span class="line">response.css(<span class="string">&#x27;title::text&#x27;</span>).re(<span class="string">r&#x27;Q\w+&#x27;</span>)</span><br><span class="line"><span class="comment"># [&#x27;Quotes&#x27;]</span></span><br><span class="line">response.css(<span class="string">&#x27;title::text&#x27;</span>).re(<span class="string">r&#x27;(\w+) to (\w+)&#x27;</span>)</span><br><span class="line"><span class="comment"># [&#x27;Quotes&#x27;, &#x27;Scrape&#x27;]</span></span><br></pre></td></tr></table></figure><h2 id="Xpath"><a href="#Xpath" class="headerlink" title="Xpath"></a>Xpath</h2><p><a href="https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors">官方推荐使用这个</a>，但我觉得css写的更方便一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">&#x27;//title&#x27;</span>)</span><br><span class="line"><span class="comment"># [&lt;Selector xpath=&#x27;//title&#x27; data=&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;&gt;]</span></span><br><span class="line">response.xpath(<span class="string">&#x27;//title/text()&#x27;</span>).get()</span><br><span class="line"><span class="comment"># &#x27;Quotes to Scrape&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;div class=&quot;quote&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our</span></span><br><span class="line"><span class="string">    thinking. It cannot be changed without changing our thinking.”&lt;/span&gt;</span></span><br><span class="line"><span class="string">    &lt;span&gt;</span></span><br><span class="line"><span class="string">        by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt;</span></span><br><span class="line"><span class="string">        &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt;</span></span><br><span class="line"><span class="string">    &lt;/span&gt;</span></span><br><span class="line"><span class="string">    &lt;div class=&quot;tags&quot;&gt;</span></span><br><span class="line"><span class="string">        Tags:</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><code>scrapy shell &#39;https://quotes.toscrape.com&#39;</code></p><h2 id="单个提取"><a href="#单个提取" class="headerlink" title="单个提取"></a>单个提取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&quot;div.quote&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&lt;Selector xpath=&quot;descendant-or-self::div[@class and contains(concat(&#x27; &#x27;, normalize-space(@class), &#x27; &#x27;), &#x27; quote &#x27;)]&quot; data=&#x27;&lt;div class=&quot;quote&quot; itemscope itemtype...&#x27;&gt;,</span></span><br><span class="line"><span class="string"> &lt;Selector xpath=&quot;descendant-or-self::div[@class and contains(concat(&#x27; &#x27;, normalize-space(@class), &#x27; &#x27;), &#x27; quote &#x27;)]&quot; data=&#x27;&lt;div class=&quot;quote&quot; itemscope itemtype...&#x27;&gt;,</span></span><br><span class="line"><span class="string"> ...]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>分为两个部分selector和data ，data就是我们操作的分布</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">quote = response.css(<span class="string">&quot;div.quote&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">text = quote.css(<span class="string">&quot;span.text::text&quot;</span>).get()</span><br><span class="line">text</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27; &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">author = quote.css(<span class="string">&quot;small.author::text&quot;</span>).get()</span><br><span class="line">author</span><br><span class="line"><span class="comment"># &#x27;Albert Einstein&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>response.css搜寻的格式为’标签.标签名称’</p><p>quote为我们html文件中所有class&#x3D;quote的标签组，</p><p>组内span.text标签下为名言、组内small.author为作者</p></blockquote><h2 id="小组提取"><a href="#小组提取" class="headerlink" title="小组提取"></a>小组提取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;div class=&quot;quote&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our</span></span><br><span class="line"><span class="string">    thinking. It cannot be changed without changing our thinking.”&lt;/span&gt;</span></span><br><span class="line"><span class="string">    &lt;span&gt;</span></span><br><span class="line"><span class="string">        by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt;</span></span><br><span class="line"><span class="string">        &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt;</span></span><br><span class="line"><span class="string">    &lt;/span&gt;</span></span><br><span class="line"><span class="string">    &lt;div class=&quot;tags&quot;&gt;</span></span><br><span class="line"><span class="string">        Tags:</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tags = quote.css(<span class="string">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class="line">tags</span><br><span class="line"><span class="comment"># [&#x27;change&#x27;, &#x27;deep-thoughts&#x27;, &#x27;thinking&#x27;, &#x27;world&#x27;]</span></span><br></pre></td></tr></table></figure><h2 id="全部提取"><a href="#全部提取" class="headerlink" title="全部提取"></a>全部提取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&quot;div.quote&quot;</span>):</span><br><span class="line">    text = quote.css(<span class="string">&quot;span.text::text&quot;</span>).get()</span><br><span class="line">    author = quote.css(<span class="string">&quot;small.author::text&quot;</span>).get()</span><br><span class="line">    tags = quote.css(<span class="string">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">dict</span>(text=text, author=author, tags=tags))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;text&#x27;: &#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27;, &#x27;author&#x27;: &#x27;Albert Einstein&#x27;, &#x27;tags&#x27;: [&#x27;change&#x27;, &#x27;deep-thoughts&#x27;, &#x27;thinking&#x27;, &#x27;world&#x27;]&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;text&#x27;: &#x27;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#x27;, &#x27;author&#x27;: &#x27;J.K. Rowling&#x27;, &#x27;tags&#x27;: [&#x27;abilities&#x27;, &#x27;choices&#x27;]&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="数据保存"><a href="#数据保存" class="headerlink" title="数据保存"></a>数据保存</h1><p>scrapy crawl spiderman -O spn.json</p><h1 id=""><a href="#" class="headerlink" title=""></a></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/2/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: quote.css(<span class="string">&#x27;small.author::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;tags&#x27;</span>: quote.css(<span class="string">&#x27;div.tags a.tag::text&#x27;</span>).getall(),</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><p>启动爬虫会获得如下内容:</p><p>注要在爬虫的根目录启动爬虫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://quotes.toscrape.com/page/1/&gt;</span></span><br><span class="line"><span class="string">&#123;&#x27;tags&#x27;: [&#x27;life&#x27;, &#x27;love&#x27;], &#x27;author&#x27;: &#x27;André Gide&#x27;, &#x27;text&#x27;: &#x27;“It is better to be hated for what you are than to be loved for what you are not.”&#x27;&#125;</span></span><br><span class="line"><span class="string">2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://quotes.toscrape.com/page/1/&gt;</span></span><br><span class="line"><span class="string">&#123;&#x27;tags&#x27;: [&#x27;edison&#x27;, &#x27;failure&#x27;, &#x27;inspirational&#x27;, &#x27;paraphrased&#x27;], &#x27;author&#x27;: &#x27;Thomas A. Edison&#x27;, &#x27;text&#x27;: &quot;“I have not failed. I&#x27;ve just found 10,000 ways that won&#x27;t work.”&quot;&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p><code>scrapy crawl quotes -O quotes.json</code></p><blockquote><p>-O将会覆写同名文件已存在的内容，</p><p>-o则会在已存在文件的后面增加内容，但是新旧格式可能不同，可以使用</p><p><code>scrapy crawl quotes -o quotes.jsonl</code></p></blockquote><p>有json、jsonl、csv、xml四种格式</p><h1 id="爬取整个网站"><a href="#爬取整个网站" class="headerlink" title="爬取整个网站"></a>爬取整个网站</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;ul class=&quot;pager&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;li class=&quot;next&quot;&gt;</span></span><br><span class="line"><span class="string">        &lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="string">    &lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;/ul&gt;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;li.next a&#x27;</span>).get()</span><br><span class="line"><span class="comment"># &#x27;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line"><span class="comment"># &#x27;/page/2/&#x27;</span></span><br><span class="line"></span><br><span class="line">response.css(<span class="string">&#x27;li.next a&#x27;</span>).attrib[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line"><span class="comment"># &#x27;/page/2/&#x27;</span></span><br></pre></td></tr></table></figure><p><code>::attr()</code>同<code>::text</code>一样是可以调用的属性</p><h2 id="递归调用"><a href="#递归调用" class="headerlink" title="递归调用"></a>递归调用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: quote.css(<span class="string">&#x27;small.author::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;tags&#x27;</span>: quote.css(<span class="string">&#x27;div.tags a.tag::text&#x27;</span>).getall(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            next_page = response.urljoin(next_page)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure><p><code>urljoin()</code>方法创建一个完整的链接，如上<code>next_page</code>这是一个简单的字符串</p><h2 id="response-follow"><a href="#response-follow" class="headerlink" title="response.follow"></a>response.follow</h2><p>使用<code>response.follow</code>方法不需要你有完整的链接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: quote.css(<span class="string">&#x27;span small::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;tags&#x27;</span>: quote.css(<span class="string">&#x27;div.tags a.tag::text&#x27;</span>).getall(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure><p>可以使用<code>response.follow_all </code>平替循环结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> response.css(<span class="string">&#x27;ul.pager a&#x27;</span>):</span><br><span class="line">    <span class="keyword">yield</span> response.follow(a, callback=self.parse)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">anchors = response.css(<span class="string">&#x27;ul.pager a&#x27;</span>)</span><br><span class="line"><span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(anchors, callback=self.parse)</span><br></pre></td></tr></table></figure><p>yield from 使得anchors的数据一个一个出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(css=<span class="string">&#x27;ul.pager a&#x27;</span>, callback=self.parse)</span><br></pre></td></tr></table></figure><h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line">os.chdir(os.path.dirname(os.path.realpath(__file__)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    execute(</span><br><span class="line">        [</span><br><span class="line">            <span class="string">&#x27;scrapy&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;crawl&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;spiderman&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;-o&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;out.json&#x27;</span>,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">except</span> SystemExit:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>在根目录创建run.py函数复制上面内容运行，就可以断点调试了。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>一下是我对作者生平的爬取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;spiderman&quot;</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        author_list = []</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            auth_name = quote.css(<span class="string">&#x27;small.author::text&#x27;</span>).get()</span><br><span class="line">            <span class="keyword">if</span> auth_name <span class="keyword">not</span> <span class="keyword">in</span> author_list:</span><br><span class="line">                author_list.append(auth_name)</span><br><span class="line">                url = <span class="string">&#x27;http://quotes.toscrape.com/author/&#x27;</span>+ auth_name.replace(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.au_parse)</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">au_parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        born_dt = response.css(<span class="string">&#x27;span.author-born-date::text&#x27;</span>).get().strip()</span><br><span class="line">        born_lc = response.css(<span class="string">&#x27;span.author-born-location::text&#x27;</span>).get().strip()</span><br><span class="line">        des = response.css(<span class="string">&#x27;div.author-description::text&#x27;</span>).get().strip()</span><br><span class="line">        name = response.css(<span class="string">&#x27;h3.author-title::text&#x27;</span>).get().strip()</span><br><span class="line">        <span class="keyword">yield</span> &#123;     </span><br><span class="line">                    <span class="string">&#x27;name&#x27;</span>: name,</span><br><span class="line">                    <span class="string">&#x27;data&#x27;</span>: born_dt,</span><br><span class="line">                    <span class="string">&#x27;location&#x27;</span>: born_lc,</span><br><span class="line">                    <span class="string">&#x27;description&#x27;</span>: des,</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure><p>parse函数内处理每个页面的作者名，并组合成作者页面的形式</p><p><code>url = &#39;http://quotes.toscrape.com/author/&#39;+ auth_name.replace(&#39; &#39;, &#39;-&#39;)</code> 生成url送给scrapy生成，</p><p>并调用au_parse函数生成数据</p><p>命令行调用<code>scrapy crawl spiderman -O au_bio.json</code></p>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scrapy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>diffusion 综述</title>
      <link href="/posts/55870.html"/>
      <url>/posts/55870.html</url>
      
        <content type="html"><![CDATA[<h1 id="待排版"><a href="#待排版" class="headerlink" title="待排版"></a>待排版</h1><ul><li><strong>第一部分是DDPM时代的图像编辑</strong>。因为还没有任何的引导生成技术的出现，这一阶段的论文都属于利用输入图像引导生成的范式。</li><li><strong>第二部分是在显式分类器引导生成技术出现后，基于CLIP模型的多模态引导生成技术的调研。</strong></li><li><strong>第三部分是最近（2022.11）一两个月基于Stable-Diffusion&#x2F;Imagen等一系列模型所产生的图像编辑技术的调研</strong>。</li></ul><p><del>不像人话</del></p><p>第一阶段: DDPM</p><p>加噪 diffusion 再降噪还原 全局修改</p><p>逐步发现对梯度的控制很重要，于是加入对梯度控制。DDPM-&gt;IVLR-&gt;SDEdit-&gt;RePaint</p><p>最后从打补丁控制生成的基础上，引导出了对导数的控制</p><p>第二阶段: DDIM </p><p>Diffusion Models Beat GANs on Image Synthesis 加入10倍的定向梯度控制 </p><p>More Control for Free! Image Synthesis with Semantic Diffusion Guidance : CLIP 可以局部修改</p><p><code>想要使用一个文本来引导图像生成，我们可以每一步都计算现在的图像表征和文本表征的距离，使用方程的梯度来计算缩小这个距离的方向</code></p><p>但就在十天之后OpenAI发布了GLIDE，使用了下面会提到的隐式分类器引导的图像生成</p><p>随着新的更强大更便捷的模型如Stable-Diffusion, Imagen等如雨后春笋般涌现，上面的各项工作可能只剩下了借鉴意义。</p><p>Classifier-Free Diffusion Guidance :基于隐式分类器的文生图大模型</p><p>无分类器引导可以说是GLIDE&#x2F;Stable-Diffusion&#x2F;Imagen的做法的直接奠基工作之一</p><p>第三阶段: </p><p>在隐式分类器上引导生成过程中的调控生成</p><blockquote><p>第一种是根据扩散模型迭代去噪的特性，我们在模型的低频细节上继续生成。这种做法虽然能保留大部分几何特征，但是也同样<strong>无法调控几何特征。</strong></p></blockquote><p>Imagic: Text-Based Real Image Editing with Diffusion Models:</p><p>具体来说，Imagic将概念绑定这件事拆成了三个步骤，对于输入图像x和我们希望生成的目标描述文本text_target来说：</p><p>1：我们首先冻结整个模型，使用模型训练时的生成目标来微调text_target的文本表征，使其接近于图像的表征。</p><p>2：我们放开整个模型的权重更新，依然使用训练时的生成目标，但这次全模型微调。模型的输入是图像x和我们微调后的文本表征。这一步是因为哪怕我们让目标文本表征和原图的表征接近了，也不能保证我们输入让我们微调后的目标文本表征可以生成我们的原图，所以我们再次将这两个概念一起训练，<strong>使得我们可以使用微调后的目标文本表征生成我们的原图</strong></p><p>3：既然我们已经将原图和微调后的新文本表征绑定起来了，现在我们再使用<strong>原本的目标文本表征</strong>与微调后的文本表征做插值，来对原图像施加影响即可。</p><blockquote><p>训练好图形输出锁住参数-&gt; 微调全参数适应文本输出-&gt; 开放全参数两个一起训练 -&gt;将两个概念捆绑并开放修改</p><p>简单来讲可以将微调后的目标文本表征近似当作原图像原生的文本表征，那么最后一步使用目标表征对原生的表征施加影响就非常自然了</p></blockquote><p>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</p><p>具体来说作者提出了<strong>使用稀缺词加种类词</strong>如“beikkpic dog”的组合文本<strong>来微调一组照片和这个文本的绑定</strong>。但是仅仅用<strong>少量的照片</strong>来微调一个有着大量参数的模型很明显会<strong>带来极为严重的过拟合</strong>。并且还会带来一个语言模型里特别常见的事情–<strong>灾难性遗忘</strong>。这两个问题的表现<strong>一个是绑定词的形态很难变换</strong>，就如上篇的Unitune一样。另一个问题是对种类词里面的种类生成也会<strong>快速失去多样性和变化性</strong>。于是针对这个问题作者针对性地提出了一个叫<code>自身类别先验保存损失的损失函数</code>。</p><p>这个函数的设计是在用户提供一个指定的类别和这个类别的一组图片（如自家的宠物狗的多张照片）后，模型<strong>同时使用“特殊词+类别”对用户照片训练和“类别”与模型生成的该类别图训练</strong>。这样做的好处是模型可以在将<strong>特定的照片主体与特殊词绑定的时候可以一起学到和其类别的关系</strong>，并且同时<strong>该类别的信息在不断的被重申以对抗用户照片信息的冲击</strong>。作者在训练的时候特意将这两个损失以一比一的比例训练了200个轮次左右。(单卡GPU 15分钟左右就可以)</p><p>Prompt-to-Prompt Image Editing with Cross-Attention Control</p><p>这篇文章的洞见来自于一个重要思考：即多模态里文生图的文本是如何对生成过程施加影响的？</p><blockquote><p>基于隐式分类器的文图模型是通过训练一个既可以做无条件生成的梯度预估，也可以做条件生成的梯度预估的模型实现的。而其中这个条件交互的方式在Imagen和Stable-Diffusion里都是通过cross-attention实现信息融合的。那么很明显，我们的着眼点也应该在cross-attention上。</p></blockquote><p>而作者的洞见则在于：<strong>我们输入的文本和像素之间存在着一个空间对应的关系。通过调控注意力和像素间的映射。我们能够对图像的不同区域实施准确的引导。</strong></p><p>![](..&#x2F;..&#x2F;article_img&#x2F;paper_img&#x2F;diffusion&#x2F;bear attn.png)</p><p>有了以上洞见据此进行图像引导生成就很直观了，作者将其分为三个主要场景：<strong>单词替换</strong>（比如在上图里将熊换成猫则将猫这个token对应的map换成熊的map），<strong>单词增添</strong>（在原有的map上增加新的单词的map），<strong>注意力重加权</strong>（如果想放大或减弱某个词对原图的引导效果则对其map乘上新的权重值，如降低下雪的效果开花的程度等）</p><p><a href="https://mp.weixin.qq.com/s/brvSAAmhkSKTTOXZqT0HKQ">原链</a></p>]]></content>
      
      
      <categories>
          
          <category> Dive Into Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>22-12-03 trick总结</title>
      <link href="/posts/43944.html"/>
      <url>/posts/43944.html</url>
      
        <content type="html"><![CDATA[<h1 id="巧取下标"><a href="#巧取下标" class="headerlink" title="巧取下标"></a>巧取下标</h1><p>NER任务中对 标签下标的偏移量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(2,4).softmax(dim=-1)</span><br><span class="line">b = torch.randn(2,4).softmax(dim=-1)</span><br><span class="line">a,b</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">(tensor([[0.1310, 0.3604, 0.1726, 0.3360],</span><br><span class="line">         [0.1955, 0.3245, 0.0700, 0.4101]]),</span><br><span class="line"> tensor([[0.2199, 0.1130, 0.3976, 0.2695],</span><br><span class="line">         [0.0455, 0.0804, 0.0514, 0.8227]]))&#x27;&#x27;&#x27;</span><br><span class="line">         </span><br><span class="line">scores = a.unsqueeze(1).transpose(-1,-2) @ b.unsqueeze(1)</span><br><span class="line">scores</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">tensor([[[0.0288, 0.0148, 0.0521, 0.0353],</span><br><span class="line">         [0.0792, 0.0407, 0.1433, 0.0971],</span><br><span class="line">         [0.0380, 0.0195, 0.0686, 0.0465],</span><br><span class="line">         [0.0739, 0.0380, 0.1336, 0.0905]],</span><br><span class="line"></span><br><span class="line">        [[0.0089, 0.0157, 0.0100, 0.1608],</span><br><span class="line">         [0.0148, 0.0261, 0.0167, 0.2670],</span><br><span class="line">         [0.0032, 0.0056, 0.0036, 0.0576],</span><br><span class="line">         [0.0187, 0.0330, 0.0211, 0.3374]]])&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure><p>以上我们需要在三维张量中得到二维矩阵中最大值的坐标(x, y)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">fin = torch.triu(scores)</span><br><span class="line">fin</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">tensor([[[0.0288, 0.0148, 0.0521, 0.0353],</span><br><span class="line">         [0.0000, 0.0407, 0.1433, 0.0971],</span><br><span class="line">         [0.0000, 0.0000, 0.0686, 0.0465],</span><br><span class="line">         [0.0000, 0.0000, 0.0000, 0.0905]],</span><br><span class="line"></span><br><span class="line">        [[0.0089, 0.0157, 0.0100, 0.1608],</span><br><span class="line">         [0.0000, 0.0261, 0.0167, 0.2670],</span><br><span class="line">         [0.0000, 0.0000, 0.0036, 0.0576],</span><br><span class="line">         [0.0000, 0.0000, 0.0000, 0.3374]]])&#x27;&#x27;&#x27;</span><br><span class="line">         </span><br><span class="line">fin[0].argmax(), fin[0].argmax() % 4, fin[0].argmax() // 4</span><br><span class="line"></span><br><span class="line"># (tensor(6), tensor(2), tensor(1)) ——&gt; 0.1433</span><br></pre></td></tr></table></figure><p>以上我们通过对<code>argmax()</code>返回的绝对坐标，对绝对坐标<code>%</code>得到列坐标(填满多少行后，余量在本行的列位)，<code>//</code>得到行坐标(整除的行号)</p><p>接下来对fin的每个矩阵做循环即可。</p><h1 id="JNotebook-魔法命令"><a href="#JNotebook-魔法命令" class="headerlink" title="JNotebook 魔法命令"></a>JNotebook 魔法命令</h1><p><code>%%time</code> 可以查看运行时间</p><p><code>!zip</code> <code>!unzip</code>  </p><p><code>!pip</code></p><h2 id="kaggle-数据加载"><a href="#kaggle-数据加载" class="headerlink" title="kaggle 数据加载"></a>kaggle 数据加载</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!mkdir -p ~/.kaggle/# 创建一个专门的文件夹</span><br><span class="line">!mv /content/kaggle.json ~/.kaggle/  # 将json转到你的kaggle文件</span><br><span class="line">!kaggle competitions download -c ml2021-spring-hw7    # kaggle api 要去比赛处同意条款 获得</span><br><span class="line">!unzip /content/ml2021-spring-hw7.zip   #解压文件</span><br></pre></td></tr></table></figure><h1 id="图片查看"><a href="#图片查看" class="headerlink" title="图片查看"></a>图片查看</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 将数据集 使用dataloader装载后，使其可迭代，并用next方法调用</span><br><span class="line"># samples, labels = next(iter(train_dl)) 取出第一个batch的data和target</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10,5))</span><br><span class="line">for i, imgs in enumerate(samples[:20], 0):</span><br><span class="line">    print(imgs.size())</span><br><span class="line">    npimg = imgs.numpy().transpose((1, 2, 0))</span><br><span class="line">    plt.subplot(2, 10, i+1)</span><br><span class="line">    plt.imshow(npimg, cmap=plt.cm.binary)</span><br><span class="line">    plt.axis(&#x27;off&#x27;)</span><br><span class="line">print(labels[:10])</span><br></pre></td></tr></table></figure><h1 id="tensor转换"><a href="#tensor转换" class="headerlink" title="tensor转换"></a>tensor转换</h1><p>尽量使用<code>torch.as_tensor(data: Any, dtype: _dtype=None, device: Optional[_device]=None)</code></p>]]></content>
      
      
      <categories>
          
          <category> Trick </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Code Collection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 09 Custom Tokenizer</title>
      <link href="/posts/7194.html"/>
      <url>/posts/7194.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>与之前继承式的分词器不同，这词我们将从语料库中训练一个全新的分词器</p><p>首先我们设置一个WordPiece类型的分词器</p></blockquote><h1 id="加载文档"><a href="#加载文档" class="headerlink" title="加载文档"></a>加载文档</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), <span class="number">1000</span>):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + <span class="number">1000</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以从本地打开文档</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;wikitext-2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">        f.write(dataset[i][<span class="string">&quot;text&quot;</span>] + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="加载构件"><a href="#加载构件" class="headerlink" title="加载构件"></a>加载构件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (</span><br><span class="line">    decoders,</span><br><span class="line">    models,</span><br><span class="line">    normalizers,</span><br><span class="line">    pre_tokenizers,</span><br><span class="line">    processors,</span><br><span class="line">    trainers,</span><br><span class="line">    Tokenizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br></pre></td></tr></table></figure><p>我们从<code>tokenizer库</code>中加载特殊的<code>model</code>构件，来使用<code>WordPiece</code>方法</p><p>设定遇到没见过的词标记为[UNK]， 同时可以设置<code>max_input_chars_per_word</code>作为最大词长</p><h2 id="设置Normalizer"><a href="#设置Normalizer" class="headerlink" title="设置Normalizer"></a>设置Normalizer</h2><p>这里我们选择bert的设置，包括: </p><p>所有字母小写、strip_accents除去重音、删除控制字符、将所有多个空格设置为单个空格、汉字周围放置空格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你也可以自定义</span></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="string">&quot;Héllò hôw are ü?&quot;</span>))</span><br><span class="line"><span class="comment"># hello how are u?</span></span><br></pre></td></tr></table></figure><p>上面自定义中我们使用<code>Sequence</code>方法定义我们自己的规范化规则</p><h2 id="Pre-tokenization"><a href="#Pre-tokenization" class="headerlink" title="Pre-tokenization"></a>Pre-tokenization</h2><p>和上面一样可以通过<code>tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</code>套用bert的设置</p><p>下面是custom版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()</span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)),</span></span><br><span class="line"><span class="string"> (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><code>.Whitespace()</code>是对标点空格分隔，你可用下面的分隔</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pre_tokenizer = pre_tokenizers.WhitespaceSplit()</span><br><span class="line">pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&quot;Let&#x27;s&quot;, (0, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre-tokenizer.&#x27;, (14, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>推荐使用Sequence方法组合你的预分词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pre_tokenizer = pre_tokenizers.<span class="type">Sequence</span>(</span><br><span class="line">    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]</span><br><span class="line">)</span><br><span class="line">pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)),</span></span><br><span class="line"><span class="string"> (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p>训练之前我们需要加入特殊token因为他不在你的词库之中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br></pre></td></tr></table></figure><ul><li>As well as specifying the <code>vocab_size</code> and <code>special_tokens</code>, we can set the <code>min_frequency</code> (the number of times a token must appear to be included in the vocabulary) or change the <code>continuing_subword_prefix</code> (if we want to use something different from <code>##</code>).<ul><li>改某个token必须出现多少次、改连接前缀##为别的</li></ul></li></ul><p>开始训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另外的版本</span></span><br><span class="line">tokenizer.model = models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>)</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure><p>第一种方法是用上面定义的生成器</p><p>第二种传入”wikitext-2.txt”文件</p><p>到此我们tokenizer就具有了一般tokenizer的所有方法如<code>encode</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [&#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;]</span></span><br></pre></td></tr></table></figure><h2 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h2><p>最后我们需要包裹我们的token到特殊的格式如: [CLS]…[SEP]…[SEP]</p><p>首先我们获取所需的特殊token的下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2, 3)</span></span><br></pre></td></tr></table></figure><p>接下来处理我们的模板</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id)],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>模板我们需要设置两种模式:</p><ul><li><p>single–单个句子情况下</p><ul><li>[0,0,0,0]</li></ul></li><li><p>pair</p><ul><li>[0,0,0,1,1,1]</li></ul></li><li><p>最后指定特殊token的id</p></li></ul><p>查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="comment"># [&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span></span><br><span class="line"></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="string">&quot;on a pair of sentences.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="built_in">print</span>(encoding.type_ids)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;...&#x27;, &#x27;[SEP]&#x27;, &#x27;on&#x27;, &#x27;a&#x27;, &#x27;pair&#x27;, &#x27;of&#x27;, &#x27;sentences&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span></span><br><span class="line"><span class="string">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h2><p>接下来对解码器做一定设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.decode(encoding.ids)</span><br><span class="line"><span class="comment"># &quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span></span><br></pre></td></tr></table></figure><h2 id="保存-amp-加载-Custom-Tokenizer"><a href="#保存-amp-加载-Custom-Tokenizer" class="headerlink" title="保存 &amp; 加载 Custom Tokenizer"></a>保存 &amp; 加载 Custom Tokenizer</h2><p><code>tokenizer.save(&quot;tokenizer.json&quot;)</code></p><p><code>new_tokenizer = Tokenizer.from_file(&quot;tokenizer.json&quot;)</code></p><h1 id="转成Fast-Tokenizer"><a href="#转成Fast-Tokenizer" class="headerlink" title="转成Fast Tokenizer"></a>转成Fast Tokenizer</h1><p>To use this tokenizer in 🤗 Transformers, we have to wrap it in a <code>PreTrainedTokenizerFast</code>. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, <code>BertTokenizerFast</code>). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.</p><ul><li>可以继承你的特定类<code>BertTokenizerFast</code>，也可以用泛类<code>PreTrainedTokenizerFast</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    <span class="comment"># tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively</span></span><br><span class="line">    unk_token=<span class="string">&quot;[UNK]&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;[PAD]&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;[CLS]&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;[SEP]&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;[MASK]&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里可以从文件中加载你的tokenizer设置、也可直接赋值、注意你的<code>特殊符号必须重新定义</code></p><h1 id="BPE类型的分词器"><a href="#BPE类型的分词器" class="headerlink" title="BPE类型的分词器"></a>BPE类型的分词器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.BPE())</span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;s&quot;, (3, 5)), (&#x27;Ġtest&#x27;, (5, 10)), (&#x27;Ġpre&#x27;, (10, 14)), (&#x27;-&#x27;, (14, 15)),</span></span><br><span class="line"><span class="string"> (&#x27;tokenization&#x27;, (15, 27)), (&#x27;!&#x27;, (27, 28))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>GPT2只需要开始和结束的特殊token</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">trainer = trainers.BpeTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>])</span><br><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br><span class="line"></span><br><span class="line">tokenizer.model = models.BPE()</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br><span class="line"></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;L&#x27;, &#x27;et&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;Ġtest&#x27;, &#x27;Ġthis&#x27;, &#x27;Ġto&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span></span><br><span class="line">encoding = tokenizer.encode(sentence)</span><br><span class="line">start, end = encoding.offsets[<span class="number">4</span>]</span><br><span class="line">sentence[start:end]</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27; test&#x27;</span></span><br></pre></td></tr></table></figure><p>The <code>trim_offsets = False</code> option indicates to the post-processor that we should leave the offsets of tokens that begin with ‘Ġ’ as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Let’s have a look at the result with the text we just encoded, where <code>&#39;Ġtest&#39;</code> is the token at index 4</p><ul><li><code>trim_offsets</code>设定是否修正字符的空格位置进入偏移量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.ByteLevel()</span><br><span class="line">tokenizer.decode(encoding.ids)</span><br><span class="line"><span class="comment"># &quot;Let&#x27;s test this tokenizer.&quot;</span></span><br></pre></td></tr></table></figure><p>包装</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2TokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure><h1 id="Unigram类型的分词器"><a href="#Unigram类型的分词器" class="headerlink" title="Unigram类型的分词器"></a>Unigram类型的分词器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.Unigram())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Regex</span><br><span class="line"></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [</span><br><span class="line">        normalizers.Replace(<span class="string">&quot;``&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>),</span><br><span class="line">        normalizers.Replace(<span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>),</span><br><span class="line">        normalizers.NFKD(),</span><br><span class="line">        normalizers.StripAccents(),</span><br><span class="line">        normalizers.Replace(Regex(<span class="string">&quot; &#123;2,&#125;&quot;</span>), <span class="string">&quot; &quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>第一、二个norm将符号替换，最后一个将多个空格替换成一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()</span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [(&quot;▁Let&#x27;s&quot;, (0, 5)), (&#x27;▁test&#x27;, (5, 10)), (&#x27;▁the&#x27;, (10, 14)), (&#x27;▁pre-tokenizer!&#x27;, (14, 29))]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;&lt;cls&gt;&quot;</span>, <span class="string">&quot;&lt;sep&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;mask&gt;&quot;</span>, <span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>]</span><br><span class="line">trainer = trainers.UnigramTrainer(</span><br><span class="line">    vocab_size=<span class="number">25000</span>, special_tokens=special_tokens, unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">tokenizer.model = models.Unigram()</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;▁Let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;▁test&#x27;, &#x27;▁this&#x27;, &#x27;▁to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;&lt;cls&gt;&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;&lt;sep&gt;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"><span class="comment"># 0 1</span></span><br><span class="line"></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,</span><br><span class="line">    pair=<span class="string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="string">&quot;on a pair of sentences!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="built_in">print</span>(encoding.type_ids)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;▁Let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;▁test&#x27;, &#x27;▁this&#x27;, &#x27;▁to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;, &#x27;.&#x27;, &#x27;.&#x27;, &#x27;&lt;sep&gt;&#x27;, &#x27;▁&#x27;, &#x27;on&#x27;, &#x27;▁&#x27;, &#x27;a&#x27;, &#x27;▁pair&#x27;, </span></span><br><span class="line"><span class="string">  &#x27;▁of&#x27;, &#x27;▁sentence&#x27;, &#x27;s&#x27;, &#x27;!&#x27;, &#x27;&lt;sep&gt;&#x27;, &#x27;&lt;cls&gt;&#x27;]</span></span><br><span class="line"><span class="string">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>XLnet填充pad在左边且[CLS]在最后，这些我们都需要指明给Fast</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;s&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span><br><span class="line">    unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;&lt;pad&gt;&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;&lt;cls&gt;&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;&lt;sep&gt;&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;&lt;mask&gt;&quot;</span>,</span><br><span class="line">    padding_side=<span class="string">&quot;left&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 08 Tokenizer底层算法</title>
      <link href="/posts/7193.html"/>
      <url>/posts/7193.html</url>
      
        <content type="html"><![CDATA[<h1 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h1><p>一般来说我们的tokenizer有如下流程</p><p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline."></p><h2 id="规范化"><a href="#规范化" class="headerlink" title="规范化"></a>规范化</h2><p>规范化是对字符做大小写处理之类的我们可以通过如下API查看底层的normalization方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tokenizer.backend_tokenizer))</span><br><span class="line"><span class="comment"># &lt;class &#x27;tokenizers.Tokenizer&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenizer.backend_tokenizer.normalizer.normalize_str(<span class="string">&quot;Héllò hôw are ü?&quot;</span>))</span><br><span class="line"><span class="comment"># &#x27;hello how are u?&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="预分词"><a href="#预分词" class="headerlink" title="预分词"></a>预分词</h2><p>通过如下api查看分词器是如何做pre_tokenize的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;how&#x27;, (7, 10)), (&#x27;are&#x27;, (11, 14)), (&#x27;you&#x27;, (16, 19)), (&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>可以看到后面的偏移量坐标，这也是上一节offset-mapping的由来</p><h2 id="不同的预分词"><a href="#不同的预分词" class="headerlink" title="不同的预分词"></a>不同的预分词</h2><p>gpt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;Ġhow&#x27;, (6, 10)), (&#x27;Ġare&#x27;, (10, 14)), (&#x27;Ġ&#x27;, (14, 15)), (&#x27;Ġyou&#x27;, (15, 19)),(&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;</span><span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><p>t5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;t5-small&quot;</span>)</span><br><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line"><span class="comment"># [(&#x27;▁Hello,&#x27;, (0, 6)), (&#x27;▁how&#x27;, (7, 10)), (&#x27;▁are&#x27;, (11, 14)), (&#x27;▁you?&#x27;, (16, 20))]</span></span><br></pre></td></tr></table></figure><h1 id="三种分词算法总览"><a href="#三种分词算法总览" class="headerlink" title="三种分词算法总览"></a>三种分词算法总览</h1><p>如上，不同的模型适用不同的分词算法</p><h2 id="sentencepiece"><a href="#sentencepiece" class="headerlink" title="sentencepiece"></a>sentencepiece</h2><p>它经常与unigram算法一起，且并不需要预分词，是特攻中文日文，这种无法分词的语言的</p><h2 id="算法预览"><a href="#算法预览" class="headerlink" title="算法预览"></a>算法预览</h2><table><thead><tr><th>Model</th><th>BPE</th><th>WordPiece</th><th>Unigram</th></tr></thead><tbody><tr><td>Training</td><td>Starts from a small vocabulary and learns rules to merge tokens</td><td>Starts from a small vocabulary and learns rules to merge tokens</td><td>Starts from a large vocabulary and learns rules to remove tokens</td></tr><tr><td>Training step</td><td>Merges the tokens corresponding to the most common pair</td><td>Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent</td><td>Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus</td></tr><tr><td>Learns</td><td>Merge rules and a vocabulary</td><td>Just a vocabulary</td><td>A vocabulary with a score for each token</td></tr><tr><td>Encoding</td><td>Splits a word into characters and applies the merges learned during training</td><td>Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word</td><td>Finds the most likely split into tokens, using the scores learned during training</td></tr></tbody></table><h1 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h1><h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>BPE是Byte-Pair Encoding 的简写，他有三步</p><ol><li>将corpus所有独一无二字符拆出来，如英文中的26个字母，标点和其他特殊符号</li><li>在有基础字符的基础上，以频率作为选取标准，将两个字符匹配在一起，选择频率最高的词进行入库</li><li>重复第二步直到满足你设定的词库大小</li></ol><blockquote><p>The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don’t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called <em>byte-level BPE</em>.</p><p>GPT和roberta使用的是比特级别的字符，就是0100这种，这就是他们的基础语料库，然后在基础上融合出来词进行构建词库</p></blockquote><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>下面进行实例解析设定语料库如下</p><p>语料库: <code>&quot;hug&quot;, &quot;pug&quot;, &quot;pun&quot;, &quot;bun&quot;, &quot;hugs&quot;</code></p><p>词频: <code>(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (&quot;h&quot; &quot;u&quot; &quot;g&quot;, 10), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, 5)</span></span><br></pre></td></tr></table></figure><p>第一轮</p><p>最多的是 ug的组合，20次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;]</span></span><br><span class="line"><span class="string">Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>第二轮</p><p>最多是un</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;]</span></span><br><span class="line"><span class="string">Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>第三轮</p><p>最多的是hug</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]</span></span><br><span class="line"><span class="string">Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>…如何循环到设定的词库大小</p><h2 id="简要代码"><a href="#简要代码" class="headerlink" title="简要代码"></a>简要代码</h2><p>语料库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>统计词频</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(word_freqs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">defaultdict(int, &#123;&#x27;This&#x27;: 3, &#x27;Ġis&#x27;: 2, &#x27;Ġthe&#x27;: 1, &#x27;ĠHugging&#x27;: 1, &#x27;ĠFace&#x27;: 1, &#x27;ĠCourse&#x27;: 1, &#x27;.&#x27;: 4, &#x27;Ġchapter&#x27;: 1,</span></span><br><span class="line"><span class="string">    &#x27;Ġabout&#x27;: 1, &#x27;Ġtokenization&#x27;: 1, &#x27;Ġsection&#x27;: 1, &#x27;Ġshows&#x27;: 1, &#x27;Ġseveral&#x27;: 1, &#x27;Ġtokenizer&#x27;: 1, &#x27;Ġalgorithms&#x27;: 1,</span></span><br><span class="line"><span class="string">    &#x27;Hopefully&#x27;: 1, &#x27;,&#x27;: 1, &#x27;Ġyou&#x27;: 1, &#x27;Ġwill&#x27;: 1, &#x27;Ġbe&#x27;: 1, &#x27;Ġable&#x27;: 1, &#x27;Ġto&#x27;: 1, &#x27;Ġunderstand&#x27;: 1, &#x27;Ġhow&#x27;: 1,</span></span><br><span class="line"><span class="string">    &#x27;Ġthey&#x27;: 1, &#x27;Ġare&#x27;: 1, &#x27;Ġtrained&#x27;: 1, &#x27;Ġand&#x27;: 1, &#x27;Ġgenerate&#x27;: 1, &#x27;Ġtokens&#x27;: 1&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>首先载入gpt的分词器，做预分词</li><li>再载入collection中的defaultdict设定为int类型</li></ul><p>基础词汇表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">alphabet = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys():</span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">        <span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">            alphabet.append(letter)</span><br><span class="line">alphabet.sort()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(alphabet)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[ &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;r&#x27;, &#x27;s&#x27;,</span></span><br><span class="line"><span class="string">  &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;Ġ&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>加个表头<code>vocab = [&quot;&lt;|endoftext|&gt;&quot;] + alphabet.copy()</code></p><p>将单词映射为{‘word’: [‘w’, ‘o’, ‘r’, ‘d’]}的形式进行训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">splits = &#123;word: [c <span class="keyword">for</span> c <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我觉可以改一下</span></span><br><span class="line">`splits = &#123;word: <span class="built_in">list</span>(word) <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys()&#125;`</span><br></pre></td></tr></table></figure><p>字母对频率函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_pair_freqs</span>(<span class="params">splits</span>):</span><br><span class="line">    pair_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">        split = splits[word]<span class="comment"># 取得word对应的值如[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(split) - <span class="number">1</span>):</span><br><span class="line">            pair = (split[i], split[i + <span class="number">1</span>])</span><br><span class="line">            pair_freqs[pair] += freq<span class="comment"># 记录字母对的频率</span></span><br><span class="line">    <span class="keyword">return</span> pair_freqs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">pair_freqs = compute_pair_freqs(splits)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> <span class="built_in">enumerate</span>(pair_freqs.keys()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;pair_freqs[key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> i &gt;= <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&#x27;T&#x27;, &#x27;h&#x27;): 3</span></span><br><span class="line"><span class="string">(&#x27;h&#x27;, &#x27;i&#x27;): 3</span></span><br><span class="line"><span class="string">(&#x27;i&#x27;, &#x27;s&#x27;): 5</span></span><br><span class="line"><span class="string">(&#x27;Ġ&#x27;, &#x27;i&#x27;): 2</span></span><br><span class="line"><span class="string">(&#x27;Ġ&#x27;, &#x27;t&#x27;): 7</span></span><br><span class="line"><span class="string">(&#x27;t&#x27;, &#x27;h&#x27;): 3&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取最大值</span></span><br><span class="line">best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">max_freq = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items():</span><br><span class="line">    <span class="keyword">if</span> max_freq <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_freq &lt; freq:</span><br><span class="line">        best_pair = pair</span><br><span class="line">        max_freq = freq</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(best_pair, max_freq)</span><br><span class="line"><span class="comment"># (&#x27;Ġ&#x27;, &#x27;t&#x27;) 7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并入库</span></span><br><span class="line">merges = &#123;(<span class="string">&quot;Ġ&quot;</span>, <span class="string">&quot;t&quot;</span>): <span class="string">&quot;Ġt&quot;</span>&#125;</span><br><span class="line">vocab.append(<span class="string">&quot;Ġt&quot;</span>)</span><br></pre></td></tr></table></figure><p>将字符对构建进新的基础词表split (不是vocab)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_pair</span>(<span class="params">a, b, splits</span>):</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs:</span><br><span class="line">        split = splits[word] <span class="comment"># 取得word对应的值如[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> split[i] == a <span class="keyword">and</span> split[i + <span class="number">1</span>] == b: </span><br><span class="line">            <span class="comment"># 找到词对的位置，将ab字符串连接起来，做个列表存起来</span></span><br><span class="line">                split = split[:i] + [a + b] + split[i + <span class="number">2</span> :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += <span class="number">1</span> </span><br><span class="line">        splits[word] = split <span class="comment"># 更新 [&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;] -&gt; [&#x27;wo&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]</span></span><br><span class="line">    <span class="keyword">return</span> splits</span><br><span class="line"></span><br><span class="line">splits = merge_pair(<span class="string">&quot;Ġ&quot;</span>, <span class="string">&quot;t&quot;</span>, splits)</span><br><span class="line"><span class="built_in">print</span>(splits[<span class="string">&quot;Ġtrained&quot;</span>])</span><br><span class="line"><span class="comment"># [&#x27;Ġt&#x27;, &#x27;r&#x27;, &#x27;a&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]</span></span><br></pre></td></tr></table></figure><p>构建循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(vocab) &lt; vocab_size:</span><br><span class="line">    pair_freqs = compute_pair_freqs(splits) <span class="comment"># 计算配对的频率</span></span><br><span class="line">    best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">    max_freq = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items(): <span class="comment"># 最大值</span></span><br><span class="line">        <span class="keyword">if</span> max_freq <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_freq &lt; freq:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_freq = freq</span><br><span class="line">    splits = merge_pair(*best_pair, splits) <span class="comment"># merge并更新</span></span><br><span class="line">    merges[best_pair] = best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>] <span class="comment"># 记录merge规则</span></span><br><span class="line">    vocab.append(best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(merges)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;(&#x27;Ġ&#x27;, &#x27;t&#x27;): &#x27;Ġt&#x27;, (&#x27;i&#x27;, &#x27;s&#x27;): &#x27;is&#x27;, (&#x27;e&#x27;, &#x27;r&#x27;): &#x27;er&#x27;, (&#x27;Ġ&#x27;, &#x27;a&#x27;): &#x27;Ġa&#x27;, (&#x27;Ġt&#x27;, &#x27;o&#x27;): &#x27;Ġto&#x27;, (&#x27;e&#x27;, &#x27;n&#x27;): &#x27;en&#x27;,</span></span><br><span class="line"><span class="string"> (&#x27;T&#x27;, &#x27;h&#x27;): &#x27;Th&#x27;, (&#x27;Th&#x27;, &#x27;is&#x27;): &#x27;This&#x27;, (&#x27;o&#x27;, &#x27;u&#x27;): &#x27;ou&#x27;, (&#x27;s&#x27;, &#x27;e&#x27;): &#x27;se&#x27;, (&#x27;Ġto&#x27;, &#x27;k&#x27;): &#x27;Ġtok&#x27;,</span></span><br><span class="line"><span class="string"> (&#x27;Ġtok&#x27;, &#x27;en&#x27;): &#x27;Ġtoken&#x27;, (&#x27;n&#x27;, &#x27;d&#x27;): &#x27;nd&#x27;, (&#x27;Ġ&#x27;, &#x27;is&#x27;): &#x27;Ġis&#x27;, (&#x27;Ġt&#x27;, &#x27;h&#x27;): &#x27;Ġth&#x27;, (&#x27;Ġth&#x27;, &#x27;e&#x27;): &#x27;Ġthe&#x27;,</span></span><br><span class="line"><span class="string"> (&#x27;i&#x27;, &#x27;n&#x27;): &#x27;in&#x27;, (&#x27;Ġa&#x27;, &#x27;b&#x27;): &#x27;Ġab&#x27;, (&#x27;Ġtoken&#x27;, &#x27;i&#x27;): &#x27;Ġtokeni&#x27;&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>查看词表vocab</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(vocab)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;&lt;|endoftext|&gt;&#x27;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;p&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;Ġ&#x27;, &#x27;Ġt&#x27;, &#x27;is&#x27;, &#x27;er&#x27;, &#x27;Ġa&#x27;, &#x27;Ġto&#x27;, &#x27;en&#x27;, &#x27;Th&#x27;, &#x27;This&#x27;, &#x27;ou&#x27;, &#x27;se&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;Ġtok&#x27;, &#x27;Ġtoken&#x27;, &#x27;nd&#x27;, &#x27;Ġis&#x27;, &#x27;Ġth&#x27;, &#x27;Ġthe&#x27;, &#x27;in&#x27;, &#x27;Ġab&#x27;, &#x27;Ġtokeni&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>运用分词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> pre_tokenize_result]</span><br><span class="line">    splits = [[l <span class="keyword">for</span> l <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="comment"># splits = [list(word) for word in pre_tokenized_text]</span></span><br><span class="line">    <span class="keyword">for</span> pair, merge <span class="keyword">in</span> merges.items():</span><br><span class="line">        <span class="keyword">for</span> idx, split <span class="keyword">in</span> <span class="built_in">enumerate</span>(splits):</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> split[i] == pair[<span class="number">0</span>] <span class="keyword">and</span> split[i + <span class="number">1</span>] == pair[<span class="number">1</span>]:</span><br><span class="line">                    split = split[:i] + [merge] + split[i + <span class="number">2</span> :]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            splits[idx] = split</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(splits, [])</span><br></pre></td></tr></table></figure><p>不怎么用，不理解也可</p><h1 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h1><p>它运用在 BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNE等模型</p><p>它的基本词库基于频率构建，但是merge是通过最大似然等算法构建</p><p><a href="https://huggingface.co/course/en/chapter6/6?fw=pt">这是HF团队对google算法的预测，因为google没公开他的分词算法</a></p><h1 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h1><p>它运用在AlBERT, T5, mBART, Big Bird, and XLNet.等模型</p><p>它直接构建一个超大的词汇表，然后计算删除这个单词造成的损失，每次删除造成最低损失的单词</p><p>构建基础的词汇表有使用预分词所有单词，按词频节选一部分然后计算损失，或者以BPE构建大词库然后做减法</p><p><a href="https://huggingface.co/course/en/chapter6/7?fw=pt">原链</a></p>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 07 NER QA Tokenizer</title>
      <link href="/posts/37742.html"/>
      <url>/posts/37742.html</url>
      
        <content type="html"><![CDATA[<h1 id="记得排版-分割线"><a href="#记得排版-分割线" class="headerlink" title="记得排版 分割线"></a>记得排版 分割线</h1><h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>QA部分</li></ul><blockquote><p>本章我们需要对做特殊的tokenizer以适应NER和QA任务数据的特殊性</p></blockquote><h1 id="Fast-Tokenizer"><a href="#Fast-Tokenizer" class="headerlink" title="Fast Tokenizer"></a>Fast Tokenizer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">example = <span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span></span><br><span class="line">encoding = tokenizer(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(encoding))</span><br><span class="line"><span class="comment"># &lt;class &#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>分词后返回的结果类型不简单是字典的映射</p><p>还包含很多方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.is_fast, encoding.is_fast</span><br><span class="line">(<span class="literal">True</span>,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">encoding.tokens()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">encoding.word_ids()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>word_ids()</code>方法可看到分词的结果来自哪个单词</p><p>最后我们可以使用<code>word_to_chars()</code> or <code>token_to_chars()</code> and <code>char_to_word()</code> or <code>char_to_token()</code> 查看单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start, end = encoding.word_to_chars(<span class="number">3</span>)</span><br><span class="line">example[start:end]</span><br><span class="line"><span class="comment"># Sylvain</span></span><br></pre></td></tr></table></figure><h1 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h1><blockquote><p>在NER中我们以偏移量的标记来锁定原文的字符</p></blockquote><h2 id="pipeline方法"><a href="#pipeline方法" class="headerlink" title="pipeline方法"></a>pipeline方法</h2><p>首先查看pipeline方法的ner流程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">token_classifier = pipeline(<span class="string">&quot;token-classification&quot;</span>)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 35&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;, &#x27;start&#x27;: 35, &#x27;end&#x27;: 40&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;, &#x27;start&#x27;: 41, &#x27;end&#x27;: 45&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>简洁版</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">token_classifier = pipeline(<span class="string">&quot;token-classification&quot;</span>, aggregation_strategy=<span class="string">&quot;simple&quot;</span>)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#123;&#x27;entity_group&#x27;: &#x27;PER&#x27;, &#x27;score&#x27;: 0.9981694, &#x27;word&#x27;: &#x27;Sylvain&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 18&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity_group&#x27;: &#x27;ORG&#x27;, &#x27;score&#x27;: 0.97960204, &#x27;word&#x27;: &#x27;Hugging Face&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 45&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity_group&#x27;: &#x27;LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>aggregation_strategy有不同的参数，simple是分词后的平均分数</p><p>如上面的sylvain分数来自 正常版的四项平均<code>&#39;S&#39;, &#39;##yl&#39;, &#39;##va&#39;, &#39;##in&#39;</code></p><ul><li><code>&quot;first&quot;</code>, where the score of each entity is the score of the first token of that entity (so for “Sylvain” it would be 0.993828, the score of the token <code>S</code>)</li><li><code>&quot;max&quot;</code>, where the score of each entity is the maximum score of the tokens in that entity (so for “Hugging Face” it would be 0.98879766, the score of “Face”)</li><li><code>&quot;average&quot;</code>, where the score of each entity is the average of the scores of the words composing that entity (so for “Sylvain” there would be no difference from the <code>&quot;simple&quot;</code> strategy, but “Hugging Face” would have a score of 0.9819, the average of the scores for “Hugging”, 0.975, and “Face”, 0.98879)</li></ul><h2 id="logits"><a href="#logits" class="headerlink" title="logits"></a>logits</h2><p>这里通过返回的结果使用<code>argmax(-1)</code>得到映射的分类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForTokenClassification</span><br><span class="line"></span><br><span class="line">model_checkpoint = <span class="string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br><span class="line">model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)</span><br><span class="line"></span><br><span class="line">example = <span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span></span><br><span class="line">inputs = tokenizer(example, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;input_ids&quot;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(outputs.logits.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([1, 19])</span></span><br><span class="line"><span class="string">torch.Size([1, 19, 9])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">probabilities = torch.nn.functional.softmax(outputs.logits, dim=-<span class="number">1</span>)[<span class="number">0</span>].tolist()</span><br><span class="line">predictions = outputs.logits.argmax(dim=-<span class="number">1</span>)[<span class="number">0</span>].tolist()</span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br><span class="line"><span class="comment"># [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]</span></span><br><span class="line"></span><br><span class="line">model.config.id2label</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;0: &#x27;O&#x27;,</span></span><br><span class="line"><span class="string"> 1: &#x27;B-MISC&#x27;,</span></span><br><span class="line"><span class="string"> 2: &#x27;I-MISC&#x27;,</span></span><br><span class="line"><span class="string"> 3: &#x27;B-PER&#x27;,</span></span><br><span class="line"><span class="string"> 4: &#x27;I-PER&#x27;,</span></span><br><span class="line"><span class="string"> 5: &#x27;B-ORG&#x27;,</span></span><br><span class="line"><span class="string"> 6: &#x27;I-ORG&#x27;,</span></span><br><span class="line"><span class="string"> 7: &#x27;B-LOC&#x27;,</span></span><br><span class="line"><span class="string"> 8: &#x27;I-LOC&#x27;&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="偏移量postprocessing"><a href="#偏移量postprocessing" class="headerlink" title="偏移量postprocessing"></a>偏移量postprocessing</h2><p>组织一下格式，复现上面的内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">tokens = inputs.tokens()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(predictions):</span><br><span class="line">    label = model.config.id2label[pred]</span><br><span class="line">    <span class="keyword">if</span> label != <span class="string">&quot;O&quot;</span>:</span><br><span class="line">        results.append(</span><br><span class="line">            &#123;<span class="string">&quot;entity&quot;</span>: label, <span class="string">&quot;score&quot;</span>: probabilities[idx][pred], <span class="string">&quot;word&quot;</span>: tokens[idx]&#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;&#125;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>偏移量 offsets_mapping</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),</span></span><br><span class="line"><span class="string"> (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>这里的19对元组就是对应19个分词后token的下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;My&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;##yl&#x27;</span>, <span class="string">&#x27;##va&#x27;</span>, <span class="string">&#x27;##in&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;work&#x27;</span>, <span class="string">&#x27;at&#x27;</span>, <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;##gging&#x27;</span>, <span class="string">&#x27;Face&#x27;</span>, <span class="string">&#x27;in&#x27;</span>,<span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br></pre></td></tr></table></figure><p>比如(0,0)是留给[CLS]的；比如第六个token对应的是 ##ly  那么他在原文中的标注就是（12,14），如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example[<span class="number">12</span>:<span class="number">14</span>]</span><br><span class="line"><span class="comment"># yl</span></span><br></pre></td></tr></table></figure><p>继续我们的复现pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">tokens = inputs_with_offsets.tokens()</span><br><span class="line">offsets = inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(predictions):</span><br><span class="line">    label = model.config.id2label[pred]</span><br><span class="line">    <span class="keyword">if</span> label != <span class="string">&quot;O&quot;</span>:</span><br><span class="line">        start, end = offsets[idx]</span><br><span class="line">        results.append(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;entity&quot;</span>: label,</span><br><span class="line">                <span class="string">&quot;score&quot;</span>: probabilities[idx][pred],</span><br><span class="line">                <span class="string">&quot;word&quot;</span>: tokens[idx],</span><br><span class="line">                <span class="string">&quot;start&quot;</span>: start,</span><br><span class="line">                <span class="string">&quot;end&quot;</span>: end,</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 35&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;, &#x27;start&#x27;: 35, &#x27;end&#x27;: 40&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;, &#x27;start&#x27;: 41, &#x27;end&#x27;: 45&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>最后对组织名处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">tokens = inputs_with_offsets.tokens()</span><br><span class="line">offsets = inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(predictions):</span><br><span class="line">    pred = predictions[idx]</span><br><span class="line">    label = model.config.id2label[pred]</span><br><span class="line">    <span class="keyword">if</span> label != <span class="string">&quot;O&quot;</span>:</span><br><span class="line">        <span class="comment"># Remove the B- or I-</span></span><br><span class="line">        label = label[<span class="number">2</span>:]</span><br><span class="line">        start, _ = offsets[idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Grab all the tokens labeled with I-label</span></span><br><span class="line">        all_scores = []</span><br><span class="line">        <span class="keyword">while</span> (</span><br><span class="line">            idx &lt; <span class="built_in">len</span>(predictions)</span><br><span class="line">            <span class="keyword">and</span> model.config.id2label[predictions[idx]] == <span class="string">f&quot;I-<span class="subst">&#123;label&#125;</span>&quot;</span></span><br><span class="line">        ):</span><br><span class="line">            all_scores.append(probabilities[idx][pred])</span><br><span class="line">            _, end = offsets[idx]</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># The score is the mean of all the scores of the tokens in that grouped entity</span></span><br><span class="line">        score = np.mean(all_scores).item()</span><br><span class="line">        word = example[start:end]</span><br><span class="line">        results.append(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;entity_group&quot;</span>: label,</span><br><span class="line">                <span class="string">&quot;score&quot;</span>: score,</span><br><span class="line">                <span class="string">&quot;word&quot;</span>: word,</span><br><span class="line">                <span class="string">&quot;start&quot;</span>: start,</span><br><span class="line">                <span class="string">&quot;end&quot;</span>: end,</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">    idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#123;&#x27;entity_group&#x27;: &#x27;PER&#x27;, &#x27;score&#x27;: 0.9981694, &#x27;word&#x27;: &#x27;Sylvain&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 18&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity_group&#x27;: &#x27;ORG&#x27;, &#x27;score&#x27;: 0.97960204, &#x27;word&#x27;: &#x27;Hugging Face&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 45&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity_group&#x27;: &#x27;LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h1><blockquote><p>QA中我们主要解决文本答案跨段落的问题</p></blockquote><h2 id="pipeline方法-1"><a href="#pipeline方法-1" class="headerlink" title="pipeline方法"></a>pipeline方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">question_answerer = pipeline(<span class="string">&quot;question-answering&quot;</span>)</span><br><span class="line">context = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration</span></span><br><span class="line"><span class="string">between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">question = <span class="string">&quot;Which deep learning libraries back 🤗 Transformers?&quot;</span></span><br><span class="line">question_answerer(question=question, context=context)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;score&#x27;: 0.97773,</span></span><br><span class="line"><span class="string"> &#x27;start&#x27;: 78,</span></span><br><span class="line"><span class="string"> &#x27;end&#x27;: 105,</span></span><br><span class="line"><span class="string"> &#x27;answer&#x27;: &#x27;Jax, PyTorch and TensorFlow&#x27;&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>这个pipeline可以处理很长的序列，基本不会截断你的文本，但是过长如果你的答案在截掉的部分，那也没辙了。</p><h2 id="logits-1"><a href="#logits-1" class="headerlink" title="logits"></a>logits</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForQuestionAnswering</span><br><span class="line"></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-cased-distilled-squad&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(question, context, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br></pre></td></tr></table></figure><p>QA任务中，我们会获得，两个logits，分别是开始的坐标和结束的坐标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_logits = outputs.start_logits</span><br><span class="line">end_logits = outputs.end_logits</span><br><span class="line"><span class="built_in">print</span>(start_logits.shape, end_logits.shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 66]) torch.Size([1, 66])</span></span><br></pre></td></tr></table></figure><p>由于我们的序列包含[CLS]、[SEP]*n，的特殊token所以需要屏蔽这些影响。根据softmax的特性选择填充一个很大的负数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">sequence_ids = inputs.sequence_ids()</span><br><span class="line"><span class="comment"># Mask everything apart from the tokens of the context</span></span><br><span class="line">mask = [i != <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> sequence_ids]</span><br><span class="line"><span class="comment"># Unmask the [CLS] token</span></span><br><span class="line">mask[<span class="number">0</span>] = <span class="literal">False</span></span><br><span class="line">mask = torch.tensor(mask)[<span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">start_logits[mask] = -<span class="number">10000</span></span><br><span class="line">end_logits[mask] = -<span class="number">10000</span></span><br></pre></td></tr></table></figure><blockquote><p>sequence_ids会将每个token映射到原本的句子，而特殊token则会标记为None</p><p>如[None, 0,0,0, None, 1,1,1, None] 这里标识的就是 [CLS]…[SEP]….[SEP]</p><p>这里的mask就是跟输出一样的形状他的值是T&#x2F;F, start_logits[mask]就会选择所有True的地方填充-10000</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>上面我们得到了两组向量，形状是 (batch_size, seq_len)  seq_len列表就表示每个坐标的概率</p><p>接下来就像做attention分数矩阵一样，将开始_结束位置的概率匹配上，<code>mask start&gt;end的组合，并取得概率最大的组合</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">start = torch.randn(<span class="number">2</span>,<span class="number">4</span>).softmax(dim=-<span class="number">1</span>)</span><br><span class="line">end = torch.randn(<span class="number">2</span>,<span class="number">4</span>).softmax(dim=-<span class="number">1</span>)</span><br><span class="line">start,end</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([[0.1310, 0.3604, 0.1726, 0.3360],</span></span><br><span class="line"><span class="string">         [0.1955, 0.3245, 0.0700, 0.4101]]),</span></span><br><span class="line"><span class="string"> tensor([[0.2199, 0.1130, 0.3976, 0.2695],</span></span><br><span class="line"><span class="string">         [0.0455, 0.0804, 0.0514, 0.8227]]))&#x27;&#x27;&#x27;</span></span><br><span class="line">         </span><br><span class="line">start.unsqueeze(<span class="number">1</span>).transpose(-<span class="number">1</span>,-<span class="number">2</span>) @ end.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[0.0288, 0.0148, 0.0521, 0.0353],</span></span><br><span class="line"><span class="string">         [0.0792, 0.0407, 0.1433, 0.0971],</span></span><br><span class="line"><span class="string">         [0.0380, 0.0195, 0.0686, 0.0465],</span></span><br><span class="line"><span class="string">         [0.0739, 0.0380, 0.1336, 0.0905]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.0089, 0.0157, 0.0100, 0.1608],</span></span><br><span class="line"><span class="string">         [0.0148, 0.0261, 0.0167, 0.2670],</span></span><br><span class="line"><span class="string">         [0.0032, 0.0056, 0.0036, 0.0576],</span></span><br><span class="line"><span class="string">         [0.0187, 0.0330, 0.0211, 0.3374]]])&#x27;&#x27;&#x27;</span></span><br><span class="line">         </span><br><span class="line">a[<span class="number">0</span>, <span class="number">0</span>]* b[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># tensor(0.0148)</span></span><br></pre></td></tr></table></figure><p>如上 start(2, 4, 1) @ end(2, 1, 4) &#x3D; (2, 4, 4) 单看(4, 4)的矩阵就是 横为end的坐标，纵为start的坐标 中间就是他们的概率积</p><p>接下里mask掉 不合理的位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fin = torch.triu(scores)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[0.0288, 0.0148, 0.0521, 0.0353],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0407, 0.1433, 0.0971],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0000, 0.0686, 0.0465],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0000, 0.0000, 0.0905]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.0089, 0.0157, 0.0100, 0.1608],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0261, 0.0167, 0.2670],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0000, 0.0036, 0.0576],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0000, 0.0000, 0.3374]]])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>这里官网刷了个把戏，代码只是对一个句子的坐标处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fin[<span class="number">0</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[[0.0288, 0.0148, 0.0521, 0.0353],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0407, 0.1433, 0.0971],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0000, 0.0686, 0.0465],</span></span><br><span class="line"><span class="string">         [0.0000, 0.0000, 0.0000, 0.0905]],&#x27;&#x27;&#x27;</span></span><br><span class="line">         </span><br><span class="line">         </span><br><span class="line">start_index = fin[<span class="number">0</span>].argmax() // <span class="number">4</span></span><br><span class="line">end_index = fin[<span class="number">0</span>].argmax() % <span class="number">4</span></span><br><span class="line">start_index, end_index, fin[<span class="number">0</span>].argmax()</span><br><span class="line"><span class="comment"># 1, 2, 6</span></span><br></pre></td></tr></table></figure><p>（1,2）就是0.1433 也就是开始坐标是1，结束是2，argmax返回的是绝对坐标</p><p>相对应的，做batch处理只要使用循环语句即可</p><h2 id="offset-map"><a href="#offset-map" class="headerlink" title="offset-map"></a>offset-map</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">offsets = inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">start_char, _ = offsets[start_index]</span><br><span class="line">_, end_char = offsets[end_index]</span><br><span class="line">answer = context[start_char:end_char]</span><br></pre></td></tr></table></figure><p>这里offsets是元组形式表示一个token结束和开始 如(3, 7)，表示这个token在原文的开始为3结束为7</p><p>取得start_token的最小值，再取得end_token的最大值，中间的区间就是，原文字符串。</p><p>另外offset是除去##等prefix的值，所以不会有影响。</p><h2 id="超长文处理"><a href="#超长文处理" class="headerlink" title="超长文处理"></a>超长文处理</h2><p>对于过长的文章，我们使用trucation截断，并使用overlap进行覆盖，防止答案截成两段没法做答</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span></span><br><span class="line">inputs = tokenizer(</span><br><span class="line">    sentence, truncation=<span class="literal">True</span>, return_overflowing_tokens=<span class="literal">True</span>, max_length=<span class="number">6</span>, stride=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ids <span class="keyword">in</span> inputs[<span class="string">&quot;input_ids&quot;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.decode(ids))</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] This sentence is not [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] is not too long [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] too long but we [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] but we are going [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] are going to split [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] to split it anyway [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] it anyway. [SEP]&#x27;&#x27;&#x27;</span><span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><p>如上，我们设定overla的窗口为2 这使得每次截断都有两个词是重叠的，可以看到特殊token并不计算在窗口内</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(inputs.keys())</span><br><span class="line"><span class="comment"># dict_keys([&#x27;input_ids&#x27;, &#x27;attention_mask&#x27;, &#x27;overflow_to_sample_mapping&#x27;])</span></span><br><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;overflow_to_sample_mapping&quot;</span>])</span><br><span class="line"><span class="comment">#[0, 0, 0, 0, 0, 0, 0]</span></span><br></pre></td></tr></table></figure><p><code>overflow_to_sample_mapping</code>标记我们截断后的句子来自原文中的哪个句子，具体如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    <span class="string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This sentence is shorter but will still get split.&quot;</span>,</span><br><span class="line">]</span><br><span class="line">inputs = tokenizer(</span><br><span class="line">    sentences, truncation=<span class="literal">True</span>, return_overflowing_tokens=<span class="literal">True</span>, max_length=<span class="number">6</span>, stride=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;overflow_to_sample_mapping&quot;</span>])</span><br><span class="line"><span class="comment"># [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]</span></span><br></pre></td></tr></table></figure><p>很明显了。</p><p>接下来回到我们长文处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(</span><br><span class="line">    question,</span><br><span class="line">    long_context,</span><br><span class="line">    stride=<span class="number">128</span>,</span><br><span class="line">    max_length=<span class="number">384</span>,</span><br><span class="line">    padding=<span class="string">&quot;longest&quot;</span>,</span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>QA tokenizer一般第一个位置为question，设定<code>truncation=&quot;only_second&quot;</code>表示只对<code>long_context</code>进行截断</p><p>下面我们处理tokenizer的输出，将不必要的部分pop出去</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">_ = inputs.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line">offsets = inputs.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line"></span><br><span class="line">inputs = inputs.convert_to_tensors(<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;input_ids&quot;</span>].shape)</span><br><span class="line"><span class="comment"># torch.Size([2, 384])</span></span><br></pre></td></tr></table></figure><p>这里测试下输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(**inputs)</span><br><span class="line"></span><br><span class="line">start_logits = outputs.start_logits</span><br><span class="line">end_logits = outputs.end_logits</span><br><span class="line"><span class="built_in">print</span>(start_logits.shape, end_logits.shape)</span><br><span class="line"><span class="comment"># torch.Size([2, 384]) torch.Size([2, 384])</span></span><br></pre></td></tr></table></figure><p>处理坐标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sequence_ids = inputs.sequence_ids()</span><br><span class="line">mask = [i != <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> sequence_ids]</span><br><span class="line">mask[<span class="number">0</span>] = <span class="literal">False</span></span><br><span class="line">mask = torch.logical_or(torch.tensor(mask)[<span class="literal">None</span>], (inputs[<span class="string">&quot;attention_mask&quot;</span>] == <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">start_logits[mask] = -<span class="number">10000</span></span><br><span class="line">end_logits[mask] = -<span class="number">10000</span></span><br><span class="line"></span><br><span class="line">start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="number">1</span>)</span><br><span class="line">end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><code>torch.logical_or</code>函数做的是或门的处理，只要有非零项就为True，输入的两个矩阵对应位置都为0才输出False</p><p>下面循环处理取得下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">candidates = []</span><br><span class="line"><span class="keyword">for</span> start_probs, end_probs <span class="keyword">in</span> <span class="built_in">zip</span>(start_probabilities, end_probabilities):</span><br><span class="line">    scores = start_probs[:, <span class="literal">None</span>] * end_probs[<span class="literal">None</span>, :]</span><br><span class="line">    idx = torch.triu(scores).argmax().item()</span><br><span class="line"></span><br><span class="line">    start_idx = idx // scores.shape[<span class="number">1</span>]</span><br><span class="line">    end_idx = idx % scores.shape[<span class="number">1</span>]</span><br><span class="line">    score = scores[start_idx, end_idx].item()</span><br><span class="line">    candidates.append((start_idx, end_idx, score))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(candidates)</span><br><span class="line"><span class="comment"># [(0, 18, 0.33867), (173, 184, 0.97149)]</span></span><br></pre></td></tr></table></figure><p>通过之前保存的offset映射到原文</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> candidate, offset <span class="keyword">in</span> <span class="built_in">zip</span>(candidates, offsets):</span><br><span class="line">    start_token, end_token, score = candidate</span><br><span class="line">    start_char, _ = offset[start_token]</span><br><span class="line">    _, end_char = offset[end_token]</span><br><span class="line">    answer = long_context[start_char:end_char]</span><br><span class="line">    result = &#123;<span class="string">&quot;answer&quot;</span>: answer, <span class="string">&quot;start&quot;</span>: start_char, <span class="string">&quot;end&quot;</span>: end_char, <span class="string">&quot;score&quot;</span>: score&#125;</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;answer&#x27;: &#x27;\n🤗 Transformers: State of the Art NLP&#x27;, &#x27;start&#x27;: 0, &#x27;end&#x27;: 37, &#x27;score&#x27;: 0.33867&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;answer&#x27;: &#x27;Jax, PyTorch and TensorFlow&#x27;, &#x27;start&#x27;: 1892, &#x27;end&#x27;: 1919, &#x27;score&#x27;: 0.97149&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>以上就是QA的一般处理，但是对一般的数据集来说，他的答案下标跟你truncation之后的是有偏差的，所以对原文答案的map要另做处理。</p>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 06 继承式的Tokenizer</title>
      <link href="/posts/55913.html"/>
      <url>/posts/55913.html</url>
      
        <content type="html"><![CDATA[<p>这种方法是基于旧的模型分词器上，针对你的语料库训练一个新的分词器的方法。<del>夺舍属于是</del></p><p>这里我们以GPT的分词器为例，它使用unigram的算法进行分词</p><h1 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># This can take a few minutes to load, so grab a coffee or tea while you wait!</span></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;code_search_net&quot;</span>, <span class="string">&quot;python&quot;</span>)</span><br><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Dataset(&#123;</span></span><br><span class="line"><span class="string">    features: [&#x27;repository_name&#x27;, &#x27;func_path_in_repository&#x27;, &#x27;func_name&#x27;, &#x27;whole_func_string&#x27;, &#x27;language&#x27;, </span></span><br><span class="line"><span class="string">      &#x27;func_code_string&#x27;, &#x27;func_code_tokens&#x27;, &#x27;func_documentation_string&#x27;, &#x27;func_documentation_tokens&#x27;, &#x27;split_name&#x27;, </span></span><br><span class="line"><span class="string">      &#x27;func_code_url&#x27;</span></span><br><span class="line"><span class="string">    ],</span></span><br><span class="line"><span class="string">    num_rows: 412178</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="生成器加载数据"><a href="#生成器加载数据" class="headerlink" title="生成器加载数据"></a>生成器加载数据</h2><p>下面的方法会一次加载所有数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Don&#x27;t uncomment the following line unless your dataset is small!</span></span><br><span class="line"><span class="comment"># training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]</span></span><br></pre></td></tr></table></figure><p>一般使用python生成器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">training_corpus = (</span><br><span class="line">    raw_datasets[<span class="string">&quot;train&quot;</span>][i : i + <span class="number">1000</span>][<span class="string">&quot;whole_func_string&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(raw_datasets[<span class="string">&quot;train&quot;</span>]), <span class="number">1000</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>将列表推导式的方括号换成圆括号就可以变成生成器了，好厉害。</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;gen = (i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">&gt;<span class="built_in">print</span>(<span class="built_in">list</span>(gen))</span><br><span class="line">&gt;<span class="built_in">print</span>(<span class="built_in">list</span>(gen))</span><br><span class="line">&gt;<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&gt;[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="string">&gt;[]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>使用之后会清空内存，如上所示</p></blockquote><p>更一般的生成器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), <span class="number">1000</span>):</span><br><span class="line">        samples = dataset[start_idx : start_idx + <span class="number">1000</span>]</span><br><span class="line">        <span class="keyword">yield</span> samples[<span class="string">&quot;whole_func_string&quot;</span>]</span><br></pre></td></tr></table></figure><h1 id="train-new-from-iterator"><a href="#train-new-from-iterator" class="headerlink" title="train_new_from_iterator()"></a>train_new_from_iterator()</h1><h2 id="载入模型"><a href="#载入模型" class="headerlink" title="载入模型"></a>载入模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">old_tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line">example = <span class="string">&#x27;&#x27;&#x27;def add_numbers(a, b):</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;Add the two numbers `a` and `b`.&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    return a + b&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">tokens = old_tokenizer.tokenize(example)</span><br><span class="line">tokens</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;def&#x27;, &#x27;Ġadd&#x27;, &#x27;_&#x27;, &#x27;n&#x27;, &#x27;umbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ġb&#x27;, &#x27;):&#x27;, &#x27;Ċ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ġthe&#x27;, &#x27;Ġtwo&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;Ġnumbers&#x27;, &#x27;Ġ`&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ġand&#x27;, &#x27;Ġ`&#x27;, &#x27;b&#x27;, &#x27;`&#x27;, &#x27;.&quot;&#x27;, &#x27;&quot;&quot;&#x27;, &#x27;Ċ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġreturn&#x27;, &#x27;Ġa&#x27;, &#x27;Ġ+&#x27;, &#x27;Ġb&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>This tokenizer has a few special symbols, like <code>Ġ</code> and <code>Ċ</code>, which denote spaces and newlines, respectively。</p><p>两个G表示空格和换行符。他还为多个空格在一起的单独编码，带下划线的词也不认识，所以不太合适。</p><h2 id="训练新分词器"><a href="#训练新分词器" class="headerlink" title="训练新分词器"></a>训练新分词器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, <span class="number">52000</span>)</span><br></pre></td></tr></table></figure><blockquote><p>注意，只有Fast的tokenizer支持<code>train_new_from_iterator</code>方法，他们是根据rust写的。没有fast的是纯python写的。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenizer.tokenize(example)</span><br><span class="line">tokens</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;def&#x27;, &#x27;Ġadd&#x27;, &#x27;_&#x27;, &#x27;numbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ġb&#x27;, &#x27;):&#x27;, &#x27;ĊĠĠĠ&#x27;, &#x27;Ġ&quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ġthe&#x27;, &#x27;Ġtwo&#x27;, &#x27;Ġnumbers&#x27;, &#x27;Ġ`&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ġand&#x27;, &#x27;Ġ`&#x27;, &#x27;b&#x27;, &#x27;`.&quot;&quot;&quot;&#x27;, &#x27;ĊĠĠĠ&#x27;, &#x27;Ġreturn&#x27;, &#x27;Ġa&#x27;, &#x27;Ġ+&#x27;, &#x27;Ġb&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>起码多个空格学会了</p><h2 id="存储新分词器"><a href="#存储新分词器" class="headerlink" title="存储新分词器"></a>存储新分词器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.save_pretrained(<span class="string">&quot;code-search-net-tokenizer&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 05 faiss 搜索引擎</title>
      <link href="/posts/5596.html"/>
      <url>/posts/5596.html</url>
      
        <content type="html"><![CDATA[<p>在我们创建好自己的数据集后，可以用faiss 和 hf 来搜索一些数据。</p><p>我们通过<code>multi-qa-mpnet-base-dot-v1</code>模型embedding我们的数据，然后通过 faiss给每个embedding得到index</p><p>最后将我们的query 给tokenizer转换之后喂给模型，得到最匹配我们问题的数据。</p><blockquote><p>Fortunately, there’s a library called <code>sentence-transformers</code> that is dedicated to creating embeddings. As described in the library’s <a href="https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search">documentation</a>, our use case is an example of <em>asymmetric semantic search</em> because we have a short query whose answer we’d like to find in a longer document, like a an issue comment. The handy <a href="https://www.sbert.net/docs/pretrained_models.html#model-overview">model overview table</a> in the documentation indicates that the <code>multi-qa-mpnet-base-dot-v1</code> checkpoint has the best performance for semantic search, so we’ll use that for our application.</p><p>我们主要使用了<code>sentence-transformers</code> <code>faiss</code>两个额外库处理</p></blockquote><h1 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">model_ckpt = <span class="string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt)</span><br><span class="line">model = AutoModel.from_pretrained(model_ckpt)</span><br></pre></td></tr></table></figure><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cls_pooling</span>(<span class="params">model_output</span>):</span><br><span class="line">    <span class="keyword">return</span> model_output.last_hidden_state[:, <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings</span>(<span class="params">text_list</span>):</span><br><span class="line">    encoded_input = tokenizer(</span><br><span class="line">        text_list, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">    )</span><br><span class="line">    encoded_input = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> encoded_input.items()&#125;</span><br><span class="line">    model_output = model(**encoded_input)</span><br><span class="line">    <span class="keyword">return</span> cls_pooling(model_output)</span><br></pre></td></tr></table></figure><h1 id="加入-faiss-的index"><a href="#加入-faiss-的index" class="headerlink" title="加入 faiss 的index"></a>加入 faiss 的index</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">embeddings_dataset = comments_dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="string">&quot;text&quot;</span>]).detach().cpu().numpy()[<span class="number">0</span>]&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embeddings_dataset.add_faiss_index(column=<span class="string">&quot;embeddings&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;How can I load a dataset offline?&quot;</span></span><br><span class="line">question_embedding = get_embeddings([question]).cpu().detach().numpy()</span><br><span class="line">question_embedding.shape</span><br><span class="line"><span class="comment"># torch.Size([1, 768])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores, samples = embeddings_dataset.get_nearest_examples(</span><br><span class="line">    <span class="string">&quot;embeddings&quot;</span>, question_embedding, k=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">samples_df = pd.DataFrame.from_dict(samples)</span><br><span class="line">samples_df[<span class="string">&quot;scores&quot;</span>] = scores</span><br><span class="line">samples_df.sort_values(<span class="string">&quot;scores&quot;</span>, ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, row <span class="keyword">in</span> samples_df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;COMMENT: <span class="subst">&#123;row.comments&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;SCORE: <span class="subst">&#123;row.scores&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;TITLE: <span class="subst">&#123;row.title&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;URL: <span class="subst">&#123;row.html_url&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><p>可以查看最匹配的评论</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how `transformers` loads models offline fine.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?</span></span><br><span class="line"><span class="string">SCORE: 25.505046844482422</span></span><br><span class="line"><span class="string">TITLE: Discussion using datasets in offline mode</span></span><br><span class="line"><span class="string">URL: https://github.com/huggingface/datasets/issues/824</span></span><br><span class="line"><span class="string">==================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)</span></span><br><span class="line"><span class="string">You can now use them offline</span></span><br><span class="line"><span class="string">\`\`\`python</span></span><br><span class="line"><span class="string">datasets = load_dataset(&quot;text&quot;, data_files=data_files)</span></span><br><span class="line"><span class="string">\`\`\`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We&#x27;ll do a new release soon</span></span><br><span class="line"><span class="string">SCORE: 24.555509567260742</span></span><br><span class="line"><span class="string">TITLE: Discussion using datasets in offline mode</span></span><br><span class="line"><span class="string">URL: https://github.com/huggingface/datasets/issues/824</span></span><br><span class="line"><span class="string">==================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.</span></span><br><span class="line"><span class="string">For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do</span></span><br><span class="line"><span class="string">\`\`\`python</span></span><br><span class="line"><span class="string">load_dataset(&quot;./my_dataset&quot;)</span></span><br><span class="line"><span class="string">\`\`\`</span></span><br><span class="line"><span class="string">and the dataset script will generate your dataset once and for all.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">About I&#x27;m looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.</span></span><br><span class="line"><span class="string">cf #1724</span></span><br><span class="line"><span class="string">SCORE: 24.14896583557129</span></span><br><span class="line"><span class="string">TITLE: Discussion using datasets in offline mode</span></span><br><span class="line"><span class="string">URL: https://github.com/huggingface/datasets/issues/824</span></span><br><span class="line"><span class="string">==================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine</span></span><br><span class="line"><span class="string">&gt;</span></span><br><span class="line"><span class="string">&gt; 1. (online machine)</span></span><br><span class="line"><span class="string">&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>import datasets</p><p>data &#x3D; datasets.load_dataset(…)</p><p>data.save_to_disk(&#x2F;YOUR&#x2F;DATASET&#x2F;DIR)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. copy the dir from online to the offline machine</span><br><span class="line"></span><br><span class="line">3. (offline machine)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>import datasets</p><p>data &#x3D; datasets.load_from_disk(&#x2F;SAVED&#x2F;DATA&#x2F;DIR)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">HTH.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SCORE: 22.893993377685547</span><br><span class="line">TITLE: Discussion using datasets in offline mode</span><br><span class="line">URL: https://github.com/huggingface/datasets/issues/824</span><br><span class="line">==================================================</span><br><span class="line"></span><br><span class="line">COMMENT: here is my way to load a dataset offline, but it **requires** an online machine</span><br><span class="line">1. (online machine)</span><br><span class="line">\`\`\`</span><br><span class="line">import datasets</span><br><span class="line">data = datasets.load_dataset(...)</span><br><span class="line">data.save_to_disk(/YOUR/DATASET/DIR)</span><br><span class="line">\`\`\`</span><br><span class="line">2. copy the dir from online to the offline machine</span><br><span class="line">3. (offline machine)</span><br><span class="line">\`\`\`</span><br><span class="line">import datasets</span><br><span class="line">data = datasets.load_from_disk(/SAVED/DATA/DIR)</span><br><span class="line">\`\`\`</span><br><span class="line"></span><br><span class="line">HTH.</span><br><span class="line">SCORE: 22.406635284423828</span><br><span class="line">TITLE: Discussion using datasets in offline mode</span><br><span class="line">URL: https://github.com/huggingface/datasets/issues/824</span><br><span class="line">==================================================</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></blockquote>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 04 Dataset</title>
      <link href="/posts/4498.html"/>
      <url>/posts/4498.html</url>
      
        <content type="html"><![CDATA[<h1 id="加载本地数据"><a href="#加载本地数据" class="headerlink" title="加载本地数据"></a>加载本地数据</h1><table><thead><tr><th>Data format</th><th>Loading script</th><th>Example</th></tr></thead><tbody><tr><td>CSV &amp; TSV</td><td><code>csv</code></td><td><code>load_dataset(&quot;csv&quot;, data_files=&quot;my_file.csv&quot;)</code></td></tr><tr><td>Text files</td><td><code>text</code></td><td><code>load_dataset(&quot;text&quot;, data_files=&quot;my_file.txt&quot;)</code></td></tr><tr><td>JSON &amp; JSON Lines</td><td><code>json</code></td><td><code>load_dataset(&quot;json&quot;, data_files=&quot;my_file.jsonl&quot;)</code></td></tr><tr><td>Pickled DataFrames</td><td><code>pandas</code></td><td><code>load_dataset(&quot;pandas&quot;, data_files=&quot;my_dataframe.pkl&quot;)</code></td></tr></tbody></table><p>分别需要做，指明数据类型，指明文件路径</p><h2 id="data-files参数"><a href="#data-files参数" class="headerlink" title="data_files参数"></a>data_files参数</h2><p>The <code>data_files</code> argument of the <code>load_dataset()</code> function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting <code>data_files=&quot;*.json&quot;</code>). See the 🤗 Datasets <a href="https://huggingface.co/docs/datasets/loading.html#local-and-remote-files">documentation</a> for more details.</p><ul><li><p>可以做文件路径</p></li><li><p>可以做split将数据映射成想要的字典格式</p><ul><li><p>&#96;&#96;&#96;python<br>data_files &#x3D; {“train”: “SQuAD_it-train.json”, “test”: “SQuAD_it-test.json”}<br>squad_it_dataset &#x3D; load_dataset(“json”, data_files&#x3D;data_files, field&#x3D;”data”)<br>squad_it_dataset</p><p>‘’’<br>DatasetDict({<br>train: Dataset({<br>    features: [‘title’, ‘paragraphs’],<br>    num_rows: 442<br>})<br>test: Dataset({<br>    features: [‘title’, ‘paragraphs’],<br>    num_rows: 48<br>})<br>})’’’</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">## 加载服务器数据</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">url = &quot;https://github.com/crux82/squad-it/raw/master/&quot;</span><br><span class="line">data_files = &#123;</span><br><span class="line">    &quot;train&quot;: url + &quot;SQuAD_it-train.json.gz&quot;,</span><br><span class="line">    &quot;test&quot;: url + &quot;SQuAD_it-test.json.gz&quot;,</span><br><span class="line">&#125;</span><br><span class="line">squad_it_dataset = load_dataset(&quot;json&quot;, data_files=data_files, field=&quot;data&quot;)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><h2 id="分隔符"><a href="#分隔符" class="headerlink" title="分隔符"></a>分隔符</h2><p>如果你的数据不是传统的CSV格式(以逗号分割)，你可以指定分隔符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">data_files = &#123;<span class="string">&quot;train&quot;</span>: <span class="string">&quot;drugsComTrain_raw.tsv&quot;</span>, <span class="string">&quot;test&quot;</span>: <span class="string">&quot;drugsComTest_raw.tsv&quot;</span>&#125;</span><br><span class="line"><span class="comment"># \t is the tab character in Python</span></span><br><span class="line">drug_dataset = load_dataset(<span class="string">&quot;csv&quot;</span>, data_files=data_files, delimiter=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="随机选取样本"><a href="#随机选取样本" class="headerlink" title="随机选取样本"></a>随机选取样本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">drug_sample = drug_dataset[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line"><span class="comment"># Peek at the first few examples</span></span><br><span class="line">drug_sample[:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;Unnamed: 0&#x27;: [87571, 178045, 80482],</span></span><br><span class="line"><span class="string"> &#x27;drugName&#x27;: [&#x27;Naproxen&#x27;, &#x27;Duloxetine&#x27;, &#x27;Mobic&#x27;],</span></span><br><span class="line"><span class="string"> &#x27;condition&#x27;: [&#x27;Gout, Acute&#x27;, &#x27;ibromyalgia&#x27;, &#x27;Inflammatory Conditions&#x27;],</span></span><br><span class="line"><span class="string"> &#x27;review&#x27;: [&#x27;&quot;like the previous person mention, I&amp;#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!&quot;&#x27;,</span></span><br><span class="line"><span class="string">  &#x27;&quot;I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.&quot;&#x27;,</span></span><br><span class="line"><span class="string">  &#x27;&quot;I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.&quot;&#x27;],</span></span><br><span class="line"><span class="string"> &#x27;rating&#x27;: [9.0, 3.0, 10.0],</span></span><br><span class="line"><span class="string"> &#x27;date&#x27;: [&#x27;September 2, 2015&#x27;, &#x27;November 7, 2011&#x27;, &#x27;June 5, 2013&#x27;],</span></span><br><span class="line"><span class="string"> &#x27;usefulCount&#x27;: [36, 13, 128]&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="重命名"><a href="#重命名" class="headerlink" title="重命名"></a>重命名</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.rename_column(</span><br><span class="line">    original_column_name=<span class="string">&quot;Unnamed: 0&quot;</span>, new_column_name=<span class="string">&quot;patient_id&quot;</span></span><br><span class="line">)</span><br><span class="line">drug_dataset</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 161297</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    test: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 53766</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>补充一个匿名表达式的细节</p><blockquote><p><code>(lambda base, height: 0.5 * base * height)(4, 8)</code></p><p>16 </p></blockquote><h2 id="转换大小写"><a href="#转换大小写" class="headerlink" title="转换大小写"></a>转换大小写</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lowercase_condition</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;condition&quot;</span>: example[<span class="string">&quot;condition&quot;</span>].lower()&#125;</span><br><span class="line"></span><br><span class="line">drug_dataset.<span class="built_in">map</span>(lowercase_condition)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">AttributeError: &#x27;NoneType&#x27; object has no attribute &#x27;lower&#x27;&#x27;&#x27;</span><span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><p>这里报错了</p><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p><code>dataset.filter</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x[<span class="string">&quot;condition&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">drug_dataset = drug_dataset.<span class="built_in">map</span>(lowercase_condition)</span><br><span class="line"><span class="comment"># Check that lowercasing worked</span></span><br><span class="line">drug_dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;condition&quot;</span>][:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;left ventricular dysfunction&#x27;, &#x27;adhd&#x27;, &#x27;birth control&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>过滤筛选合格的数据样本</p><h2 id="增加列"><a href="#增加列" class="headerlink" title="增加列"></a>增加列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_review_length</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;review_length&quot;</span>: <span class="built_in">len</span>(example[<span class="string">&quot;review&quot;</span>].split())&#125;</span><br><span class="line">    </span><br><span class="line">drug_dataset = drug_dataset.<span class="built_in">map</span>(compute_review_length)</span><br><span class="line"><span class="comment"># Inspect the first training example</span></span><br><span class="line">drug_dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;patient_id&#x27;: 206461,</span></span><br><span class="line"><span class="string"> &#x27;drugName&#x27;: &#x27;Valsartan&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;condition&#x27;: &#x27;left ventricular dysfunction&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;review&#x27;: &#x27;&quot;It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil&quot;&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;rating&#x27;: 9.0,</span></span><br><span class="line"><span class="string"> &#x27;date&#x27;: &#x27;May 20, 2012&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;usefulCount&#x27;: 27,</span></span><br><span class="line"><span class="string"> &#x27;review_length&#x27;: 17&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>补充一个sort</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;drug_dataset[<span class="string">&quot;train&quot;</span>].sort(<span class="string">&quot;review_length&quot;</span>)[:<span class="number">3</span>]</span><br><span class="line">&gt;<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&gt;&#123;&#x27;patient_id&#x27;: [103488, 23627, 20558],</span></span><br><span class="line"><span class="string">&#x27;drugName&#x27;: [&#x27;Loestrin 21 1 / 20&#x27;, &#x27;Chlorzoxazone&#x27;, &#x27;Nucynta&#x27;],</span></span><br><span class="line"><span class="string">&#x27;condition&#x27;: [&#x27;birth control&#x27;, &#x27;muscle spasm&#x27;, &#x27;pain&#x27;],</span></span><br><span class="line"><span class="string">&#x27;review&#x27;: [&#x27;&quot;Excellent.&quot;&#x27;, &#x27;&quot;useless&quot;&#x27;, &#x27;&quot;ok&quot;&#x27;],</span></span><br><span class="line"><span class="string">&#x27;rating&#x27;: [10.0, 1.0, 6.0],</span></span><br><span class="line"><span class="string">&#x27;date&#x27;: [&#x27;November 4, 2008&#x27;, &#x27;March 24, 2017&#x27;, &#x27;August 20, 2016&#x27;],</span></span><br><span class="line"><span class="string">&#x27;usefulCount&#x27;: [5, 2, 10],</span></span><br><span class="line"><span class="string">&#x27;review_length&#x27;: [1, 1, 1]&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>sort应该也有reverse选项，如果真要做EDA还是用Pandas好了, <a href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.sort">查看可配置参数</a></p></blockquote><p>在补充一个<code>Dataset.add_column()</code></p><blockquote><p>An alternative way to add new columns to a dataset is with the <code>Dataset.add_column()</code> function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where <code>Dataset.map()</code> is not well suited for your analysis.</p></blockquote><h2 id="解析html字符"><a href="#解析html字符" class="headerlink" title="解析html字符"></a>解析html字符</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> html</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;I&amp;#039;m a transformer called BERT&quot;</span></span><br><span class="line">html.unescape(text)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&quot;I&#x27;m a transformer called BERT&quot;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: &#123;<span class="string">&quot;review&quot;</span>: html.unescape(x[<span class="string">&quot;review&quot;</span>])&#125;)</span><br></pre></td></tr></table></figure><h1 id="map-方法"><a href="#map-方法" class="headerlink" title="map 方法"></a>map 方法</h1><h2 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h2><p>When you specify <code>batched=True</code> the function receives a dictionary with the fields of the dataset, but each value is now a <em>list of values</em>, and not just a single value. The return value of <code>Dataset.map()</code> should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_drug_dataset = drug_dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;review&quot;</span>: [html.unescape(o) <span class="keyword">for</span> o <span class="keyword">in</span> x[<span class="string">&quot;review&quot;</span>]]&#125;, batched=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>批量处理为True的话，每次传进来就是一个字典批次。一般我们做的就是更新这个数据集</p><p>If you’re running this code in a notebook, you’ll see that this command executes way faster than the previous one. And it’s not because our reviews have already been HTML-unescaped — if you re-execute the instruction from the previous section (without <code>batched=True</code>), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a <code>for</code> loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.</p><p>之前单个处理的用的是for循环，这里批量处理就可以用列表推导式，要快的多</p><h2 id="配合tokenizer使用"><a href="#配合tokenizer使用" class="headerlink" title="配合tokenizer使用"></a>配合tokenizer使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_and_split</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;review&quot;</span>],</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">        max_length=<span class="number">128</span>,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">result = tokenize_and_split(drug_dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>])</span><br><span class="line">[<span class="built_in">len</span>(inp) <span class="keyword">for</span> inp <span class="keyword">in</span> result[<span class="string">&quot;input_ids&quot;</span>]]</span><br><span class="line"><span class="comment"># [128, 49]</span></span><br></pre></td></tr></table></figure><p>使用<code>return_overflowing_tokens</code>参数来接受截断的部分，这里我们177的长度变成了128和49两份</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenized_dataset = drug_dataset.<span class="built_in">map</span>(tokenize_and_split, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h1><h2 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h2><p>To enable the conversion between various third-party libraries, 🤗 Datasets provides a <code>Dataset.set_format()</code> function. This function only changes the <em>output format</em> of the dataset, so you can easily switch to another format without affecting the underlying <em>data format</em>, which is Apache Arrow. The formatting is done in place. To demonstrate, let’s convert our dataset to Pandas:</p><p><code>drug_dataset.set_format(&quot;pandas&quot;)</code></p><p>一般使用<code>train_df = drug_dataset[&quot;train&quot;][:]</code> 获得整体的切片作为新的Dataframe 可以自己尝试是否返回对象为hf的dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">freq_dataset = Dataset.from_pandas(frequencies)</span><br><span class="line">freq_dataset</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Dataset(&#123;</span></span><br><span class="line"><span class="string">    features: [&#x27;condition&#x27;, &#x27;frequency&#x27;],</span></span><br><span class="line"><span class="string">    num_rows: 819</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>可以转换回来</p><h1 id="train-test-split"><a href="#train-test-split" class="headerlink" title="train_test_split"></a>train_test_split</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset_clean = drug_dataset[<span class="string">&quot;train&quot;</span>].train_test_split(train_size=<span class="number">0.8</span>, seed=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Rename the default &quot;test&quot; split to &quot;validation&quot;</span></span><br><span class="line">drug_dataset_clean[<span class="string">&quot;validation&quot;</span>] = drug_dataset_clean.pop(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"><span class="comment"># Add the &quot;test&quot; set to our `DatasetDict`</span></span><br><span class="line">drug_dataset_clean[<span class="string">&quot;test&quot;</span>] = drug_dataset[<span class="string">&quot;test&quot;</span>]</span><br><span class="line">drug_dataset_clean</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;, &#x27;review_length&#x27;, &#x27;review_clean&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 110811</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    validation: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;, &#x27;review_length&#x27;, &#x27;review_clean&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 27703</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    test: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;, &#x27;review_length&#x27;, &#x27;review_clean&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 46108</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="保存文件"><a href="#保存文件" class="headerlink" title="保存文件"></a>保存文件</h1><table><thead><tr><th>Data format</th><th>Function</th></tr></thead><tbody><tr><td>Arrow</td><td><code>Dataset.save_to_disk()</code></td></tr><tr><td>CSV</td><td><code>Dataset.to_csv()</code></td></tr><tr><td>JSON</td><td><code>Dataset.to_json()</code></td></tr></tbody></table><h1 id="对超大数据的处理"><a href="#对超大数据的处理" class="headerlink" title="对超大数据的处理"></a>对超大数据的处理</h1><h2 id="psutil-查看内存"><a href="#psutil-查看内存" class="headerlink" title="psutil 查看内存"></a>psutil 查看内存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">!pip install psutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> psutil</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process.memory_info is expressed in bytes, so convert to megabytes</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;RAM used: <span class="subst">&#123;psutil.Process().memory_info().rss / (<span class="number">1024</span> * <span class="number">1024</span>):<span class="number">.2</span>f&#125;</span> MB&quot;</span>)</span><br></pre></td></tr></table></figure><p>If you’re familiar with Pandas, this result might come as a surprise because of Wes Kinney’s famous <a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">rule of thumb</a> that you typically <code>need 5 to 10 times as much RAM as the size of your dataset</code>. So how does 🤗 Datasets solve this memory management problem? 🤗 Datasets treats each dataset as a <a href="https://en.wikipedia.org/wiki/Memory-mapped_file">memory-mapped file</a>, which provides a mapping between RAM and filesystem storage that allows the library to access and operate on elements of the dataset without needing to fully load it into memory.</p><p>Memory-mapped files can also be shared across multiple processes, which enables methods like <code>Dataset.map()</code> to be parallelized without needing to move or copy the dataset. Under the hood, these capabilities are all realized by the <a href="https://arrow.apache.org/">Apache Arrow</a> memory format and <a href="https://arrow.apache.org/docs/python/index.html"><code>pyarrow</code></a> library, which make the data loading and processing lightning fast.</p><p>通常你需要五到十倍于你文件大小的内存，而dataset的内存管理使得你可以加载巨大的数据集的一部分，这是通过pyarrow实现的，并且你的map方法可并行处理数据。</p><h2 id="stream-data"><a href="#stream-data" class="headerlink" title="stream data"></a>stream data</h2><p> if we tried to download the Pile in its entirety, we’d need<code>825 GB</code>of free disk space! To handle these cases, 🤗 Datasets provides a streaming feature that allows us to download and access elements on the fly, without needing to download the whole dataset. Let’s take a look at how this works.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pubmed_dataset_streamed = load_dataset(</span><br><span class="line">    <span class="string">&quot;json&quot;</span>, data_files=data_files, split=<span class="string">&quot;train&quot;</span>, streaming=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>返回的是一个<code>IterableDataset</code>对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-uncased&quot;</span>)</span><br><span class="line">tokenized_dataset = pubmed_dataset_streamed.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: tokenizer(x[<span class="string">&quot;text&quot;</span>]))</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(tokenized_dataset))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;input_ids&#x27;: [101, 4958, 5178, 4328, 6779, ...], &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, ...]&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>💡 To speed up tokenization with streaming you can pass <code>batched=True</code>, as we saw in the last section. It will process the examples batch by batch; the default batch size is 1,000 and can be specified with the <code>batch_size</code> argument.</p></blockquote><h2 id="合并数据集"><a href="#合并数据集" class="headerlink" title="合并数据集"></a>合并数据集</h2><p>🤗 Datasets provides an <code>interleave_datasets()</code> function that converts a list of <code>IterableDataset</code> objects into a single <code>IterableDataset</code>, where the elements of the new dataset are obtained by alternating among the source examples. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">law_dataset_streamed = load_dataset(</span><br><span class="line">    <span class="string">&quot;json&quot;</span>,</span><br><span class="line">    data_files=<span class="string">&quot;https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst&quot;</span>,</span><br><span class="line">    split=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">    streaming=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(law_dataset_streamed))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;meta&#x27;: &#123;&#x27;case_ID&#x27;: &#x27;110921.json&#x27;,</span></span><br><span class="line"><span class="string">  &#x27;case_jurisdiction&#x27;: &#x27;scotus.tar.gz&#x27;,</span></span><br><span class="line"><span class="string">  &#x27;date_created&#x27;: &#x27;2010-04-28T17:12:49Z&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#x27;text&#x27;: &#x27;\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 03 微调范式</title>
      <link href="/posts/18130.html"/>
      <url>/posts/18130.html</url>
      
        <content type="html"><![CDATA[<h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1><p>In this section we will use as an example the <strong>MRPC</strong> (Microsoft Research Paraphrase Corpus) dataset, introduced in a <a href="https://www.aclweb.org/anthology/I05-5002.pdf">paper</a> by William B. Dolan and Chris Brockett. <strong>The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases</strong> or not (i.e., if both sentences mean the same thing). We’ve selected it for this chapter <strong>because it’s a small dataset,</strong> so it’s easy to experiment with training on it.</p><p> let’s focus on the MRPC dataset! This is one of the 10 datasets composing the <a href="https://gluebenchmark.com/">GLUE benchmark</a></p><ul><li><p>使用的是MRPC，很小很好实验</p></li><li><p>它属于 GLUE</p></li></ul><p>接下来查看下数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">raw_datasets</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 3668</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    validation: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 408</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    test: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 1725</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">raw_train_dataset[<span class="number">0</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;idx&#x27;: 0,</span></span><br><span class="line"><span class="string"> &#x27;label&#x27;: 1,</span></span><br><span class="line"><span class="string"> &#x27;sentence1&#x27;: &#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;sentence2&#x27;: &#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">raw_train_dataset.features</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;sentence1&#x27;: Value(dtype=&#x27;string&#x27;, id=None),</span></span><br><span class="line"><span class="string"> &#x27;sentence2&#x27;: Value(dtype=&#x27;string&#x27;, id=None),</span></span><br><span class="line"><span class="string"> &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;not_equivalent&#x27;, &#x27;equivalent&#x27;], names_file=None, id=None),</span></span><br><span class="line"><span class="string"> &#x27;idx&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>tokenizer方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokenized_dataset = tokenizer(</span><br><span class="line">    raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence1&quot;</span>],</span><br><span class="line">    raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence2&quot;</span>],</span><br><span class="line">    padding=<span class="literal">True</span>,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>This works well, but it has the disadvantage of returning a dictionary (with our keys, <code>input_ids</code>, <code>attention_mask</code>, and <code>token_type_ids</code>, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the 🤗 Datasets library are <a href="https://arrow.apache.org/">Apache Arrow</a> files stored on the disk, so you only keep the samples you ask for loaded in memory).</p><ul><li>占内存</li></ul><p>dataset.map 方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">tokenized_datasets</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;attention_mask&#x27;, &#x27;idx&#x27;, &#x27;input_ids&#x27;, &#x27;label&#x27;, &#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;token_type_ids&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 3668</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    validation: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;attention_mask&#x27;, &#x27;idx&#x27;, &#x27;input_ids&#x27;, &#x27;label&#x27;, &#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;token_type_ids&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 408</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    test: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;attention_mask&#x27;, &#x27;idx&#x27;, &#x27;input_ids&#x27;, &#x27;label&#x27;, &#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;token_type_ids&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 1725</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>since the <code>tokenizer</code> works on lists of pairs of sentences, as seen before. This will allow us to use the option <code>batched=True</code> in our call to <code>map()</code>, which will greatly speed up the tokenization. The <code>tokenizer</code> is backed by a tokenizer written in Rust from the <a href="https://github.com/huggingface/tokenizers">🤗 Tokenizers</a> library. This tokenizer can be very fast, but only if we give it lots of inputs at once.</p><ul><li>batch</li></ul><p>This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, <strong>as then we only need to pad to the maximum length in that batch</strong>, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!</p><ul><li>batch内最长padding，将在下一小节介绍<code>DataCollatorWithPadding</code></li></ul><p>You can even use multiprocessing when applying your preprocessing function with <code>map()</code> by passing along a <code>num_proc</code> argument. We didn’t do this here because the 🤗 Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.</p><ul><li>多线程</li></ul><blockquote><p>but note that if you’re training on a TPU it can cause problems — TPUs prefer fixed shapes, even when that requires extra padding.</p><ul><li>tpu更喜欢恒定形状，所以你很少数据with large pad 也没事</li></ul></blockquote><h2 id="Dynamic-padding"><a href="#Dynamic-padding" class="headerlink" title="Dynamic padding"></a>Dynamic padding</h2><p>The function that is responsible for putting together samples inside a batch is called a <em>collate function</em>. It’s an argument you can pass when you build a <code>DataLoader</code>, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won’t be possible in our case since the inputs we have won’t all be of the same size.</p><ul><li>DataCollator可以看成是一个函数，可以传入pytorch的dataloader的<code>collate_fn</code>参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer) <span class="comment"># 有model参数，可以把model也让collator知道</span></span><br></pre></td></tr></table></figure><p>军火展示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">samples = tokenized_datasets[<span class="string">&quot;train&quot;</span>][:<span class="number">8</span>]</span><br><span class="line">samples = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> samples.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;idx&quot;</span>, <span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>]&#125;</span><br><span class="line">[<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> samples[<span class="string">&quot;input_ids&quot;</span>]]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[50, 59, 47, 67, 59, 50, 62, 32]&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">batch = data_collator(samples)</span><br><span class="line">&#123;k: v.shape <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;attention_mask&#x27;: torch.Size([8, 67]),</span></span><br><span class="line"><span class="string"> &#x27;input_ids&#x27;: torch.Size([8, 67]),</span></span><br><span class="line"><span class="string"> &#x27;token_type_ids&#x27;: torch.Size([8, 67]),</span></span><br><span class="line"><span class="string"> &#x27;labels&#x27;: torch.Size([8])&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="Fine-tune"><a href="#Fine-tune" class="headerlink" title="Fine-tune"></a>Fine-tune</h1><h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p>首先是TrainingArguments的定义</p><p>The first step before we can define our <code>Trainer</code> is to define a <code>TrainingArguments</code> class that will contain all the hyperparameters the <code>Trainer</code> will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(<span class="string">&quot;test-trainer&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>You will notice that unlike in <a href="https://huggingface.co/course/chapter2">Chapter 2</a>, you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.</p><ul><li>使用细分模型 (automode后带任务名称的)不会得到警告，是因为他会加载 最后面那个多分类的权重给你，这样预训练就又快了些，上面写的head 应该是指classifier的那几层吧。</li></ul><p>接下来可以train了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    training_args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train() <span class="comment"># 调用训练</span></span><br></pre></td></tr></table></figure><p>Note that when you pass the <code>tokenizer</code> as we did here, the default <code>data_collator</code> used by the <code>Trainer</code> will be a <code>DataCollatorWithPadding</code> as defined previously, so you can skip the line <code>data_collator=data_collator</code> in this call.</p><p>This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps.</p><ul><li>不写collate的话默认就是DataCollatorWithPadding，不过声明一下，比较好，为了可读性</li><li>每500步给你一个loss返回，<del>看看need loss有多大</del></li></ul><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictions = trainer.predict(tokenized_datasets[<span class="string">&quot;validation&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(predictions.predictions.shape, predictions.label_ids.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (408, 2) (408,)</span></span><br></pre></td></tr></table></figure><ul><li>predict的结果就是二分类的logits 接下来做个argmax 取位置信息即可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">preds = np.argmax(predictions.predictions, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">metric.compute(predictions=preds, references=predictions.label_ids)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;accuracy&#x27;: 0.8578431372549019, &#x27;f1&#x27;: 0.8996539792387542&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>The table in the <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT paper</a> reported an F1 score of 88.9 for the base model. That was the <code>uncased</code> model while we are currently using the <code>cased</code> model, which explains the better result.</p><ul><li>上面说 微调的power！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_preds</span>):</span><br><span class="line">    metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">    logits, labels = eval_preds</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure><p>封装代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">training_args = TrainingArguments(<span class="string">&quot;test-trainer&quot;</span>, evaluation_strategy=<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    training_args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train() <span class="comment"># 狠狠的训练</span></span><br></pre></td></tr></table></figure><ul><li>这里我们设置每个epoch进行一次evaluation，评测方法用上面封装的函数</li></ul><p>The <code>Trainer</code> will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use <code>fp16 = True</code> in your training arguments)</p><ul><li>如果你的机器支持float 16训练速度会更快</li></ul><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上是通过trainer定义的训练，可以作为我们正式工作前的训练测试，接下来我们使用pytorch来正式工作。</p><h1 id="full-training"><a href="#full-training" class="headerlink" title="full training"></a>full training</h1><h2 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h2><p>we need to apply a bit of postprocessing to our <code>tokenized_datasets</code>, to take care of some things that the <code>Trainer</code> did for us automatically. </p><ul><li><p>Remove the columns corresponding to values the model does not expect (like the <code>sentence1</code> and <code>sentence2</code> columns).</p></li><li><p>Rename the column <code>label</code> to <code>labels</code> (because the model expects the argument to be named <code>labels</code>).</p></li><li><p><strong>Set the format of the datasets</strong> so they return PyTorch tensors instead of lists.</p></li><li><p>做点后处理，删除、改名、return PyTorch tensors</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>, <span class="string">&quot;idx&quot;</span>])</span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)</span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">tokenized_datasets[<span class="string">&quot;train&quot;</span>].column_names</span><br></pre></td></tr></table></figure><p>remove的操作可以在<code>dataset.map</code>里面设置<code>remove_columns=split_datasets[&quot;train&quot;].column_names</code>, 记得查看一下name别删除了不该删除的东西</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>], shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;validation&quot;</span>], batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">&#123;k: v.shape <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;attention_mask&#x27;: torch.Size([8, 65]),</span></span><br><span class="line"><span class="string"> &#x27;input_ids&#x27;: torch.Size([8, 65]),</span></span><br><span class="line"><span class="string"> &#x27;labels&#x27;: torch.Size([8]),</span></span><br><span class="line"><span class="string"> &#x27;token_type_ids&#x27;: torch.Size([8, 65])&#125;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>加载数据并check一下形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">outputs = model(**batch)</span><br><span class="line"><span class="built_in">print</span>(outputs.loss, outputs.logits.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(0.5441, grad_fn=&lt;NllLossBackward&gt;) torch.Size([8, 2])&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></table></figure><p>Finally, the learning rate scheduler used by default is just a linear decay from the <strong>maximum value (5e-5) to 0</strong>. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). </p><ul><li>我推荐 3e-4 cosine的组合</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(num_training_steps)</span><br><span class="line"><span class="comment"># 1377</span></span><br></pre></td></tr></table></figure><h2 id="loop-amp-evaluation"><a href="#loop-amp-evaluation" class="headerlink" title="loop &amp; evaluation"></a>loop &amp; evaluation</h2><p>train 阶段就是pytorch那几样，eval换成了HF的api</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line"></span><br><span class="line">metric.compute()</span><br><span class="line"><span class="comment"># &#123;&#x27;accuracy&#x27;: 0.8431372549019608, &#x27;f1&#x27;: 0.8907849829351535&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Accelerate"><a href="#Accelerate" class="headerlink" title="Accelerate"></a>Accelerate</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">+ <span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line">  <span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler</span><br><span class="line"></span><br><span class="line">+ accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line">  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">  optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br><span class="line"></span><br><span class="line">- device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">- model.to(device)</span><br><span class="line"></span><br><span class="line">+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(</span><br><span class="line">+     train_dataloader, eval_dataloader, model, optimizer</span><br><span class="line">+ )</span><br><span class="line"></span><br><span class="line">  num_epochs = <span class="number">3</span></span><br><span class="line">  num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">  lr_scheduler = get_scheduler(</span><br><span class="line">      <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">      optimizer=optimizer,</span><br><span class="line">      num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">      num_training_steps=num_training_steps</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">      <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">-         batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">          outputs = model(**batch)</span><br><span class="line">          loss = outputs.loss</span><br><span class="line">-         loss.backward()</span><br><span class="line">+         accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">          optimizer.step()</span><br><span class="line">          lr_scheduler.step()</span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line">          progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>进行以上代码的修改，减号就是在原先的代码上删减的，加号反之</p><p>The first line to add is the import line. The second line instantiates an <code>Accelerator</code> object that will look at the environment and initialize the proper distributed setup. 🤗 Accelerate handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use <code>accelerator.device</code> instead of <code>device</code>).</p><p>Then the main bulk of the work is done in the line that <strong>sends the dataloaders, the model, and the optimizer to</strong> <code>accelerator.prepare()</code>. This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the <code>device</code> (again, if you want to keep this you can just change it to use <code>accelerator.device</code>) and replacing <code>loss.backward()</code> with <code>accelerator.backward(loss)</code>.</p><ul><li>引入并实例化</li><li>将 the dataloaders, the model, and the optimizer送入accelerator</li><li>使用accelerator.backward(loss)</li></ul><blockquote><p>In order to benefit from the speed-up offered by Cloud TPUs, we recommend padding your samples to a fixed length with the <code>padding=&quot;max_length&quot;</code> and <code>max_length</code> arguments of the tokenizer.</p><ul><li>如果你使用tpu记得直接放最大长度就可以了</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 02 API概要</title>
      <link href="/posts/64185.html"/>
      <url>/posts/64185.html</url>
      
        <content type="html"><![CDATA[<h1 id="outputs"><a href="#outputs" class="headerlink" title="outputs"></a>outputs</h1><p>Note that the outputs of 🤗 Transformers models behave like <code>namedtuple</code>s or dictionaries. You can access the elements by attributes (like we did) or by key (<code>outputs[&quot;last_hidden_state&quot;]</code>), or even by index if you know exactly where the thing you are looking for is (<code>outputs[0]</code>).</p><ul><li>HF的输入返回 大多以是元组或字典形式出，处理的时候要注意。</li></ul><p><strong>从二分类到多分类，多分类中每个类别分别作二分类，是否属于这个类别进行输出</strong></p><p><code>[[0.2,0.8]、[0.4,0.6]、[0.7,0.3]]</code>  &#x3D; <code>[[1]、[1]、[0]]</code></p><h1 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the config</span></span><br><span class="line">config = BertConfig()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the model from the config</span></span><br><span class="line">model = BertModel(config)</span><br><span class="line">config</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">BertConfig &#123;</span></span><br><span class="line"><span class="string">  [...]</span></span><br><span class="line"><span class="string">  &quot;hidden_size&quot;: 768,</span></span><br><span class="line"><span class="string">  &quot;intermediate_size&quot;: 3072,</span></span><br><span class="line"><span class="string">  &quot;max_position_embeddings&quot;: 512,</span></span><br><span class="line"><span class="string">  &quot;num_attention_heads&quot;: 12,</span></span><br><span class="line"><span class="string">  &quot;num_hidden_layers&quot;: 12,</span></span><br><span class="line"><span class="string">  [...]</span></span><br><span class="line"><span class="string">&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h1><p>The weights have been downloaded and cached (so future calls to the <code>from_pretrained()</code> method won’t re-download them) in the cache folder, which defaults to <em>~&#x2F;.cache&#x2F;huggingface&#x2F;transformers</em>. You can customize your cache folder by setting the <code>HF_HOME</code> environment variable.</p><ul><li>配置你当前的环境变量 <code>os.environ[&#39;HF_HOME&#39;]= &#39;~/.cache/huggingface/transformers&#39; </code></li></ul><h1 id="Saving"><a href="#Saving" class="headerlink" title="Saving"></a>Saving</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">&quot;directory_on_my_computer&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">This saves two files to your disk:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ls directory_on_my_computer</span></span><br><span class="line"><span class="string">config.json pytorch_model.bin&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><p>If you take a look at the <em><strong>config.json</strong></em> file, you’ll recognize the attributes necessary to build the <strong>model architecture</strong>. This file also contains some <strong>metadata</strong>, such as where the <strong>checkpoint originated</strong> and what 🤗 <strong>Transformers version you were using</strong> when you last saved the checkpoint</p></li><li><p>The <em><strong>pytorch_model.bin</strong></em> file is known as the <em><strong>state dictionary</strong></em>; it contains all your <strong>model’s weights</strong>. The two files go hand in hand; the <strong>configuration is necessary to know your model’s architecture</strong>, while <strong>the model weights are your model’s parameters</strong>.</p></li></ul><h1 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h1><h2 id="encode"><a href="#encode" class="headerlink" title="encode"></a>encode</h2><p>通过<code>tokenizer.tokenize(sequence)</code>查看分词后的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line">sequence = <span class="string">&quot;Using a Transformer network is simple&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(sequence)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;Using&#x27;, &#x27;a&#x27;, &#x27;transform&#x27;, &#x27;##er&#x27;, &#x27;network&#x27;, &#x27;is&#x27;, &#x27;simple&#x27;]&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><code>ids = tokenizer.convert_tokens_to_ids(tokens)</code></p><p>还可以反过来得到token，也就是跟上面的 tokenize(seq) 一样的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ids)</span><br></pre></td></tr></table></figure><h2 id="decode"><a href="#decode" class="headerlink" title="decode"></a>decode</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">decoded_string = tokenizer.decode([<span class="number">7993</span>, <span class="number">170</span>, <span class="number">11303</span>, <span class="number">1200</span>, <span class="number">2443</span>, <span class="number">1110</span>, <span class="number">3014</span>])</span><br><span class="line"><span class="built_in">print</span>(decoded_string)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;Using a Transformer network is simple&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><p>查看<code>tokenizer.pad_token_id</code> pad的id</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Will pad the sequences up to the maximum sequence length</span></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="string">&quot;longest&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Will pad the sequences up to the model max length</span></span><br><span class="line"><span class="comment"># (512 for BERT or DistilBERT)</span></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="string">&quot;max_length&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Will pad the sequences up to the specified max length</span></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="string">&quot;max_length&quot;</span>, max_length=<span class="number">8</span>)</span><br></pre></td></tr></table></figure><ul><li>直接max_length是到模型的最大长度，longest是到批次里句子的最大长度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sequence1_ids = [[<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>]]</span><br><span class="line">sequence2_ids = [[<span class="number">200</span>, <span class="number">200</span>]]</span><br><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string">tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string">tensor([[ 1.5694, -1.3895],</span></span><br><span class="line"><span class="string">        [ 1.3373, -1.2163]], grad_fn=&lt;AddmmBackward&gt;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>This is because the key feature of Transformer models is attention layers that <em>contextualize</em> each token. <strong>These will take into account the padding tokens since they attend to all of the tokens of a sequence</strong>. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, <strong>we need to tell those attention layers to ignore the padding tokens</strong>. This is done by using an <strong>attention mask.</strong></p><ul><li>这里两条单独的数据的结果跟组合起来的是不同的，是因为pad的位置也分散了模型的注意力，这不是我们希望模型学习的地方</li></ul><h3 id="ATTENTION-MASK"><a href="#ATTENTION-MASK" class="headerlink" title="ATTENTION MASK"></a>ATTENTION MASK</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">attention_mask = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))</span><br><span class="line"><span class="built_in">print</span>(outputs.logits)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 1.5694, -1.3895],</span></span><br><span class="line"><span class="string">        [ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward&gt;)&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="长句子处理"><a href="#长句子处理" class="headerlink" title="长句子处理"></a>长句子处理</h1><p>Models have different supported sequence lengths, and some specialize in handling very long sequences. <strong><a href="https://huggingface.co/transformers/model_doc/longformer.html">Longformer</a> is one example, and another is <a href="https://huggingface.co/transformers/model_doc/led.html">LED</a>. If you’re working on a task that requires very long sequences, we recommend you take a look at those models.</strong></p><p>一般是在tokenizer里设置truncation max_len，这里没讲，可以去看看模型。</p>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF Course 01 基础概念</title>
      <link href="/posts/16149.html"/>
      <url>/posts/16149.html</url>
      
        <content type="html"><![CDATA[<p>Some of the currently <a href="https://huggingface.co/transformers/main_classes/pipelines.html">available pipelines</a> are:</p><ul><li><code>feature-extraction</code> (get the vector representation of a text)</li><li><code>fill-mask</code></li><li><code>ner</code> (named entity recognition)</li><li><code>question-answering</code></li><li><code>sentiment-analysis</code></li><li><code>summarization</code></li><li><code>text-generation</code></li><li><code>translation</code></li><li><code>zero-shot-classification</code></li></ul><h1 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">generator = pipeline(<span class="string">&quot;text-generation&quot;</span>, model=<span class="string">&quot;distilgpt2&quot;</span>)</span><br><span class="line">generator(</span><br><span class="line">    <span class="string">&quot;In this course, we will teach you how to&quot;</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    num_return_sequences=<span class="number">2</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="model"><a href="#model" class="headerlink" title="model"></a>model</h1><p>模型发展时间史</p><ul><li><p><strong>June 2018</strong>: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results</p></li><li><p><strong>October 2018</strong>: <a href="https://arxiv.org/abs/1810.04805">BERT</a>, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)</p></li><li><p><strong>February 2019</strong>: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns</p></li><li><p><strong>October 2019</strong>: <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance</p></li><li><p><strong>October 2019</strong>: <a href="https://arxiv.org/abs/1910.13461">BART</a> and <a href="https://arxiv.org/abs/1910.10683">T5</a>, two large pretrained models using the same architecture as the original Transformer model (the first to do so)</p></li><li><p><strong>May 2020</strong>, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called <em>zero-shot learning</em>)</p></li><li><p>GPT-like (also called <em>auto-regressive</em> Transformer models)</p></li><li><p>BERT-like (also called <em>auto-encoding</em> Transformer models)</p></li><li><p>BART&#x2F;T5-like (also called <em>sequence-to-sequence</em> Transformer models)</p></li></ul><h1 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder-decoder"></a>encoder-decoder</h1><p>特攻类编码器的主要用处</p><ul><li><strong>Encoder-only models</strong>: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.<ul><li><a href="https://huggingface.co/transformers/model_doc/albert.html">ALBERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/bert.html">BERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/electra.html">ELECTRA</a></li><li><a href="https://huggingface.co/transformers/model_doc/roberta.html">RoBERTa</a></li></ul></li><li><strong>Decoder-only models</strong>: Good for generative tasks such as text generation<ul><li><a href="https://huggingface.co/transformers/model_doc/ctrl.html">CTRL</a></li><li><a href="https://huggingface.co/transformers/model_doc/gpt.html">GPT</a></li><li><a href="https://huggingface.co/transformers/model_doc/gpt2.html">GPT-2</a></li><li><a href="https://huggingface.co/transformers/model_doc/transfo-xl.html">Transformer XL</a>.</li></ul></li><li><strong>Encoder-decoder models</strong> or <strong>sequence-to-sequence models</strong>: Good for generative tasks that require an input, such as translation or summarization.<ul><li><a href="https://huggingface.co/transformers/model_doc/bart.html">BART</a></li><li><a href="https://huggingface.co/transformers/model_doc/mbart.html">mBART</a></li><li><a href="https://huggingface.co/transformers/model_doc/marian.html">Marian</a></li><li><a href="https://huggingface.co/transformers/model_doc/t5.html">T5</a></li></ul></li></ul><p>cross-attention层使得decoder能查看整个句意，以调整顺序翻译输出</p><p>Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, <strong>but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word</strong>. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.</p><p>架构和检查点</p><ul><li><strong>Architecture</strong>: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.</li><li><strong>Checkpoints</strong>: These are the weights that will be loaded in a given architecture.</li><li><strong>Model</strong>: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify <em>architecture</em> or <em>checkpoint</em> when it matters to reduce ambiguity.</li></ul><p>For example, BERT is an architecture while <code>bert-base-cased</code>, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the <code>bert-base-cased</code> model.”</p><p>即使使用干净的词库，也可能产生性别歧视，种族歧视</p><p>When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter&#x2F;waitress). The others are work occupations usually associated with one specific gender — and yes, prostitute ended up in the top 5 possibilities the model associates with “woman” and “work.” <strong>This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet</strong>, but rather using apparently neutral data (it’s trained on the <a href="https://huggingface.co/datasets/wikipedia">English Wikipedia</a> and <a href="https://huggingface.co/datasets/bookcorpus">BookCorpus</a> datasets).</p><p>When you use these tools, you therefore need to keep in the back of your mind that <strong>the original model you are using could very easily generate sexist, racist, or homophobic content</strong>. Fine-tuning the model on your data won’t make this intrinsic bias disappear.</p>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP Baseline 01 翻译</title>
      <link href="/posts/58033.html"/>
      <url>/posts/58033.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>从头训练，不如fine-tune，如果你比Google &amp; Mate 有钱当我没说</p></blockquote><h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>Accelarator</li><li>get_scheduler</li><li>custom_wandb</li></ul><h1 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h1><p>这里我们使用zh-en的数据集和模型，进行翻译任务</p><p>这里需要注册一个wandb的账号，记得啊。</p><h1 id="示例查看"><a href="#示例查看" class="headerlink" title="示例查看"></a>示例查看</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">prx = &#123;<span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://127.0.0.1:7890&#x27;</span>&#125;</span><br><span class="line">model_name = <span class="string">&quot;Helsinki-NLP/opus-mt-zh-en&quot;</span></span><br><span class="line">save_path = <span class="string">r&#x27;D:\00mydataset\huggingface model&#x27;</span></span><br><span class="line">data_path = <span class="string">r&#x27;D:\00mydataset\huggingface dataset&#x27;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name, proxies=prx, cache_dir=save_path)</span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name, proxies=prx, cache_dir=save_path)</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&#x27;news_commentary&#x27;</span>,<span class="string">&#x27;en-fr&#x27;</span>,cache_dir=data_path)</span><br><span class="line">dataset</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;id&#x27;, &#x27;translation&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 69206</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个挂个代理加速下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">PreTrainedTokenizer(name_or_path=&#x27;Helsinki-NLP/opus-mt-zh-en&#x27;, vocab_size=65001, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;&#125;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">dataset[&#x27;</span>train<span class="string">&#x27;][1][&#x27;</span>translation<span class="string">&#x27;]</span></span><br><span class="line"><span class="string">&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;translation&#x27;</span>: &#123;<span class="string">&#x27;en&#x27;</span>: <span class="string">&#x27;PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening. At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;zh&#x27;</span>: <span class="string">&#x27;巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。&#x27;</span>&#125;&#125;</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  </span></span><br></pre></td></tr></table></figure><p>查看下数据, 可以看到返回的是字典形式，我们主要用到translation下的en、zh</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">s1 = <span class="string">&#x27;天下第一美少女, 罢了&#x27;</span></span><br><span class="line">inputs = tokenizer(s1, return_tensors=<span class="string">&#x27;pt&#x27;</span>,)</span><br><span class="line">inputs</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&#123;&#x27;input_ids&#x27;: tensor([[ 9705,   359,  3615,  2797, 14889,     2,     7, 40798,     0]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])&#125;,)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">outputs = model.generate(**inputs)</span><br><span class="line">tokenizer.batch_decode(outputs, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&quot;The most beautiful girl in the world, that&#x27;s all.&quot;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>看下输出，还可以</p><blockquote><p>注意，AutoModelForSeq2SeqLM不同于AutoModel的就是加入了<code>model.generate</code>这个特性。</p><p>不然model(**inputs)是要你补充目标语言的。</p></blockquote><blockquote><p>If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting <code>tokenizer.src_lang</code> and <code>tokenizer.tgt_lang</code> to the right values.</p><ul><li>​如果你使用多语言模型，你得指定你的源语言和目标语言的参数</li></ul></blockquote><hr><h1 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">split_datasets = raw_datasets[<span class="string">&quot;train&quot;</span>].train_test_split(train_size=<span class="number">0.9</span>, seed=<span class="number">20</span>)</span><br><span class="line">split_datasets</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DatasetDict(&#123;</span></span><br><span class="line"><span class="string">    train: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;id&#x27;, &#x27;translation&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 189155</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    test: Dataset(&#123;</span></span><br><span class="line"><span class="string">        features: [&#x27;id&#x27;, &#x27;translation&#x27;],</span></span><br><span class="line"><span class="string">        num_rows: 21018</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">&#125;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">split_datasets[<span class="string">&quot;validation&quot;</span>] = split_datasets.pop(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>HF的dataset可以直接调用<code>.train_test_split(train_size=0.9, seed=20)</code></p><ul><li>HF的dataset可以直接转DataFrame，这样你也可以直接配合Sklearn使用</li></ul></li><li><p>给test重命名为validation</p></li></ul><h2 id="DataCollatorForSeq2Seq"><a href="#DataCollatorForSeq2Seq" class="headerlink" title="DataCollatorForSeq2Seq"></a>DataCollatorForSeq2Seq</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">max_length = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = [ex[<span class="string">&quot;en&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    targets = [ex[<span class="string">&quot;fr&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    model_inputs = tokenizer(</span><br><span class="line">        inputs, text_target=targets, max_length=max_length, truncation=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br><span class="line">    </span><br><span class="line">tokenized_datasets = split_datasets.<span class="built_in">map</span>(</span><br><span class="line">    preprocess_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=split_datasets[<span class="string">&quot;train&quot;</span>].column_names,)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">这里本来还有[&#x27;id&#x27;, &#x27;translation&#x27;],通过下面的设置就删除了。</span></span><br><span class="line"><span class="string">remove_columns:</span></span><br><span class="line"><span class="string">    Remove a selection of columns while doing the mapping.</span></span><br><span class="line"><span class="string">    Columns will be removed before updating the examples with the output of `function`,i.e. </span></span><br><span class="line"><span class="string">    if `function` is adding columns with names in `remove_columns`, these columns will be kept.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>We don’t pay attention to the attention mask of the targets, as the model won’t expect it. Instead, <strong>the labels corresponding to a padding token should be set to <code>-100</code> so they are ignored in the loss computatio</strong>n. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to <code>-100</code>.</p><p>这里我们不会加入padding，mask。之后我们的mask会设成-100 使其不会计算损失。这些都是下一步的操作</p></blockquote><p>ps: 今天看到个bug，应该是没有更新到最新版版，具体来说就是tokenizer之后没有label，如果bug了，可以进行以下替换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">    inputs = [ex[<span class="string">&quot;zh&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    targets = [ex[<span class="string">&quot;en&quot;</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples[<span class="string">&quot;translation&quot;</span>]]</span><br><span class="line">    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set up the tokenizer for targets</span></span><br><span class="line">    <span class="keyword">with</span> tokenizer.as_target_tokenizer():</span><br><span class="line">        labels = tokenizer(targets, max_length=max_target_length, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model_inputs[<span class="string">&quot;labels&quot;</span>] = labels[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br><span class="line"></span><br><span class="line">tokenized_datasets = split_datasets.<span class="built_in">map</span>(</span><br><span class="line">    preprocess_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=split_datasets[<span class="string">&quot;train&quot;</span>].column_names,)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForSeq2Seq</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)</span><br><span class="line">batch = data_collator([tokenized_datasets[<span class="string">&quot;train&quot;</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>)])</span><br><span class="line">batch.keys()</span><br><span class="line"><span class="comment"># dict_keys([&#x27;attention_mask&#x27;, &#x27;input_ids&#x27;, &#x27;labels&#x27;, &#x27;decoder_input_ids&#x27;])</span></span><br><span class="line"></span><br><span class="line">batch[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> tensor([[57483,     7,  3241,   403,     3,   289,  1817, 25787,    22,     6,</span></span><br><span class="line"><span class="string">          38697,    22,     2,     3,   426,    64,    72, 27734,    14,  9054,</span></span><br><span class="line"><span class="string">          56467,  6667,     8,   721,   512,  2498,   209,    64,    72, 11468,</span></span><br><span class="line"><span class="string">              5,   393,     3,  2597,     4,     3,  1817,     2,   469,   235,</span></span><br><span class="line"><span class="string">            238, 24898,    39,     8, 13579,    50, 17528,     2,    60,    42,</span></span><br><span class="line"><span class="string">          56548,     2,   695,   443, 10119,  5543,     8, 53617,     7, 38261,</span></span><br><span class="line"><span class="string">          40490,    22,     5,     0],</span></span><br><span class="line"><span class="string">         [   24, 22026,    30,  2329, 10349, 22901,    20, 52813,    17,    50,</span></span><br><span class="line"><span class="string">             12, 29940,     4,     3,  2121,    20,  1843,    45,    67,   243,</span></span><br><span class="line"><span class="string">           1945,    30,   368, 36681,    10,     3,  1796,     4, 14961,  2203,</span></span><br><span class="line"><span class="string">              6, 28291,     3, 22986,     2, 11355,     3,  3368,    64,  8700,</span></span><br><span class="line"><span class="string">             18,   469, 38575,    10,   278,    54,     8,  4291,    57, 22301,</span></span><br><span class="line"><span class="string">           1718,     8,   959, 30229,  1294,  6855,  4298,     5,     0,  -100,</span></span><br><span class="line"><span class="string">           -100,  -100,  -100,  -100]])&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看下原来的token</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokenized_datasets[<span class="string">&quot;train&quot;</span>][i][<span class="string">&quot;labels&quot;</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[57483, 7, 3241, 403, 3, 289, 1817, 25787, 22, 6, 38697, 22, 2, 3, 426, 64, 72, 27734, 14, 9054, 56467, 6667, 8, 721, 512, 2498, 209, 64, 72, 11468, 5, 393, 3, 2597, 4, 3, 1817, 2, 469, 235, 238, 24898, 39, 8, 13579, 50, 17528, 2, 60, 42, 56548, 2, 695, 443, 10119, 5543, 8, 53617, 7, 38261, 40490, 22, 5, 0]</span></span><br><span class="line"><span class="string">[24, 22026, 30, 2329, 10349, 22901, 20, 52813, 17, 50, 12, 29940, 4, 3, 2121, 20, 1843, 45, 67, 243, 1945, 30, 368, 36681, 10, 3, 1796, 4, 14961, 2203, 6, 28291, 3, 22986, 2, 11355, 3, 3368, 64, 8700, 18, 469, 38575, 10, 278, 54, 8, 4291, 57, 22301, 1718, 8, 959, 30229, 1294, 6855, 4298, 5, 0]&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>可以看到padding的位置都变成-100了，<a href="https://poyone.github.io/posts/13310.html">pytorch中也有这个设定可见我之前讲Transformer的内容</a></li></ul><blockquote><p> This is all done by a <a href="https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq"><code>DataCollatorForSeq2Seq</code></a>. Like the <code>DataCollatorWithPadding</code>, it takes the <code>tokenizer</code> used to preprocess the inputs, but it also takes the <code>model</code>. This is because this <strong>data collator will also be responsible for preparing the decoder input IDs</strong>, which are <strong>shifted versions of the labels</strong>（移动版的标签） with a special token at the beginning. Since this shift is done slightly differently for different architectures, the <code>DataCollatorForSeq2Seq</code> needs to know the <code>model</code> object<del>搞得还挺复杂</del></p></blockquote><hr><h1 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h1><blockquote><p>One weakness with <strong>BLEU is that it expects the text to already be tokenized</strong>, <strong>which makes it difficult to compare scores</strong> <strong>between models that use different tokenizers</strong>. So instead, the most commonly used metric for <strong>benchmarking translation models today is <a href="https://github.com/mjpost/sacrebleu">SacreBLEU</a>,</strong> which addresses this weakness (and others) by standardizing the tokenization step</p></blockquote><ul><li>这里我们加入SacreBLEU作为评分标准</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!pip install sacrebleu</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line">metric = evaluate.load(<span class="string">&quot;sacrebleu&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>示例1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">predictions = [</span><br><span class="line">    <span class="string">&quot;This plugin lets you translate web pages between several languages automatically.&quot;</span></span><br><span class="line">]</span><br><span class="line">references = [</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span></span><br><span class="line">    ]</span><br><span class="line">]</span><br><span class="line">metric.compute(predictions=predictions, references=references)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;score&#x27;: 46.750469682990165,</span></span><br><span class="line"><span class="string"> &#x27;counts&#x27;: [11, 6, 4, 3],</span></span><br><span class="line"><span class="string"> &#x27;totals&#x27;: [12, 11, 10, 9],</span></span><br><span class="line"><span class="string"> &#x27;precisions&#x27;: [91.67, 54.54, 40.0, 33.33],</span></span><br><span class="line"><span class="string"> &#x27;bp&#x27;: 0.9200444146293233,</span></span><br><span class="line"><span class="string"> &#x27;sys_len&#x27;: 12,</span></span><br><span class="line"><span class="string"> &#x27;ref_len&#x27;: 13&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure><p>示例2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">predictions = [<span class="string">&quot;This This This This&quot;</span>]</span><br><span class="line">references = [</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;This plugin allows you to automatically translate web pages between several languages.&quot;</span></span><br><span class="line">    ]</span><br><span class="line">]</span><br><span class="line">metric.compute(predictions=predictions, references=references)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;score&#x27;: 1.683602693167689,</span></span><br><span class="line"><span class="string"> &#x27;counts&#x27;: [1, 0, 0, 0],</span></span><br><span class="line"><span class="string"> &#x27;totals&#x27;: [4, 3, 2, 1],</span></span><br><span class="line"><span class="string"> &#x27;precisions&#x27;: [25.0, 16.67, 12.5, 12.5],</span></span><br><span class="line"><span class="string"> &#x27;bp&#x27;: 0.10539922456186433,</span></span><br><span class="line"><span class="string"> &#x27;sys_len&#x27;: 4,</span></span><br><span class="line"><span class="string"> &#x27;ref_len&#x27;: 13&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure><blockquote><p>This gets a BLEU score of 46.75, which is rather good — for reference, the original Transformer model in the <a href="https://arxiv.org/pdf/1706.03762.pdf">“Attention Is All You Need” paper</a> achieved a BLEU score of 41.8 on a similar translation task between English and French! (For more information about the individual metrics, like <code>counts</code> and <code>bp</code>, see the <a href="https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74">SacreBLEU repository</a>.) </p><ul><li>已经比擎天柱还厉害了，另外的标准如下:<ul><li>score: The BLEU score.  </li><li>counts: List of counts of correct ngrams, 1 &lt;&#x3D; n &lt;&#x3D; max_ngram_order  </li><li>totals: List of counts of total ngrams, 1 &lt;&#x3D; n &lt;&#x3D; max_ngram_order </li><li>precisions: List of precisions, 1 &lt;&#x3D; n &lt;&#x3D; max_ngram_order  </li><li>bp: The brevity penalty. </li><li>sys_len: The cumulative system length.</li><li>ref_len: The cumulative reference length.</li></ul></li></ul></blockquote><h2 id="Compute-metrics"><a href="#Compute-metrics" class="headerlink" title="Compute_metrics"></a>Compute_metrics</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_preds</span>):</span><br><span class="line">    preds, labels = eval_preds</span><br><span class="line">    <span class="comment"># In case the model returns more than the prediction logits</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(preds, <span class="built_in">tuple</span>):</span><br><span class="line">        preds = preds[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Replace -100s in the labels as we can&#x27;t decode them</span></span><br><span class="line">    labels = np.where(labels != -<span class="number">100</span>, labels, tokenizer.pad_token_id)</span><br><span class="line">    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Some simple post-processing</span></span><br><span class="line">    decoded_preds = [pred.strip() <span class="keyword">for</span> pred <span class="keyword">in</span> decoded_preds]</span><br><span class="line">    decoded_labels = [[label.strip()] <span class="keyword">for</span> label <span class="keyword">in</span> decoded_labels]</span><br><span class="line"></span><br><span class="line">    result = metric.compute(predictions=decoded_preds, references=decoded_labels)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;bleu&quot;</span>: result[<span class="string">&quot;score&quot;</span>]&#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure><ul><li>因为decode会自动处理pad_token所以使用<code>np.where</code>将-100都替换成pad_token<ul><li>numpy.where(condition,  x，y) ，x中条件不成立的都会被填充y</li></ul></li></ul><hr><h1 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h1><p>首先看下<code>Seq2SeqTrainer</code> 然后回到自定义的Loop</p><h2 id="Seq2SeqTrainer"><a href="#Seq2SeqTrainer" class="headerlink" title="Seq2SeqTrainer"></a>Seq2SeqTrainer</h2><p>这里不是重点，但是有些细节可圈可点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainingArguments</span><br><span class="line"></span><br><span class="line">args = Seq2SeqTrainingArguments(</span><br><span class="line">    <span class="string">f&quot;marian-finetuned-kde4-en-to-fr&quot;</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;no&quot;</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">32</span>,</span><br><span class="line">    per_device_eval_batch_size=<span class="number">64</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    save_total_limit=<span class="number">3</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    predict_with_generate=<span class="literal">True</span>,</span><br><span class="line">    fp16=<span class="literal">True</span>,</span><br><span class="line">    push_to_hub=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>定义参数</p><ul><li>We don’t set any regular evaluation, as evaluation takes a while; we will just evaluate our model once before training and after.<ul><li>由于我们自定义评价标准，这里我们在模型训练前测一次scores，训练完成后再测一次</li></ul></li><li>We set <code>fp16=True</code>, which speeds up training on modern GPUs.</li><li>We set <code>predict_with_generate=True</code>, as discussed above.<ul><li>the decoder performs inference by predicting tokens one by one — something that’s implemented behind the scenes in 🤗 Transformers by the <code>generate()</code> method. The <code>Seq2SeqTrainer</code> will let us use that method for evaluation if we set <code>predict_with_generate=True</code>.</li></ul></li><li>We use <code>push_to_hub=True</code> to upload the model to the Hub at the end of each epoch</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainer</span><br><span class="line"></span><br><span class="line">trainer = Seq2SeqTrainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.evaluate(max_length=max_length)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;eval_loss&#x27;: 1.6964408159255981,</span></span><br><span class="line"><span class="string"> &#x27;eval_bleu&#x27;: 39.26865061007616,</span></span><br><span class="line"><span class="string"> &#x27;eval_runtime&#x27;: 965.8884,</span></span><br><span class="line"><span class="string"> &#x27;eval_samples_per_second&#x27;: 21.76,</span></span><br><span class="line"><span class="string"> &#x27;eval_steps_per_second&#x27;: 0.341&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line"> trainer.train()</span><br><span class="line"> trainer.evaluate(max_length=max_length)</span><br><span class="line"> <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> &#123;&#x27;eval_loss&#x27;: 0.8558505773544312,</span></span><br><span class="line"><span class="string"> &#x27;eval_bleu&#x27;: 52.94161337775576,</span></span><br><span class="line"><span class="string"> &#x27;eval_runtime&#x27;: 714.2576,</span></span><br><span class="line"><span class="string"> &#x27;eval_samples_per_second&#x27;: 29.426,</span></span><br><span class="line"><span class="string"> &#x27;eval_steps_per_second&#x27;: 0.461,</span></span><br><span class="line"><span class="string"> &#x27;epoch&#x27;: 3.0&#125;&#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure><p><strong>That’s a nearly 14-point improvement, which is great.</strong></p><h2 id="Custom-Training-Loop"><a href="#Custom-Training-Loop" class="headerlink" title="Custom Training Loop"></a>Custom Training Loop</h2><p>接下来就是重点了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=data_collator, <span class="comment"># 就是上面的DataCollatorForSeq2Seq(tokenizer, model=model)</span></span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=<span class="number">8</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上是常规定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line">model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(</span><br><span class="line">    model, optimizer, train_dataloader, eval_dataloader)</span><br><span class="line"></span><br><span class="line">num_train_epochs = <span class="number">3</span></span><br><span class="line">num_update_steps_per_epoch = <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">num_training_steps = num_train_epochs * num_update_steps_per_epoch</span><br><span class="line"></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这两部分都需要注意</p><ul><li>Once we have all those objects, we can send them to the <code>accelerator.prepare()</code> method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn’t execute any cell that instantiates an <code>Accelerator</code><ul><li>不要在没有把所有部件转到TPU前使用<code>Accelerator</code></li></ul></li><li>we can use its length to compute the number of training steps. Remember we should always do this after preparing the dataloader, as that method will change the length of the <code>DataLoader</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">predictions, labels</span>):</span><br><span class="line">    predictions = predictions.cpu().numpy()</span><br><span class="line">    labels = labels.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Replace -100 in the labels as we can&#x27;t decode them.</span></span><br><span class="line">    labels = np.where(labels != -<span class="number">100</span>, labels, tokenizer.pad_token_id)</span><br><span class="line">    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Some simple post-processing</span></span><br><span class="line">    decoded_preds = [pred.strip() <span class="keyword">for</span> pred <span class="keyword">in</span> decoded_preds]</span><br><span class="line">    decoded_labels = [[label.strip()] <span class="keyword">for</span> label <span class="keyword">in</span> decoded_labels]</span><br><span class="line">    <span class="keyword">return</span> decoded_preds, decoded_labels</span><br></pre></td></tr></table></figure><p>正式训练之前，先定义一下后处理函数，输出我们预测的标签给Sacrebleu</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_train_epochs):</span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluation</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(eval_dataloader):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            generated_tokens = accelerator.unwrap_model(model).generate(</span><br><span class="line">                batch[<span class="string">&quot;input_ids&quot;</span>],</span><br><span class="line">                attention_mask=batch[<span class="string">&quot;attention_mask&quot;</span>],</span><br><span class="line">                max_length=<span class="number">128</span>,</span><br><span class="line">            )</span><br><span class="line">        labels = batch[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Necessary to pad predictions and labels for being gathered</span></span><br><span class="line">        generated_tokens = accelerator.pad_across_processes(</span><br><span class="line">            generated_tokens, dim=<span class="number">1</span>, pad_index=tokenizer.pad_token_id</span><br><span class="line">        )</span><br><span class="line">        labels = accelerator.pad_across_processes(labels, dim=<span class="number">1</span>, pad_index=-<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        predictions_gathered = accelerator.gather(generated_tokens)</span><br><span class="line">        labels_gathered = accelerator.gather(labels)</span><br><span class="line"></span><br><span class="line">        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)</span><br><span class="line">        metric.add_batch(predictions=decoded_preds, references=decoded_labels)</span><br><span class="line"></span><br><span class="line">    results = metric.compute()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch <span class="subst">&#123;epoch&#125;</span>, BLEU score: <span class="subst">&#123;results[<span class="string">&#x27;score&#x27;</span>]:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save and upload</span></span><br><span class="line">    <span class="comment"># accelerator.wait_for_everyone()</span></span><br><span class="line"><span class="comment">#     unwrapped_model = accelerator.unwrap_model(model)</span></span><br><span class="line"><span class="comment">#     unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)</span></span><br><span class="line"><span class="comment">#     if accelerator.is_main_process:</span></span><br><span class="line"><span class="comment">#         tokenizer.save_pretrained(output_dir)</span></span><br><span class="line"><span class="comment">#         repo.push_to_hub(</span></span><br><span class="line"><span class="comment">#             commit_message=f&quot;Training in progress epoch &#123;epoch&#125;&quot;, blocking=False</span></span><br><span class="line"><span class="comment">#         )</span></span><br></pre></td></tr></table></figure><hr><p>最后通过pipeline测试下输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace this with your own checkpoint</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;huggingface-course/marian-finetuned-kde4-en-to-fr&quot;</span></span><br><span class="line">translator = pipeline(<span class="string">&quot;translation&quot;</span>, model=model_checkpoint)</span><br><span class="line">translator(<span class="string">&quot;Default to expanded threads&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>首先是pipeline</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">!pip install sacrebleu</span><br><span class="line">!pip install evaluate</span><br><span class="line">!pip install accelerate</span><br><span class="line">!pip install --upgrade transformers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (AdamW, AutoModelForSeq2SeqLM, AutoTokenizer,</span><br><span class="line">                          DataCollatorForSeq2Seq, Seq2SeqTrainer,</span><br><span class="line">                          Seq2SeqTrainingArguments, get_scheduler,pipeline)</span><br><span class="line">                          </span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">logging.disable(logging.WARNING)</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_LAUNCH_BLOCKING&#x27;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">CONFIG = &#123;<span class="string">&quot;seed&quot;</span>: <span class="number">2021</span>,</span><br><span class="line">          <span class="string">&quot;epochs&quot;</span>: <span class="number">3</span>,</span><br><span class="line">          <span class="string">&quot;model_name&quot;</span>: <span class="string">&quot;roberta-base&quot;</span>,</span><br><span class="line">          <span class="string">&quot;train_batch_size&quot;</span>: <span class="number">32</span>,</span><br><span class="line">          <span class="string">&quot;valid_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">          <span class="string">&quot;max_length&quot;</span>: <span class="number">128</span>,</span><br><span class="line">          <span class="string">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">          <span class="string">&quot;scheduler&quot;</span>: <span class="string">&#x27;CosineAnnealingLR&#x27;</span>,</span><br><span class="line">          <span class="string">&quot;min_lr&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">          <span class="string">&quot;T_max&quot;</span>: <span class="number">500</span>,</span><br><span class="line">          <span class="string">&quot;weight_decay&quot;</span>: <span class="number">1e-6</span>,</span><br><span class="line">          <span class="string">&quot;n_fold&quot;</span>: <span class="number">5</span>,</span><br><span class="line">          <span class="string">&quot;n_accumulate&quot;</span>: <span class="number">1</span>,</span><br><span class="line">          <span class="string">&quot;num_classes&quot;</span>: <span class="number">1</span>,</span><br><span class="line">          <span class="string">&quot;margin&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">          <span class="string">&quot;device&quot;</span>: torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>),</span><br><span class="line">          <span class="string">&quot;hash_name&quot;</span>: HASH_NAME</span><br><span class="line">          &#125;</span><br><span class="line">          </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed=<span class="number">42</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Sets the seed of the entire notebook so results are the same every time we run.</span></span><br><span class="line"><span class="string">    This is for REPRODUCIBILITY.&#x27;&#x27;&#x27;</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    <span class="comment"># When running on the CuDNN backend, two further options must be set</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    <span class="comment"># Set a fixed value for the hash seed</span></span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    </span><br><span class="line">set_seed(CONFIG[<span class="string">&#x27;seed&#x27;</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_name = <span class="string">&quot;Helsinki-NLP/opus-mt-zh-en&quot;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">dataset_raw = load_dataset(<span class="string">&#x27;news_commentary&#x27;</span>,<span class="string">&#x27;en-zh&#x27;</span>)</span><br><span class="line">dataset_raw, dataset_raw[<span class="string">&#x27;train&#x27;</span>][<span class="string">&#x27;translation&#x27;</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>huggingface proxy</title>
      <link href="/posts/35234.html"/>
      <url>/posts/35234.html</url>
      
        <content type="html"><![CDATA[<h1 id="huggingface"><a href="#huggingface" class="headerlink" title="huggingface"></a>huggingface</h1><p>在有代理的情况下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">prx = &#123;&#x27;https&#x27;: &#x27;http://127.0.0.1:7890&#x27;&#125;</span><br><span class="line">model_name = &quot;Helsinki-NLP/opus-mt-zh-en&quot;</span><br><span class="line">save_path = r&#x27;D:\00mydataset\huggingface model&#x27;</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name, proxies=prx, cache_dir=save_path)</span><br></pre></td></tr></table></figure><p>直接设置代理就可以接到huggingface了</p><h1 id="vscode"><a href="#vscode" class="headerlink" title="vscode"></a>vscode</h1><p>在setting里面的proxy的第一栏设置<code>http://127.0.0.1:7890</code> 后面的7890就是你的端口号，在你的代理处可以查看</p><h1 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h1><p>pip 我根据<a href="https://www.cnblogs.com/bonheur/p/12306108.html">这篇文章</a>在终端直接设置 <code>set http_proxy=&#39;http://127.0.0.1:7890&#39;</code> 这就好了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 在pip目录创建并编辑pip.ini（配置文件不存在时）</span><br><span class="line">cd C:\Users\(你的用户名)   </span><br><span class="line">mkdir pip                # 创建pip文件夹</span><br><span class="line">cd pip                     # 进入pip路径目录下</span><br><span class="line">cd.&gt;pip.ini              # 创建pip.ini文件</span><br><span class="line"></span><br><span class="line"># 然后打开C:\Users(用户名)\pip\pip.ini，添加如下内容：</span><br><span class="line">[global]</span><br><span class="line">proxy=http://10.20.217.2:8080</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> HuggingFace </tag>
            
            <tag> Proxy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Huggingface Tokenizer详解</title>
      <link href="/posts/36291.html"/>
      <url>/posts/36291.html</url>
      
        <content type="html"><![CDATA[<h1 id="Main-Types-of-Tokenizers"><a href="#Main-Types-of-Tokenizers" class="headerlink" title="Main Types of Tokenizers"></a>Main Types of Tokenizers</h1><ul><li><p><a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#byte-pair-encoding">Byte-Pair Encoding (BPE)</a></p></li><li><p><a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#wordpiece">WordPiece</a></p></li><li><p><a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#sentencepiece">SentencePiece</a></p></li></ul><blockquote><p><a href="https://spacy.io/">spaCy</a> and <a href="http://www.statmt.org/moses/?n=Development.GetStarted">Moses</a> are two popular rule-based tokenizers.</p></blockquote><hr><h1 id="Subword"><a href="#Subword" class="headerlink" title="Subword"></a>Subword</h1><blockquote><p>word-level 太大，character-level包含不了很多语义</p><p>So to get the best of both worlds, transformers models use a hybrid between word-level and character-level tokenization called <strong>subword</strong> tokenization.</p></blockquote><ul><li>Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.<ul><li>将低频词(专有名词那些) 拆解、分词</li></ul></li></ul><p>例如</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">tokenizer.tokenize(<span class="string">&quot;I have a new GPU!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;, &quot;gp&quot;, &quot;##u&quot;, &quot;!&quot;]</span></span><br></pre></td></tr></table></figure><ul><li>更小的词汇表意味着失去更多的词义，在词义和运算资源的平衡引出了下面的几个算法。</li></ul><hr><h1 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte-Pair Encoding (BPE)"></a>Byte-Pair Encoding (BPE)</h1><p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2">GPT-2</a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/roberta">Roberta</a>,  <a href="https://huggingface.co/docs/transformers/main/en/model_doc/xlm">XLM</a>,  <a href="https://huggingface.co/docs/transformers/main/en/model_doc/flaubert">FlauBERT</a> ,<a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt">GPT</a> </p><p>BPE在<a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a>中被提出</p><ul><li>After pre-tokenization, <strong>a set of unique words has been created</strong> and the <strong>frequency</strong> of each word it occurred in the training data has been determined. Next, BPE creates a <strong>base vocabulary</strong> consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. <strong>It does so until the vocabulary has attained the desired vocabulary size.</strong> Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.<ul><li>首先统计词频 —&gt; 再根据词频创建词汇表 , 同时加入融合规则 —&gt; 依词频merge得到新的词汇表 —&gt; 在新的词汇表基础上在此merge —&gt; 做种使得词汇表的大小在 <strong>desired vocabulary size.</strong> 这个大小是可调的超参数</li></ul></li></ul><p>例如, 数字表示频率</p><p><code>(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)</code></p><ol><li><p>首先我们得到 base vocabulary is <code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;]</code>.</p></li><li><p><code>ug</code>的组合有20的频率第一个被加进去</p><ul><li><code>(&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)</code></li></ul></li><li><p>次高<code>un</code> —&gt; <code>hug</code></p><ul><li><code>(&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)</code></li></ul></li></ol><ul><li>另外 For instance, the word <code>&quot;bug&quot;</code> would be tokenized to <code>[&quot;b&quot;, &quot;ug&quot;]</code> but <code>&quot;mug&quot;</code> would be tokenized as <code>[&quot;&lt;unk&gt;&quot;, &quot;ug&quot;]</code> since the symbol <code>&quot;m&quot;</code> is not in the base vocabulary.</li></ul><blockquote><p>the <strong>vocabulary size</strong>, <em>i.e.</em> the <strong>base vocabulary size + the number of merges</strong></p><p>For instance <a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt">GPT</a> has a vocabulary size of 40,478 since they have <strong>478 base characters</strong> and chose to stop training after <strong>40,000 merges.</strong></p><p> <a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt">GPT-2</a> has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.</p></blockquote><hr><h1 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h1><p>Unigram introduced in <a href="https://arxiv.org/pdf/1804.10959.pdf">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)</a>. </p><p>it’s used in conjunction with <a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#sentencepiece">SentencePiece</a>. 跟SentencePiece配合使用</p><ul><li><p>In contrast to BPE or WordPiece, Unigram initializes its <strong>base vocabulary to a large number of symbols</strong> and <strong>progressively trims down</strong> each symbol to obtain a smaller vocabulary.</p></li><li><p>At each training step, the Unigram algorithm <strong>defines a loss</strong> (often defined as the <strong>log-likelihood</strong>) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes <strong>how much the overall loss would increase if the symbol was to be removed from the vocabulary</strong>. Unigram then <strong>removes p (with p usually being 10% or 20%) percent of the symbols</strong> whose loss increase is the lowest, <em>i.e.</em> those symbols that least affect the overall loss over the training data. This process is <strong>repeated until the vocabulary has reached the desired size</strong>. The Unigram algorithm <strong>always keeps the base characters</strong> so that any word can be tokenized.</p><ul><li>取对数似然损失，将所有字母嵌入基础词汇表，开始merge，计算移除某些词所降低或提升的损失，重复不断移除，直到符合理想的词汇表大小。</li></ul></li><li><p>Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of <strong>tokenizing new text after training</strong></p></li></ul><hr><h1 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h1><p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/bert">BERT</a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/distilbert">DistilBERT</a>, and <a href="https://huggingface.co/docs/transformers/main/en/model_doc/electra">Electra</a>. <del>满满的含金量</del></p><p>WordPiece was outlined in <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">Japanese and Korean Voice Search (Schuster et al., 2012)</a></p><ul><li>WordPiece first initializes the <strong>vocabulary</strong> to include <strong>every character</strong> present in the training data and <strong>progressively learns a given number of merge rules</strong>. In contrast to BPE, WordPiece does <strong>not</strong> choose the most frequent symbol pair, <strong>but the one that maximizes the likelihood of the training data once added to the vocabulary.</strong><ul><li>WordPiece使用频率记录character做基础词汇表，然后使用最大似然做评判标准 merge词汇。是BPE和Unigram的结合使用案例。</li></ul></li></ul><hr><h1 id="SentencePiece"><a href="#SentencePiece" class="headerlink" title="SentencePiece"></a>SentencePiece</h1><p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/xlm">XLM</a>,  <a href="https://huggingface.co/docs/transformers/main/en/model_doc/albert">ALBERT</a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/xlnet">XLNet</a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/marian">Marian</a>, and <a href="https://huggingface.co/docs/transformers/main/en/model_doc/t5">T5</a>.</p><p><a href="https://arxiv.org/pdf/1808.06226.pdf">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)</a> treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.</p><ul><li>All transformers models in the library that use SentencePiece use it in combination with unigram<ul><li>就是使用Unicode码编码所有字符</li></ul></li></ul><hr><h1 id="Customizing-Tokenizer"><a href="#Customizing-Tokenizer" class="headerlink" title="Customizing Tokenizer"></a>Customizing Tokenizer</h1><p>如同上面看到的不同模型选择了不同算法的分词方式，根据你的需求选择不同的分词器。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> models, Tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然这里可以直接 使用模型名字得到一个对应模型的分词器，那就是通常使用的方法</span></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece()) <span class="comment"># models.BPE() models.Unigram()</span></span><br></pre></td></tr></table></figure><p>以上即可引出你的分词器进行下面的客制化。</p><ul><li><code>Normalization</code>: Executes all the initial transformations over the initial input string. For example when you need to <strong>lowercase some text</strong>, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.<ul><li>对你的文本进行修缮，转小写字母，删除某些符号等</li></ul></li><li><code>Pre-tokenization</code>: In charge of splitting the initial input string. That’s the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.<ul><li>定制一些分词规则</li></ul></li><li><code>Model</code>: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.<ul><li>循环处理</li></ul></li><li><code>Post-Processing</code>: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.</li></ul><hr><ul><li>首先设定一下数据集和数据发生器</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1000</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_iterator</span>():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size): <span class="comment"># 0到数据集的长度, 步长为batch_size</span></span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + batch_size][<span class="string">&quot;text&quot;</span>]</span><br></pre></td></tr></table></figure><ol><li>WordPiece</li></ol><p><strong>pre-processing</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unl_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line"></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设定bert的前处理分词器</span></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;This&#x27;, (0, 4)),</span></span><br><span class="line"><span class="string"> (&#x27;is&#x27;, (5, 7)),</span></span><br><span class="line"><span class="string"> (&#x27;an&#x27;, (8, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;example&#x27;, (11, 18)),</span></span><br><span class="line"><span class="string"> (&#x27;!&#x27;, (18, 19))]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>Note that the pre-tokenizer not only split the text into words but keeps the <strong>offsets</strong><ul><li>that is the <strong>beginning and start of each of those words inside the original text.</strong>  这里是为QA做的偏移量特性</li></ul></li></ul><p><strong>processing</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line"><span class="comment"># 引入 trainer不是编辑好了，而是借助已经完成的trainer结构加入special_tokens</span></span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是开始训练了</span></span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br></pre></td></tr></table></figure><ul><li>这里我们就得到了基本处理的数据，下面进行后处理。</li></ul><p><strong>post-processing</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[</span><br><span class="line">        (<span class="string">&quot;[CLS]&quot;</span>, cls_token_id),</span><br><span class="line">        (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id),</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看数据</span></span><br><span class="line">encoder = tokenizer.encode(<span class="string">&quot;This is one sentence.&quot;</span>, <span class="string">&quot;With this one we have a pair.&quot;</span>)</span><br><span class="line">encoding.tokens</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;this&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;is&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;one&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;sentence&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;.&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;[SEP]&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;with&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;this&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;one&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;we&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;have&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;a&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;pair&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;.&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;[SEP]&#x27;]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line"> encoding.type_ids</span><br><span class="line"> <span class="comment"># [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>We have to indicate in the template how to organize the special tokens with one sentence (<code>$A</code>) or two sentences (<code>$A</code> and <code>$B</code>). The <code>:</code> followed by a number indicates the token type ID to give to each part.<ul><li>process阶段处理好的句子进行包装 单个句子single就是00，pair就是01</li></ul></li></ul><p><strong>wrap your tokenizer to transformer object</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">new_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)</span><br><span class="line"><span class="comment"># 原示例是 new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)</span></span><br></pre></td></tr></table></figure><ol start="2"><li>BPE</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.BPE())</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line"></span><br><span class="line">trainer = trainers.BpeTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>])</span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br><span class="line"></span><br><span class="line">tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="literal">False</span>)</span><br><span class="line">tokenizer.decoder = decoders.ByteLevel()</span><br><span class="line">encode2= tokenizer.encode(<span class="string">&quot;This is one sentence.&quot;</span>, <span class="string">&quot;With this one we have a pair.&quot;</span>)</span><br><span class="line">encode2</span><br><span class="line"><span class="comment"># Encoding(num_tokens=13, </span></span><br><span class="line">attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样进行封装</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line">new_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure><ul><li>与之前不同的是我们在第三行直接使用<code>pre_tokenizers.ByteLevel</code>的分词器，而不是调用bert的分词器</li><li>同样<code>trainers.BpeTrainer</code>我们使用的也是bpe的</li><li><code>trim_offsets=False</code> <em>Whether the post processing step should trim offsets to avoid including whitespaces.</em><ul><li>是否将空白格 计算在偏移量里</li></ul></li></ul><ol start="3"><li>Unigram</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.Unigram())</span><br><span class="line"></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.Replace(<span class="string">&quot;``&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>), normalizers.Replace(<span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>), normalizers.Lowercase()]</span><br><span class="line">)</span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;This is an example!&quot;</span>)</span><br><span class="line"><span class="comment"># [(&#x27;▁This&#x27;, (0, 4)), (&#x27;▁is&#x27;, (4, 7)), (&#x27;▁an&#x27;, (7, 10)), (&#x27;▁example!&#x27;, (10, 19))]</span></span><br><span class="line"></span><br><span class="line">trainer = trainers.UnigramTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>], unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)</span><br><span class="line"></span><br><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[</span><br><span class="line">        (<span class="string">&quot;[CLS]&quot;</span>, cls_token_id),</span><br><span class="line">        (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id),</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">tokenizer.decoder = decoders.Metaspace()</span><br><span class="line"></span><br><span class="line">tokenizer.encode(<span class="string">&quot;This is one sentence.&quot;</span>, <span class="string">&quot;With this one we have a pair.&quot;</span>).tokens</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁this&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁is&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁one&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁sentence&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;.&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;[SEP]&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁with&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁this&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁one&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁we&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁have&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁a&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;▁pair&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;.&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;[SEP]&#x27;]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AlbertTokenizerFast</span><br><span class="line">new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure><ul><li><code>pre_tokenizers.Metaspace()</code></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>从one-hot-ket到subword 在subword的基础上</li><li>优化vocab-size得到BPE和Unigram</li><li>综合两者有点得到WordPiece 和特攻不方便词中日韩语的SentencePiece</li><li>最后就是api上的一些细节了</li></ul><blockquote><p>参考</p><p><a href="https://huggingface.co/docs/tokenizers/main/en/api/models#tokenizers.models.BPE">BPE model</a>、<a href="https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb">notebook</a>、<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb#scrollTo=PjgSkSlYviy8">COLAB</a>、<a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#bytepair-encoding-bpe">Conceptual Guide</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HuggingFace </tag>
            
            <tag> Preprocessing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas 工作流</title>
      <link href="/posts/38639.html"/>
      <url>/posts/38639.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>示例引入</li></ul><h1 id="map-applymap-apply"><a href="#map-applymap-apply" class="headerlink" title="map applymap apply"></a>map applymap apply</h1><ul><li>map apply都是对series操作<ul><li>map只能操作 len count 基础函数</li><li>apply 能传入自定义函数 以axis&#x3D;1或者0分别按一个series操作</li></ul></li><li>applymap 对DataFrame的地图炮<ul><li>applymap对所有元素统一操作</li></ul></li></ul><h1 id="str-apply"><a href="#str-apply" class="headerlink" title="str + apply"></a>str + apply</h1><ul><li>pandas对象带的对字符串函数</li><li>apply自定义处理字符串函数</li></ul><h1 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h1><ul><li>sort_values<ul><li>by 哪一列</li><li>key + lambda 对值处理后的排序<ul><li>Apply the key function to the values before sorting. This is similar to the key argument in the builtin <code>sorted()</code> function, with the notable difference that this key function should be <em>vectorized</em>. It should expect a <code>Series</code> and return a Series with the same shape as the input. It will be applied to each column in by independently.</li></ul></li></ul></li></ul><h1 id="groupby-apply"><a href="#groupby-apply" class="headerlink" title="groupby + apply"></a>groupby + apply</h1><ul><li>groupby分组，返回一个group对象 配合apply使用</li><li>transform 分组处理返回值，并保持表格不变</li></ul><h1 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h1><ul><li>merge</li><li>join <ul><li>how&#x3D; inner left right outer</li></ul></li><li>cat</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer From Scratch</title>
      <link href="/posts/13310.html"/>
      <url>/posts/13310.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>增加decoder inference模块</li><li>前言</li></ul><p>架构分四个大块</p><ul><li>encoder <ul><li>encoder-layer</li></ul></li><li>decoder<ul><li>decoder-layer</li></ul></li></ul><p>细节三种mask</p><ul><li>encoder-mask</li><li>decoder-mask</li><li>cross-mask</li></ul><h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><p>句子表示为 [token1， token2, …tokens]</p><p>句子1 &#x3D; [token_1， token_2, …token_x]</p><p>句子2 &#x3D; [token_1， token_2, …token_y]   x 不一定等于 y</p><h2 id="token-构造"><a href="#token-构造" class="headerlink" title="token 构造"></a>token 构造</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">src_vocab_size = <span class="number">16</span></span><br><span class="line">tgt_vocab_size = <span class="number">16</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">max_len = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">src_len = torch.randint(<span class="number">2</span>,<span class="number">7</span>, (batch_size,))</span><br><span class="line">tgt_len = torch.randint(<span class="number">2</span>,<span class="number">7</span>, (batch_size,))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">结果如下</span></span><br><span class="line"><span class="string">tensor([8, 6, 5, 9]) 此batch的第一个句子长8，第二个6</span></span><br><span class="line"><span class="string">tensor([6, 5, 9, 5])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来我们把不定长的句子padding</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">randint 生成(i,)形状的数据</span></span><br><span class="line"><span class="string">padding 0次或者 max_len - len(i) 的次数</span></span><br><span class="line"><span class="string">unsqueeze 增加一个维度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">src_seq = [F.pad(torch.randint(<span class="number">1</span>, src_vocab_size, (i,)), (<span class="number">0</span>, max_len-i)).unsqueeze(<span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> src_len]</span><br><span class="line">tgt_seq = [F.pad(torch.randint(<span class="number">1</span>, tgt_vocab_size, (i,)), (<span class="number">0</span>, max_len-i)).unsqueeze(<span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> tgt_len]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将整个batch的句子整合</span></span><br><span class="line">src_seq = torch.cat(src_seq)</span><br><span class="line">tgt_seq = torch.cat(tgt_seq)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">如下</span></span><br><span class="line"><span class="string">tensor([[12, 15, 10,  5,  3, 14],</span></span><br><span class="line"><span class="string">        [ 5,  7,  9,  3, 12,  1],</span></span><br><span class="line"><span class="string">        [ 3,  1,  1,  9,  3,  4],</span></span><br><span class="line"><span class="string">        [ 9,  6,  0,  0,  0,  0]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[11, 12, 11,  3,  5, 15],</span></span><br><span class="line"><span class="string">        [ 7,  9, 11,  0,  0,  0],</span></span><br><span class="line"><span class="string">        [12,  6, 13, 11,  0,  0],</span></span><br><span class="line"><span class="string">        [13,  3,  0,  0,  0,  0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="词向量空间-映射"><a href="#词向量空间-映射" class="headerlink" title="词向量空间 映射"></a>词向量空间 映射</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">d_model = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">src_embedding = nn.Embedding(src_vocab_size+<span class="number">1</span>, d_model)</span><br><span class="line">tgt_embedding = nn.Embedding(tgt_vocab_size+<span class="number">1</span>, d_model)</span><br><span class="line"></span><br><span class="line">src_embedding.weight <span class="comment"># shape为(17, 8)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第零行为pad的数据</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[ 2.4606,  1.7139, -0.2859, -0.5058,  0.6229, -0.0470,  2.1517,  0.2996],</span></span><br><span class="line"><span class="string">        [ 0.0077, -0.4292, -0.2397,  1.2366, -0.3061,  0.9196, -1.4222, -1.6431],</span></span><br><span class="line"><span class="string">        [-0.6378, -0.7809, -0.4206,  0.5759, -1.4899,  1.2241,  0.9220, -0.6333],</span></span><br><span class="line"><span class="string">        [ 0.0303, -1.4113,  0.9164, -0.1200,  1.7224, -0.4996, -1.6708, -1.8563],</span></span><br><span class="line"><span class="string">        [ 0.0235,  0.0155, -0.1292, -0.9274, -1.1351, -0.9155,  0.4391, -0.0437],</span></span><br><span class="line"><span class="string">        [ 0.8498,  0.4709, -0.9168, -2.1307,  0.1840,  0.3554, -0.3986,  1.2806],</span></span><br><span class="line"><span class="string">        [ 0.7256,  1.2303, -0.8280, -0.2173,  0.8939,  2.4122,  0.4820, -1.9615],</span></span><br><span class="line"><span class="string">        [-0.8607,  2.4886, -0.8877, -0.8852,  0.3905,  0.9511, -0.3732,  0.4872],</span></span><br><span class="line"><span class="string">        [ 0.4882, -0.4518, -0.1945,  0.2857, -0.6832, -0.4870, -1.7165, -2.0987],</span></span><br><span class="line"><span class="string">        [-0.0512,  0.2692, -1.0003,  0.7896,  0.5004,  0.3594, -1.5923, -1.5618],</span></span><br><span class="line"><span class="string">        [ 0.4012,  0.1614,  1.8939,  0.3862, -0.6733, -1.2442, -0.6540, -1.6772],</span></span><br><span class="line"><span class="string">        [ 1.4784,  2.7430,  0.0159,  0.5944, -1.0025,  1.0843,  0.4580, -0.6515],</span></span><br><span class="line"><span class="string">        [ 0.3905,  0.6118, -0.1256, -0.6725,  1.2366,  0.8272,  0.0838, -1.5124],</span></span><br><span class="line"><span class="string">        [-0.1470,  0.2149, -1.4561,  1.8008,  0.7764, -0.8517, -0.3204, -0.2550],</span></span><br><span class="line"><span class="string">        [-1.1534, -0.6837, -1.7165, -1.7905, -1.5423,  1.8812, -0.1794, -0.2357],</span></span><br><span class="line"><span class="string">        [ 1.3046,  1.5021,  1.4846,  1.0622,  1.4066,  0.7299,  0.7929, -1.0107],</span></span><br><span class="line"><span class="string">        [-0.3920,  0.7482,  1.5976,  1.7429, -0.4683,  0.2286,  0.1320, -0.5826]],</span></span><br><span class="line"><span class="string">       requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">scr_embedding(scr_seq[<span class="number">0</span>]) <span class="comment"># 将取出对应的[12, 15, 10,  5,  3, 14]行</span></span><br></pre></td></tr></table></figure><h2 id="Key-mask"><a href="#Key-mask" class="headerlink" title="Key_mask"></a>Key_mask</h2><p>由于在encoder_layer pad的位置经过softmax 也会分得或多或少注意力分数，这些pad不是我们希望模型从数据中学到的，所以这里我们引入<strong>Key_mask</strong> 帮助encoder更好的关注在需要关注的位置上。 <strong>也是特别重要的一个细节</strong>。</p><ul><li>前置</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">max_len = <span class="number">6</span></span><br><span class="line">embed_dim = <span class="number">8</span></span><br><span class="line">vacab_max = <span class="number">5</span></span><br><span class="line">token_ids = [torch.randint(<span class="number">1</span>,<span class="number">6</span>, (<span class="built_in">len</span>,)) <span class="keyword">for</span> <span class="built_in">len</span> <span class="keyword">in</span> torch.randint(<span class="number">1</span>,<span class="number">7</span>, (max_len,))]</span><br><span class="line">token_ids</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[tensor([2, 3, 5, 2, 4]),</span></span><br><span class="line"><span class="string"> tensor([3, 3, 4, 4, 5, 4]),</span></span><br><span class="line"><span class="string"> tensor([4, 5, 3]),</span></span><br><span class="line"><span class="string"> tensor([5, 1, 4]),</span></span><br><span class="line"><span class="string"> tensor([2, 1, 5, 3]),</span></span><br><span class="line"><span class="string"> tensor([1, 3, 3, 1])]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>pad</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">token_pad_ids = [F.pad(x, (<span class="number">0</span>, max_len-x.shape[<span class="number">0</span>])).unsqueeze(<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> token_ids]</span><br><span class="line">token_pad_ids = torch.cat(token_pad_ids)</span><br><span class="line">token_pad_ids</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2, 3, 5, 2, 4, 0],</span></span><br><span class="line"><span class="string">        [3, 3, 4, 4, 5, 4],</span></span><br><span class="line"><span class="string">        [4, 5, 3, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [5, 1, 4, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [2, 1, 5, 3, 0, 0],</span></span><br><span class="line"><span class="string">        [1, 3, 3, 1, 0, 0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>取得embedding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">src_embedding = nn.Embedding(vacab_max+<span class="number">1</span>, embed_dim)</span><br><span class="line">tgt_embedding = nn.Embedding(vacab_max+<span class="number">1</span>, embed_dim)</span><br><span class="line">src_embedding.weight, tgt_embedding.weight</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(Parameter containing:</span></span><br><span class="line"><span class="string"> tensor([[-1.4019, -0.3245,  0.8569, -1.6555,  1.3478,  0.0979, -1.7458,  1.3138],</span></span><br><span class="line"><span class="string">         [-0.9099, -0.6957,  0.4430,  0.6305,  0.1099,  0.3213,  0.0841,  0.0786],</span></span><br><span class="line"><span class="string">         [-0.1215, -1.4141,  0.8802, -0.3444,  0.3444, -1.4063, -0.5057,  0.1506],</span></span><br><span class="line"><span class="string">         [ 0.9491,  1.7888,  0.3075, -0.6642,  0.3368,  0.3388, -1.2543, -0.8096],</span></span><br><span class="line"><span class="string">         [ 0.7723, -1.2258, -0.4963,  1.4007, -0.8048, -0.1338,  0.0199,  0.4295],</span></span><br><span class="line"><span class="string">         [ 1.3789, -0.9537,  0.3421,  0.0658, -0.7578, -0.7217, -1.3124,  1.6017]],</span></span><br><span class="line"><span class="string">        requires_grad=True),</span></span><br><span class="line"><span class="string"> Parameter containing:</span></span><br><span class="line"><span class="string"> tensor([[ 2.0609,  0.7302,  0.9811,  0.7390,  0.7475,  0.2903,  0.0735,  0.3407],</span></span><br><span class="line"><span class="string">         [ 1.5477, -0.5033,  1.3758, -1.5225,  0.8236,  0.6329, -0.2301,  1.2352],</span></span><br><span class="line"><span class="string">         [-0.2906, -1.8842, -0.9998,  1.6752,  0.7286, -0.4089, -0.0515,  0.5763],</span></span><br><span class="line"><span class="string">         [ 0.2128,  0.7354, -0.4248,  0.7142,  0.4635,  1.1675,  0.7193,  1.3474],</span></span><br><span class="line"><span class="string">         [ 0.3543,  1.2881, -0.8270,  0.6220, -1.6282,  0.1802, -0.9306, -0.2407],</span></span><br><span class="line"><span class="string">         [-1.3339, -0.4192, -0.0800,  0.1614,  0.7026, -0.6851,  0.2386, -0.4954]],</span></span><br><span class="line"><span class="string">        requires_grad=True))</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>查看True False ，这里我们去src_ids 的第四个，因为零比较多，(embedding的零行是pad的词向量)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pad = src_embedding.weight[<span class="number">0</span>]</span><br><span class="line">src_embedding(token_pad_ids[<span class="number">3</span>]) == pad, token_pad_ids[<span class="number">3</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([[False, False, False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True,  True,  True]]),</span></span><br><span class="line"><span class="string"> tensor([5, 1, 4, 0, 0, 0]))</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><strong>由于encoder的输入都是src 所以Q*K.T 的维度为（bs, src_len, src_len)</strong>,  mask就直接可以写了</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = token_pad_ids[<span class="number">3</span>].unsqueeze(-<span class="number">1</span>)</span><br><span class="line">b = token_pad_ids[<span class="number">3</span>].unsqueeze(<span class="number">0</span>)</span><br><span class="line">torch.matmul(a,b), a.shape, b.shape</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(tensor([[25,  5, 20,  0,  0,  0],</span></span><br><span class="line"><span class="string">         [ 5,  1,  4,  0,  0,  0],</span></span><br><span class="line"><span class="string">         [20,  4, 16,  0,  0,  0],</span></span><br><span class="line"><span class="string">         [ 0,  0,  0,  0,  0,  0],</span></span><br><span class="line"><span class="string">         [ 0,  0,  0,  0,  0,  0],</span></span><br><span class="line"><span class="string">         [ 0,  0,  0,  0,  0,  0]]),</span></span><br><span class="line"><span class="string"> torch.Size([6, 1]),</span></span><br><span class="line"><span class="string"> torch.Size([1, 6]))</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>一批量查看mask</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">mask = torch.matmul(token_pad_ids.unsqueeze(-<span class="number">1</span>),token_pad_ids.unsqueeze(<span class="number">1</span>)) ==<span class="number">0</span></span><br><span class="line">mask</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">只看前三个</span></span><br><span class="line"><span class="string">tensor([[[False, False, False, False, False,  True],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False,  True],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False,  True],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False,  True],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False,  True],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False],</span></span><br><span class="line"><span class="string">         [False, False, False, False, False, False]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[False, False, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [False, False, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [False, False, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True],</span></span><br><span class="line"><span class="string">         [ True,  True,  True,  True,  True,  True]], 。。。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>上刺刀</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scores = torch.randn(<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">mask = torch.matmul(token_pad_ids.unsqueeze(-<span class="number">1</span>),token_pad_ids.unsqueeze(<span class="number">1</span>))</span><br><span class="line">scores= scores.masked_fill(mask==<span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">scores.softmax(-<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">依旧只看前三个</span></span><br><span class="line"><span class="string">tensor([[[0.0356, 0.0985, 0.6987, 0.0902, 0.0770, 0.0000],</span></span><br><span class="line"><span class="string">         [0.4661, 0.0397, 0.3546, 0.0931, 0.0464, 0.0000],</span></span><br><span class="line"><span class="string">         [0.1917, 0.0149, 0.1564, 0.4113, 0.2259, 0.0000],</span></span><br><span class="line"><span class="string">         [0.4269, 0.0352, 0.1605, 0.1334, 0.2441, 0.0000],</span></span><br><span class="line"><span class="string">         [0.0515, 0.4421, 0.0705, 0.2934, 0.1426, 0.0000],</span></span><br><span class="line"><span class="string">         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.0803, 0.0330, 0.3310, 0.0243, 0.3612, 0.1701],</span></span><br><span class="line"><span class="string">         [0.2160, 0.1483, 0.0312, 0.1804, 0.3861, 0.0380],</span></span><br><span class="line"><span class="string">         [0.2151, 0.0807, 0.1072, 0.4335, 0.1200, 0.0435],</span></span><br><span class="line"><span class="string">         [0.0285, 0.2684, 0.1558, 0.2210, 0.1880, 0.1383],</span></span><br><span class="line"><span class="string">         [0.0889, 0.4485, 0.1067, 0.1028, 0.1901, 0.0630],</span></span><br><span class="line"><span class="string">         [0.2885, 0.1682, 0.0935, 0.0179, 0.0289, 0.4031]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.2862, 0.3934, 0.3204, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">         [0.2426, 0.2206, 0.5369, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">         [0.1487, 0.2483, 0.6030, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],</span></span><br><span class="line"><span class="string">         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],</span></span><br><span class="line"><span class="string">         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],。。。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>同时我们可以看见 全都是pad自身的注意力分数是平均的，也就是跟乱猜一样，没有意义。</p><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>按公式写就行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pe = torch.zeros(max_len, d_model)</span><br><span class="line">pos = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># 形状为(max_len, 1)</span></span><br><span class="line">idx = torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(<span class="number">0</span>, <span class="number">8</span>, <span class="number">2</span>).unsqueeze(<span class="number">0</span>)/ d_model )  <span class="comment"># 形状为 (1, 4)</span></span><br><span class="line">pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos / idx)<span class="comment"># 触发广播 (max_len, 4)</span></span><br><span class="line">pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.sin(pos / idx)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">此处演示只加奇数列的效果</span></span><br><span class="line"><span class="string">tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span></span><br><span class="line"><span class="string">        [ 0.8415,  0.0000,  0.0998,  0.0000,  0.0100,  0.0000,  0.0010,  0.0000],</span></span><br><span class="line"><span class="string">        [ 0.9093,  0.0000,  0.1987,  0.0000,  0.0200,  0.0000,  0.0020,  0.0000],</span></span><br><span class="line"><span class="string">        [ 0.1411,  0.0000,  0.2955,  0.0000,  0.0300,  0.0000,  0.0030,  0.0000],</span></span><br><span class="line"><span class="string">        [-0.7568,  0.0000,  0.3894,  0.0000,  0.0400,  0.0000,  0.0040,  0.0000],</span></span><br><span class="line"><span class="string">        [-0.9589,  0.0000,  0.4794,  0.0000,  0.0500,  0.0000,  0.0050,  0.0000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="emb-pe"><a href="#emb-pe" class="headerlink" title="emb+pe"></a>emb+pe</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab, d_model </span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.emb = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.emb(x) * math.sqrt(self.d_model) <span class="comment"># 对embedding的值缩放 制position数值的影响</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_len, d_model , dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        </span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 所有行，列起始位为1，步长为2</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)    <span class="comment"># 从(batch_size, seq_len) --&gt; (1.)</span></span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)  <span class="comment"># 设置缓冲区，表示参数不更新</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randint(<span class="number">1</span>,<span class="number">40</span>, (<span class="number">2</span>, <span class="number">28</span>))</span><br><span class="line">    emb = Embeddings(<span class="number">40</span>, <span class="number">784</span>)</span><br><span class="line">    pe = PositionalEncoding(<span class="number">40</span>, <span class="number">784</span>)</span><br><span class="line">    <span class="built_in">print</span>((pe(emb(x))).shape)</span><br></pre></td></tr></table></figure><h1 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h1><h2 id="Query-mask-amp-Scaled-Attention"><a href="#Query-mask-amp-Scaled-Attention" class="headerlink" title="Query_mask &amp; Scaled_Attention"></a>Query_mask &amp; Scaled_Attention</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ScaledAttention</span>(<span class="params">query, key, value, d_k, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)  <span class="comment"># (batch_size, channel, head, d_k)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    scores = scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = dropout(scores)</span><br><span class="line"></span><br><span class="line">    value = torch.matmul(scores, value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> value, scores</span><br></pre></td></tr></table></figure><ul><li>第一次matmul： 维度变化为 <a href="mailto:&#81;&#64;&#75;&#x2e;&#x54;">&#81;&#64;&#75;&#x2e;&#x54;</a> —&gt; (batch_size * n_head, tgt_len, src_len)   <ul><li>中间的<code>scores.masked_fill(mask, -1e9)</code> 根据不同的mask做掩码填充</li></ul></li><li>第二次matmul： 维度变化为 scores@V —&gt; (batch_size, tgt_len, embed_dim) 且有注意力分数分配权重<ul><li>这样就是decoder中的目标序列长度</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> clones</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiheadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_head, QKV_O_linear = <span class="number">4</span>, drop_rate=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % n_head == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_head = n_head</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=drop_rate)</span><br><span class="line">        self.linear = clones(nn.Linear(d_model, d_model) ,QKV_O_linear)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size, _, emb_dim = Q.shape    <span class="comment"># (batch_size, seq_len, emb_dim)</span></span><br><span class="line">        d_k = emb_dim // self.n_head</span><br><span class="line"></span><br><span class="line">        Q_heads = self.linear[<span class="number">0</span>](Q).view(batch_size, self.n_head, -<span class="number">1</span>, d_k).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        K_heads = self.linear[<span class="number">1</span>](K).view(batch_size, self.n_head, -<span class="number">1</span>, d_k).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        V_heads = self.linear[<span class="number">2</span>](V).view(batch_size, self.n_head, -<span class="number">1</span>, d_k).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        V_att, scores_att= ScaledAttention(Q_heads, K_heads, V_heads, d_k,  mask, self.dropout)</span><br><span class="line">        V_att = V_att.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.n_head * d_k)</span><br><span class="line">        V_att = self.linear[<span class="number">3</span>](V_att)</span><br><span class="line">        K_lin = K_heads.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.n_head * d_k)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  K_lin, V_att, <span class="comment"># scores_att</span></span><br></pre></td></tr></table></figure><h2 id="关于contiguous"><a href="#关于contiguous" class="headerlink" title="关于contiguous"></a>关于contiguous</h2><ul><li><p>clones之后的linear列表有4个layer</p><ul><li><p>zip函数 对不通长度的对象直接以最小长度进行截断，所以return那里可以用linear列表的最后一个返回输出</p></li><li><p>&#96;&#96;&#96;python<br>a &#x3D; list(range(6))  # 0 - 5 六个数<br>b &#x3D; list(‘asdfg’)  # 5个字母<br>list(zip(a,b))</p><p>‘’’<br>[(0, ‘a’), (1, ‘s’), (2, ‘d’), (3, ‘f’), (4, ‘g’)]<br>‘’’</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">+ contiguous()可以开辟新的内存存储数据，is_contiguous()可以判断数据的底层内存是否连续存取，这里配合view使用</span><br><span class="line"></span><br><span class="line">  + ```python</span><br><span class="line">    t = torch.arange(12).reshape(3,4)</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    tensor([[ 0,  1,  2,  3],</span><br><span class="line">            [ 4,  5,  6,  7],</span><br><span class="line">            [ 8,  9, 10, 11]])&#x27;&#x27;&#x27;</span><br><span class="line">    </span><br><span class="line">    t.stride()</span><br><span class="line">    # (4, 1)</span><br><span class="line">    t2 = t.transpose(0,1)</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    tensor([[ 0,  4,  8],</span><br><span class="line">            [ 1,  5,  9],</span><br><span class="line">            [ 2,  6, 10],</span><br><span class="line">            [ 3,  7, 11]])&#x27;&#x27;&#x27;</span><br><span class="line">    </span><br><span class="line">    t2.stride()</span><br><span class="line">    # (1, 4)</span><br><span class="line">    t.data_ptr() == t2.data_ptr() # 底层数据是同一个一维数组</span><br><span class="line">    # True</span><br><span class="line">    t.is_contiguous(),t2.is_contiguous() # t连续，t2不连续</span><br><span class="line">    # (True, False)</span><br></pre></td></tr></table></figure><p>stride（d1，d2）表示这个维度上的单位元素之间的内存距离。如原本0-11是连续的，他们在【0】维上的距离是4。</p></li></ul></li><li><p>view(b,  -1,  self.h * self.d_k) 要求其对象在内存上是连续的</p><ul><li>即上面的数组<code>[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]</code>如此在内存上排列</li><li>即使view不报错，直接在t2上做view返回的也不会是<code>[ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11]</code></li></ul></li></ul><h1 id="ADD-amp-Norm"><a href="#ADD-amp-Norm" class="headerlink" title="ADD &amp; Norm"></a>ADD &amp; Norm</h1><ul><li>展示一下实现方式，具体只调用Pytorch的接口</li></ul><p>layer_norm 做层级别的归一化，如果说Batch_norm是在 <code>[B, C, H, W]</code>的channel上得到RGB的均值方差，做归一化；</p><p>layer_norm就是在 <code>[B, C, H, W]</code>的batch上得到归一化，比如有3个批次，就分别得到【0】【1】【2】的均值方差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个layernorm模块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;Norm&quot;</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Add+Norm&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;add norm&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><h1 id="Encoder-Layer"><a href="#Encoder-Layer" class="headerlink" title="Encoder Layer"></a>Encoder Layer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> multihead_attention <span class="keyword">import</span> MultiheadAttention </span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> clones</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">encoder_layer的流程为qkv送入Multihead_Attention</span></span><br><span class="line"><span class="string">将得出的kv进行add_norm</span></span><br><span class="line"><span class="string">最终输送给decoder</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_head, drop_rate=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mul = MultiheadAttention(d_model, n_head, drop_rate=drop_rate)</span><br><span class="line">        self.norm = nn.LayerNorm(d_model)</span><br><span class="line">        self.ff = nn.Sequential( nn.Linear(d_model, d_model*<span class="number">4</span>),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Linear(d_model*<span class="number">4</span>, d_model)) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, MASK</span>):</span><br><span class="line">        K, V = self.mul(Q, K, V, MASK)</span><br><span class="line">        V_1 = self.norm(V + Q)</span><br><span class="line">        K_1 = self.norm(K + Q)</span><br><span class="line"></span><br><span class="line">        V_2= self.ff(V_1)</span><br><span class="line">        K_2= self.ff(K_1)</span><br><span class="line">        V_2 = self.norm(V_1 + V_2)</span><br><span class="line">        K_2 = self.norm(K_1 + K_2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> K_2, V_2</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">encoder主要是实现多个layer的堆叠传输</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, N</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = clones(model, N)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q,k,v, mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            K, V = layer(q,k,v,mask)</span><br><span class="line">        <span class="keyword">return</span> K, V</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    source = torch.randn(<span class="number">4</span>, <span class="number">2</span>, <span class="number">28</span>)</span><br><span class="line">    ecd_layer = EncoderLayer(<span class="number">784</span>, <span class="number">7</span>)</span><br><span class="line">    ecd = Encoder(ecd_layer, <span class="number">2</span>)</span><br><span class="line">    ecd(source)</span><br></pre></td></tr></table></figure><h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> multihead_attention <span class="keyword">import</span> MultiheadAttention, clones</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_head, drop_rate=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mmul = MultiheadAttention(d_model, n_head, drop_rate=drop_rate)</span><br><span class="line">        self.norm = nn.LayerNorm(d_model)</span><br><span class="line">        self.ff = nn.Sequential( nn.Linear(d_model, d_model*<span class="number">4</span>),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Linear(d_model*<span class="number">4</span>, d_model)) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, dec_mask, cro_mask</span>):</span><br><span class="line"></span><br><span class="line">        _, V = self.mmul(q, q, q, mask=dec_mask)</span><br><span class="line">        Q_1 = self.norm(V + q)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        这里写V_2_是因为还要传入下一层,</span></span><br><span class="line"><span class="string">        上面写Q是因为 decoder在上一步只提供Q与下一层的多头层计算</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        _, V_2_ = self.mmul(Q_1, k, v, mask=cro_mask)</span><br><span class="line">        V_2 = self.norm(V_2_ + Q_1)</span><br><span class="line"></span><br><span class="line">        V_2= self.ff(V_2)</span><br><span class="line">        V= self.norm(V_2 + V_2_) <span class="comment"># 至此decoder的一轮计算完成</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> V</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, N</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = clones(model, N)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, k, v, dec_mask, cro_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            V = layer(x, k, v, dec_mask, cro_mask)</span><br><span class="line">        <span class="keyword">return</span> V</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randn(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    k = torch.randn(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    v = torch.randn(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    dcd_layer = DecoderLayer(<span class="number">784</span>, <span class="number">7</span>)</span><br><span class="line">    dcd = Decoder(dcd_layer, <span class="number">2</span>)</span><br><span class="line">    dcd(x, k, v)</span><br></pre></td></tr></table></figure><h1 id="mask"><a href="#mask" class="headerlink" title="mask"></a>mask</h1><p>目的是想模仿真实场景中，每次翻译下个词只能在前面产生答案的基础上，所以不会一次产生所有loss</p><p>使用<code>torch.functional.cross_entropy</code>的<code>ignore_index</code>参数将想要mask的位置填为-100即可将loss mask掉</p><blockquote><p>使用ignore_index参数 要配合 reduction &#x3D; ‘none’ 参数</p><p>返回所有损失，而不是平均或者加和后的损失</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_mask</span>(<span class="params">src_ids=<span class="literal">None</span>, tgt_ids=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># encoder的掩码 正方形</span></span><br><span class="line">    <span class="keyword">if</span> tgt_ids == <span class="literal">None</span>:</span><br><span class="line">        mask = torch.matmul(src_ids.unsqueeze(-<span class="number">1</span>), src_ids.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoder第一层的掩码，返回的是一个上三角为1的矩阵(对角线上的没有操作为1)\</span></span><br><span class="line">    <span class="comment"># 因为统一是mask==0 进行填充所以这里反一下 返回的地方让 上三角=false，\</span></span><br><span class="line">    <span class="comment"># 这样就等于0 将在self-attention填充，正方形</span></span><br><span class="line">    <span class="keyword">elif</span> src_ids == <span class="literal">None</span>:</span><br><span class="line">        mask = torch.triu(torch.ones((tgt_ids.shape[-<span class="number">1</span>], tgt_ids.shape[-<span class="number">1</span>])), diagonal=<span class="number">1</span>).<span class="built_in">type</span>(torch.uint8)</span><br><span class="line">        <span class="keyword">return</span> mask == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoder第二层cro_mask的掩码，kv来自encoder，q和kv的形状可能不同，长方形</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Q= tgt_ids</span><br><span class="line">        KV = src_ids</span><br><span class="line">        mask = torch.matmul(Q.unsqueeze(-<span class="number">1</span>), KV.unsqueeze(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h1 id="汽车人变形！"><a href="#汽车人变形！" class="headerlink" title="汽车人变形！"></a>汽车人变形！</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> attn_mask</span><br><span class="line"><span class="keyword">from</span> embedding <span class="keyword">import</span> Embeddings, PositionalEncoding</span><br><span class="line"><span class="keyword">from</span> encoder <span class="keyword">import</span> Encoder, EncoderLayer</span><br><span class="line"><span class="keyword">from</span> decoder <span class="keyword">import</span> Decoder, DecoderLayer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,enc_vocab_size, dec_vocab_size, d_model, n_head, num_layer</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder_emb = Embeddings(enc_vocab_size, d_model)</span><br><span class="line">        self.decoder_emb = Embeddings(dec_vocab_size, d_model)</span><br><span class="line">        self.encoder_pos = PositionalEncoding(enc_vocab_size, d_model)</span><br><span class="line">        self.decoder_pos = PositionalEncoding(dec_vocab_size, d_model)</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(EncoderLayer(d_model=d_model,n_head= n_head), N=num_layer)</span><br><span class="line">        self.decoder = Decoder(DecoderLayer(d_model=d_model,n_head= n_head), N=num_layer)</span><br><span class="line">        self.answer = nn.Linear(d_model, dec_vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src_q, tgt_q</span>):</span><br><span class="line">        <span class="comment"># 生成掩码</span></span><br><span class="line">        encoder_mask = attn_mask(src_ids=src_q)</span><br><span class="line">        decoder_mask = attn_mask(tgt_ids=tgt_q)</span><br><span class="line">        cross_mask = attn_mask(src_q, tgt_q)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder部分   一种mask</span></span><br><span class="line">        src_q = self.encoder_emb(src_q) </span><br><span class="line">        src_q = self.encoder_pos(src_q)</span><br><span class="line">        k, v = self.encoder(src_q, src_q, src_q, encoder_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder部分   要传入两种mask</span></span><br><span class="line">        tgt_q = self.decoder_emb(tgt_q)</span><br><span class="line">        tgt_q = self.decoder_pos(tgt_q)</span><br><span class="line">        output = self.decoder(tgt_q, k, v, decoder_mask, cross_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 转换到tgt_vocab 做个argmax进行输出</span></span><br><span class="line">        output = self.answer(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.randint(<span class="number">1</span>,<span class="number">40</span>, (<span class="number">4</span>, <span class="number">23</span>))</span><br><span class="line">    y = torch.randint(<span class="number">1</span>,<span class="number">40</span>, (<span class="number">4</span>, <span class="number">17</span>))</span><br><span class="line">    trs = Transformer(<span class="number">40</span>, <span class="number">41</span>, <span class="number">784</span>, <span class="number">7</span>, <span class="number">2</span>)</span><br><span class="line">    output = trs(x, y)</span><br><span class="line">    <span class="built_in">print</span>(output.argmax(-<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><h1 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">encoder, decoder, sentence, max_length=MAX_LENGTH</span>):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_tensor = tensorFromSentence(input_lang, sentence)</span><br><span class="line">        input_length = input_tensor.size()[<span class="number">0</span>]</span><br><span class="line">        encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ei <span class="keyword">in</span> <span class="built_in">range</span>(input_length):</span><br><span class="line">            encoder_output, encoder_hidden = encoder(input_tensor[ei],</span><br><span class="line">                                                     encoder_hidden)</span><br><span class="line">            encoder_outputs[ei] += encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        decoder_input = torch.tensor([[SOS_token]], device=device)  <span class="comment"># SOS</span></span><br><span class="line"></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">        decoded_words = []</span><br><span class="line">        decoder_attentions = torch.zeros(max_length, max_length)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            decoder_attentions[di] = decoder_attention.data</span><br><span class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> topi.item() == EOS_token:</span><br><span class="line">                decoded_words.append(<span class="string">&#x27;&lt;EOS&gt;&#x27;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                decoded_words.append(output_lang.index2word[topi.item()])</span><br><span class="line"></span><br><span class="line">            decoder_input = topi.squeeze().detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</span><br></pre></td></tr></table></figure><ol><li>将encoder解出来</li><li>decoder传入<code> &lt;BOS&gt;</code></li><li>while 判断是否输出<code>&lt;EOS&gt;</code></li></ol><ul><li>encoder （64， 24，784） 准备好 KV</li><li>decoder （64， 1， 784）传入的是 SOS<ul><li><a href="mailto:&#x51;&#64;&#x4b;&#46;&#84;">&#x51;&#64;&#x4b;&#46;&#84;</a> 得到 （64，1，24）的scores</li><li>scores@V 得到 （64，1，784）</li><li>送出去（64，1，56233）做softmax取得token1<ul><li>得到目前时刻的解码<code> [&lt;BOS&gt;、token1]</code>送入下一时刻</li></ul></li><li>下一次就是（64，2，56233）<ul><li>得到目前时刻的解码<code> [&lt;BOS&gt;、token1、token2]</code>送入下一时刻</li></ul></li></ul></li><li>逐渐解码到  <code>[&lt;BOS&gt;、token1、token2....&lt;EOS&gt;]</code> 条件成立结束输出</li></ul><p><code>返回的attention_scores</code>可以做相关度矩阵分析</p><blockquote><p>os.environ[“CUDA_VISIBLE_DEVICES”] &#x3D; “1” 坑的一逼，指定你的gpu的代号，device &#x3D; torch.device(“cuda”) if torch.cuda.is_available() else torch.device(“cpu”) 就只能用1号，就算你只有一块也是1号，要在device(“cuda:1”)指定坑的一逼</p></blockquote><p><a href="http://nlp.seas.harvard.edu/annotated-transformer/">参考</a></p>]]></content>
      
      
      <categories>
          
          <category> Dive Into Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matplotlib 概述</title>
      <link href="/posts/15638.html"/>
      <url>/posts/15638.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><p>本文简单概述Matplotlib绘图库三个主要的类</p><ul><li>Axes</li><li>Figure</li><li>Artist</li></ul><p>通过以上三个类来了解作图的逻辑。<a href="https://matplotlib.org/stable/tutorials/index.html">推荐学习官网的示例</a></p><h1 id="Axes"><a href="#Axes" class="headerlink" title="Axes"></a>Axes</h1><p>首先介绍Axes，他可以理解为坐标系，他的子类是aixs坐标轴(见名知意很好理解)。一下我们通过一个<a href="https://matplotlib.org/stable/tutorials/introductory/quick_start.html#sphx-glr-tutorials-introductory-quick-start-py">示例</a>理解</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig, axs = plt.subplots()  </span><br><span class="line">axs.plot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><img src="https://matplotlib.org/stable/_images/sphx_glr_quick_start_001.png" style="zoom:67%;" /><p>以上我们引入matplotlib中的pyplot别名为plt</p><p>通过调用<code>plt.subplots() </code>返回两个对象 fig( figure)，axs( axes)。</p><h2 id="Axis"><a href="#Axis" class="headerlink" title="Axis"></a>Axis</h2><p>Axis顾名思义数轴，可以定义刻度，单位等</p><h1 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h1><p>整体画布</p><h1 id="Artist"><a href="#Artist" class="headerlink" title="Artist"></a>Artist</h1><p>图画元素, 线条的形状等</p><h1 id="面向对象语法"><a href="#面向对象语法" class="headerlink" title="面向对象语法"></a>面向对象语法</h1><h2 id="axes"><a href="#axes" class="headerlink" title="axes"></a>axes</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot() <span class="comment"># 初始化一个axes</span></span><br><span class="line"></span><br><span class="line">plt.subplots(m, n) <span class="comment"># m行n列axes</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig , axes = plt.subplots(<span class="number">2</span>,<span class="number">1</span>, figsize=(<span class="number">6</span>,<span class="number">6</span>))  <span class="comment"># 两行一列，</span></span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>].bar(sm1, sm2)  <span class="comment"># 柱状图</span></span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].plot(sm1, sm2) <span class="comment"># 折线图</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig , axes = plt.subplots(<span class="number">2</span>,<span class="number">2</span>, figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>, <span class="number">1</span>] <span class="comment"># 以坐标调用</span></span><br></pre></td></tr></table></figure><h2 id="Figure-1"><a href="#Figure-1" class="headerlink" title="Figure"></a>Figure</h2><p>plt.figure()返回一个画布</p><p>可以设定很多参数</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">figure</span>(<span class="params">num=<span class="literal">None</span>,  <span class="comment"># autoincrement if None, else integer from 1-N</span></span></span><br><span class="line"><span class="params">           figsize=<span class="literal">None</span>,  <span class="comment"># defaults to rc figure.figsize #多少英寸</span></span></span><br><span class="line"><span class="params">           dpi=<span class="literal">None</span>,  <span class="comment"># defaults to rc figure.dpi #清晰度</span></span></span><br><span class="line"><span class="params">           facecolor=<span class="literal">None</span>,  <span class="comment"># defaults to rc figure.facecolor #背景色</span></span></span><br><span class="line"><span class="params">           edgecolor=<span class="literal">None</span>,  <span class="comment"># defaults to rc figure.edgecolor # 边缘色</span></span></span><br><span class="line"><span class="params">           frameon=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">           FigureClass=Figure,</span></span><br><span class="line"><span class="params">           clear=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">           **kwargs</span></span><br><span class="line"><span class="params">           </span>):</span><br></pre></td></tr></table></figure><h1 id="全局参数"><a href="#全局参数" class="headerlink" title="全局参数"></a>全局参数</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;Heiti SC&#x27;</span>] <span class="comment"># 字体</span></span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;figure.dpi&#x27;</span>] = <span class="number">300</span> <span class="comment"># 清晰度</span></span><br><span class="line"></span><br><span class="line">mpl.rcParams.get(<span class="string">&#x27;figure.figsize&#x27;</span>) <span class="comment"># 获得画布尺寸</span></span><br><span class="line"></span><br><span class="line">para = &#123;<span class="string">&#x27;figure.dpi&#x27;</span>: <span class="number">500</span>, <span class="string">&#x27;figure.figsize&#x27;</span>: [<span class="number">10</span>, <span class="number">10</span>]&#125; </span><br><span class="line"></span><br><span class="line">mpl.rcParams.update(para) <span class="comment"># 通过上面的参数进行一次更新</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matplotlib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>句意相似度 PipeLine总结</title>
      <link href="/posts/10656.html"/>
      <url>/posts/10656.html</url>
      
        <content type="html"><![CDATA[<p><strong>主要进行训练框架优化</strong></p><ul><li>端到端 ML 实施（训练、验证、预测、评估）</li><li>轻松适应您自己的数据集</li><li>促进其他基于 BERT 的模型（BERT、ALBERT、…）的快速实验</li><li>使用有限的计算资源进行快速训练（混合精度、梯度累积……）</li><li>多 GPU 执行</li><li>分类决策的阈值选择（不一定是 0.5）</li><li>冻结 BERT 层，只更新分类层权重或更新所有权重</li><li>种子设置，可复现结果</li></ul><h1 id="PipeLine"><a href="#PipeLine" class="headerlink" title="PipeLine"></a>PipeLine</h1><h3 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, maxlen, with_labels=<span class="literal">True</span>, bert_model=<span class="string">&#x27;albert-base-v2&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">        self.data = data  <span class="comment"># pandas dataframe</span></span><br><span class="line">        <span class="comment">#Initialize the tokenizer</span></span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  </span><br><span class="line"></span><br><span class="line">        self.maxlen = maxlen</span><br><span class="line">        self.with_labels = with_labels </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#根据索引索取DataFrame中句子1余句子2</span></span><br><span class="line">        sent1 = <span class="built_in">str</span>(self.data.loc[index, <span class="string">&#x27;sentence1&#x27;</span>])</span><br><span class="line">        sent2 = <span class="built_in">str</span>(self.data.loc[index, <span class="string">&#x27;sentence2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对句子对分词，得到input_ids、attention_mask和token_type_ids</span></span><br><span class="line">        encoded_pair = self.tokenizer(sent1, sent2, </span><br><span class="line">                                      padding=<span class="string">&#x27;max_length&#x27;</span>,  <span class="comment"># 填充到最大长度</span></span><br><span class="line">                                      truncation=<span class="literal">True</span>,  <span class="comment"># 根据最大长度进行截断</span></span><br><span class="line">                                      max_length=self.maxlen,  </span><br><span class="line">                                      return_tensors=<span class="string">&#x27;pt&#x27;</span>)  <span class="comment"># 返回torch.Tensor张量</span></span><br><span class="line">        </span><br><span class="line">        token_ids = encoded_pair[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment"># tensor token ids</span></span><br><span class="line">        attn_masks = encoded_pair[<span class="string">&#x27;attention_mask&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment"># padded values对应为 &quot;0&quot; ，其他token为1</span></span><br><span class="line">        token_type_ids = encoded_pair[<span class="string">&#x27;token_type_ids&#x27;</span>].squeeze(<span class="number">0</span>)  <span class="comment">#第一个句子的值为0，第二个句子的值为1 # 只有一句全为0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.with_labels:  <span class="comment"># True if the dataset has labels</span></span><br><span class="line">            label = self.data.loc[index, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">            <span class="keyword">return</span> token_ids, attn_masks, token_type_ids, label  </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> token_ids, attn_masks, token_type_ids</span><br></pre></td></tr></table></figure><p>建议，进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample = <span class="built_in">next</span>(<span class="built_in">iter</span>(DataLoader(tr_dataset, batch_size=<span class="number">2</span>)))</span><br><span class="line">sample</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr_model = SentencePairClassifier(freeze_bert=True)</span><br><span class="line">tr_model(sample[0], sample[1], sample[2])</span><br></pre></td></tr></table></figure><p>就是方便最后的维度转换，squeeze、flatten、view；甚至可以用reshape方法</p><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentencePairClassifier</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert_model=<span class="string">&quot;albert-base-v2&quot;</span>, freeze_bert=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SentencePairClassifier, self).__init__()</span><br><span class="line">        <span class="comment">#  初始化预训练模型Bert xxx</span></span><br><span class="line">        self.bert_layer = AutoModel.from_pretrained(bert_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  encoder 隐藏层大小</span></span><br><span class="line">        <span class="keyword">if</span> bert_model == <span class="string">&quot;albert-base-v2&quot;</span>:  <span class="comment"># 12M 参数</span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-large-v2&quot;</span>:  <span class="comment"># 18M 参数</span></span><br><span class="line">            hidden_size = <span class="number">1024</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-xlarge-v2&quot;</span>:  <span class="comment"># 60M 参数</span></span><br><span class="line">            hidden_size = <span class="number">2048</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;albert-xxlarge-v2&quot;</span>:  <span class="comment"># 235M 参数</span></span><br><span class="line">            hidden_size = <span class="number">4096</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="comment"># 110M 参数</span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line">        <span class="keyword">elif</span> bert_model == <span class="string">&quot;roberta-base&quot;</span>: <span class="comment"># </span></span><br><span class="line">            hidden_size = <span class="number">768</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 固定Bert层 更新分类输出层</span></span><br><span class="line">        <span class="keyword">if</span> freeze_bert:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> self.bert_layer.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">False</span></span><br><span class="line">                </span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 分类输出</span></span><br><span class="line">        self.cls_layer = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">    @autocast()  </span><span class="comment"># 混合精度训练</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attn_masks, token_type_ids</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">            -input_ids : Tensor  containing token ids</span></span><br><span class="line"><span class="string">            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values</span></span><br><span class="line"><span class="string">            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入给Bert，获取上下文表示</span></span><br><span class="line">        <span class="comment"># cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)</span></span><br><span class="line">        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)</span><br><span class="line">        <span class="comment"># last_hidden_state,pooler_output,all_hidden_states 12层</span></span><br><span class="line">        <span class="comment"># 将last layer hidden-state of the [CLS] 输入到 classifier layer</span></span><br><span class="line">        <span class="comment"># - last_hidden_state 的向量平均</span></span><br><span class="line">        <span class="comment"># - 取all_hidden_states最后四层，然后做平均 weighted 平均</span></span><br><span class="line">        <span class="comment"># - last_hidden_state+lstm</span></span><br><span class="line">        <span class="comment"># 获取输出</span></span><br><span class="line">        logits = self.cls_layer(self.dropout(outputs[<span class="string">&#x27;pooler_output&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h3 id="固定随机种子"><a href="#固定随机种子" class="headerlink" title="固定随机种子"></a>固定随机种子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 固定随机种子，保证结果复现</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br></pre></td></tr></table></figure><h3 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!mkdir models <span class="comment">#可以在之前补充绝对路径</span></span><br><span class="line">!mkdir results</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate</span>):</span><br><span class="line"></span><br><span class="line">    best_loss = np.Inf</span><br><span class="line">    best_ep = <span class="number">1</span></span><br><span class="line">    nb_iterations = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    print_every = nb_iterations // <span class="number">5</span>  <span class="comment"># 打印频率</span></span><br><span class="line">    iters = []</span><br><span class="line">    train_losses = []</span><br><span class="line">    val_losses = []</span><br><span class="line"></span><br><span class="line">    scaler = GradScaler()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line"></span><br><span class="line">        net.train()</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> it, (seq, attn_masks, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_loader)):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 转为cuda张量</span></span><br><span class="line">            seq, attn_masks, token_type_ids, labels = \</span><br><span class="line">                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># 混合精度加速训练</span></span><br><span class="line">            <span class="keyword">with</span> autocast():</span><br><span class="line">                <span class="comment"># Obtaining the logits from the model</span></span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Computing loss</span></span><br><span class="line">                loss = criterion(logits.squeeze(-<span class="number">1</span>), labels.<span class="built_in">float</span>())</span><br><span class="line">                loss = loss / iters_to_accumulate  <span class="comment"># Normalize the loss because it is averaged</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backpropagating the gradients</span></span><br><span class="line">            <span class="comment"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class="line">            scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % iters_to_accumulate == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Optimization step</span></span><br><span class="line">                <span class="comment"># scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.</span></span><br><span class="line">                <span class="comment"># If these gradients do not contain infs or NaNs, opti.step() is then called,</span></span><br><span class="line">                <span class="comment"># otherwise, opti.step() is skipped.</span></span><br><span class="line">                scaler.step(opti)</span><br><span class="line">                <span class="comment"># Updates the scale for next iteration.</span></span><br><span class="line">                scaler.update()</span><br><span class="line">                <span class="comment"># 根据迭代次数调整学习率。</span></span><br><span class="line">                lr_scheduler.step()</span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                opti.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (it + <span class="number">1</span>) % print_every == <span class="number">0</span>:  <span class="comment"># Print training loss information</span></span><br><span class="line">                <span class="built_in">print</span>()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;it+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;nb_iterations&#125;</span> of epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete. \</span></span><br><span class="line"><span class="string">                Loss : <span class="subst">&#123;running_loss / print_every&#125;</span> &quot;</span>)</span><br><span class="line"></span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        val_loss = evaluate_loss(net, device, criterion, val_loader)  <span class="comment"># Compute validation loss</span></span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;ep+<span class="number">1</span>&#125;</span> complete! Validation Loss : <span class="subst">&#123;val_loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;</span>.<span class="built_in">format</span>(best_loss, val_loss))</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line">            net_copy = copy.deepcopy(net)  <span class="comment"># # 保存最优模型</span></span><br><span class="line">            best_loss = val_loss</span><br><span class="line">            best_ep = ep + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    path_to_model=<span class="string">f&#x27;models/<span class="subst">&#123;bert_model&#125;</span>_lr_<span class="subst">&#123;lr&#125;</span>_val_loss_<span class="subst">&#123;<span class="built_in">round</span>(best_loss, <span class="number">5</span>)&#125;</span>_ep_<span class="subst">&#123;best_ep&#125;</span>.pt&#x27;</span></span><br><span class="line">    torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The model has been saved in &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_model))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> loss</span><br><span class="line">    torch.cuda.empty_cache() <span class="comment"># 清空显存</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, device, criterion, dataloader</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    评估输出</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    mean_loss = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> it, (seq, attn_masks, token_type_ids, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(dataloader)):</span><br><span class="line">            seq, attn_masks, token_type_ids, labels = \</span><br><span class="line">                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)</span><br><span class="line">            logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">            mean_loss += criterion(logits.squeeze(-<span class="number">1</span>), labels.<span class="built_in">float</span>()).item()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mean_loss / count</span><br></pre></td></tr></table></figure><ol><li><p>注意autocast和累计梯度 这两种加速计算的方法</p></li><li><p>evaluate的时候要注意数据的维度，标签的类型</p></li></ol><h3 id="超参数-amp-开始训练"><a href="#超参数-amp-开始训练" class="headerlink" title="超参数 &amp; 开始训练"></a>超参数 &amp; 开始训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bert_model = <span class="string">&quot;albert-base-v2&quot;</span>  <span class="comment"># &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;</span></span><br><span class="line">freeze_bert = <span class="literal">False</span>  <span class="comment"># 是否冻结Bert</span></span><br><span class="line">maxlen = <span class="number">128</span>  <span class="comment"># 最大长度</span></span><br><span class="line">bs = <span class="number">16</span>  <span class="comment"># batch size</span></span><br><span class="line">iters_to_accumulate = <span class="number">2</span>  <span class="comment"># 梯度累加</span></span><br><span class="line">lr = <span class="number">2e-5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># 训练轮数</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  固定随机种子 便于复现</span></span><br><span class="line">set_seed(<span class="number">1</span>) <span class="comment"># 2022 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集与验证集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading training data...&quot;</span>)</span><br><span class="line">train_set = CustomDataset(df_train, maxlen, bert_model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading validation data...&quot;</span>)</span><br><span class="line">val_set = CustomDataset(df_val, maxlen, bert_model)</span><br><span class="line"><span class="comment"># 常见训练集与验证集DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  <span class="comment"># if multiple GPUs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    net = nn.DataParallel(net)</span><br><span class="line"></span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">opti = AdamW(net.parameters(), lr=lr, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line">num_warmup_steps = <span class="number">0</span> <span class="comment"># The number of steps for the warmup phase.</span></span><br><span class="line">num_training_steps = epochs * <span class="built_in">len</span>(train_loader)  <span class="comment"># The total number of training steps</span></span><br><span class="line">t_total = (<span class="built_in">len</span>(train_loader) // iters_to_accumulate) * epochs  <span class="comment"># Necessary to take into account Gradient accumulation</span></span><br><span class="line">lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)</span><br><span class="line"></span><br><span class="line">train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)</span><br></pre></td></tr></table></figure><ol><li>注意多gpu训练 <code>torch.cuda.device_count() &gt; 1</code>, <code>net = nn.DataParallel(net)</code>的使用</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_probs_from_logits</span>(<span class="params">logits</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Converts a tensor of logits into an array of probabilities by applying the sigmoid function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    probs = torch.sigmoid(logits.unsqueeze(-<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> probs.detach().cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_prediction</span>(<span class="params">net, device, dataloader, with_labels=<span class="literal">True</span>, result_file=<span class="string">&quot;results/output.txt&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Predict the probabilities on a dataset with or without labels and print the result in a file</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    w = <span class="built_in">open</span>(result_file, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    probs_all = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> with_labels:</span><br><span class="line">            <span class="keyword">for</span> seq, attn_masks, token_type_ids, _ <span class="keyword">in</span> tqdm(dataloader):<span class="comment"># 训练集、验证集</span></span><br><span class="line">                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)</span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">                probs = get_probs_from_logits(logits.squeeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                probs_all += probs.tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> seq, attn_masks, token_type_ids <span class="keyword">in</span> tqdm(dataloader): <span class="comment"># 没有标签的测试集</span></span><br><span class="line">                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)</span><br><span class="line">                logits = net(seq, attn_masks, token_type_ids)</span><br><span class="line">                probs = get_probs_from_logits(logits.squeeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">                probs_all += probs.tolist()</span><br><span class="line"></span><br><span class="line">    w.writelines(<span class="built_in">str</span>(prob)+<span class="string">&#x27;\n&#x27;</span> <span class="keyword">for</span> prob <span class="keyword">in</span> probs_all)</span><br><span class="line">    w.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">path_to_model = <span class="string">&#x27;./model&#x27;</span>  </span><br><span class="line"><span class="comment"># path_to_model = &#x27;/content/models/...&#x27;  # You can add here your trained model</span></span><br><span class="line"></span><br><span class="line">path_to_output_file = <span class="string">&#x27;./results&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reading test data...&quot;</span>)</span><br><span class="line">test_set = CustomDataset(df_test, maxlen, bert_model)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=bs, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">model = SentencePairClassifier(bert_model)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:  <span class="comment"># if multiple GPUs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loading the weights of the model...&quot;</span>)</span><br><span class="line">model.load_state_dict(torch.load(path_to_model))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicting on test data...&quot;</span>)</span><br><span class="line">test_prediction(net=model, device=device, dataloader=test_loader, with_labels=<span class="literal">True</span>,  <span class="comment"># set the with_labels parameter to False if your want to get predictions on a dataset without labels</span></span><br><span class="line">                result_file=path_to_output_file)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predictions are available in : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(path_to_output_file))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">path_to_output_file = <span class="string">&#x27;results/output.txt&#x27;</span>  <span class="comment"># 预测结果概率文件</span></span><br><span class="line"></span><br><span class="line">labels_test = df_test[<span class="string">&#x27;label&#x27;</span>]  <span class="comment"># true labels</span></span><br><span class="line"></span><br><span class="line">probs_test = pd.read_csv(path_to_output_file, header=<span class="literal">None</span>)[<span class="number">0</span>]  <span class="comment"># 预测概率</span></span><br><span class="line">threshold = <span class="number">0.6</span>   <span class="comment"># you can adjust this threshold for your own dataset</span></span><br><span class="line">preds_test=(probs_test&gt;=threshold).astype(<span class="string">&#x27;uint8&#x27;</span>) <span class="comment"># predicted labels using the above fixed threshold</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># metric = load_metric(&quot;glue&quot;, &quot;mrpc&quot;)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 句意相似度 </tag>
            
            <tag> Pipeline </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weight &amp; Bias</title>
      <link href="/posts/26087.html"/>
      <url>/posts/26087.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ol><li>源码细节整理</li></ol><h1 id="torch-inference-mode"><a href="#torch-inference-mode" class="headerlink" title="torch.inference_mode()"></a><code>torch.inference_mode()</code></h1><p>with no_gradient的一种加速  <a href="https://pytorch.org/docs/stable/generated/torch.inference_mode.html">参考文档</a></p><h1 id="nn-MarginRankingLoss"><a href="#nn-MarginRankingLoss" class="headerlink" title=" nn.MarginRankingLoss()"></a><code> nn.MarginRankingLoss()</code></h1><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html">文档</a> margin &#x3D; 0  x1大于x2 则去-y，viceversa 取 y</p><p>*<em>loss(<em>x</em>1,<em>x</em>2,<em>y</em>)&#x3D;max(0,−</em>y<em>∗(<em>x</em>1−</em>x<em>2)+margin)</em>*</p><p>这里最后的loss是平均后的</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MarginRankingLoss()</span><br><span class="line">input1 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">input2 = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>).sign()</span><br><span class="line">output = loss(input1, input2, target)</span><br><span class="line">output.backward()</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">input1, input2, target, output</span><br><span class="line"></span><br><span class="line">(tensor([ <span class="number">0.0277</span>, -<span class="number">0.3806</span>,  <span class="number">1.0405</span>], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> tensor([-<span class="number">0.9075</span>,  <span class="number">0.3271</span>,  <span class="number">0.1156</span>], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> tensor([ <span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]),</span><br><span class="line"> tensor(<span class="number">0.3083</span>, grad_fn=&lt;MeanBackward0&gt;))</span><br><span class="line"> </span><br><span class="line">input1 - input2 , (input1 - input2) * (-target)</span><br><span class="line"></span><br><span class="line">(tensor([ <span class="number">0.9352</span>, -<span class="number">0.7077</span>,  <span class="number">0.9249</span>], grad_fn=&lt;SubBackward0&gt;),</span><br><span class="line"> tensor([-<span class="number">0.9352</span>, -<span class="number">0.7077</span>,  <span class="number">0.9249</span>], grad_fn=&lt;MulBackward0&gt;),</span><br><span class="line"> </span><br><span class="line">loss = <span class="number">0.9249</span>/<span class="number">3</span> </span><br><span class="line"></span><br><span class="line">```</span><br></pre></td></tr></table></figure><h1 id="gc-collect"><a href="#gc-collect" class="headerlink" title="gc.collect()"></a><code>gc.collect()</code></h1><p>清除内存</p><h1 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a><code>defaultdict</code></h1><p>获得创建key不给value也不报错的dict</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">history = defaultdict(list)</span><br><span class="line"></span><br><span class="line">history[&#x27;Train Loss&#x27;].append(1.1)</span><br></pre></td></tr></table></figure><h1 id="StratifiedKFold"><a href="#StratifiedKFold" class="headerlink" title="StratifiedKFold()"></a><code>StratifiedKFold()</code></h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold, KFold</span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=CONFIG[<span class="string">&#x27;n_fold&#x27;</span>], shuffle=<span class="literal">True</span>, random_state=CONFIG[<span class="string">&#x27;seed&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> fold, ( _, val_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf.split(X=df, y=df.worker)):</span><br><span class="line">    df.loc[val_ , <span class="string">&quot;kfold&quot;</span>] = <span class="built_in">int</span>(fold)</span><br><span class="line">    </span><br><span class="line">df[<span class="string">&quot;kfold&quot;</span>] = df[<span class="string">&quot;kfold&quot;</span>].astype(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure><p>第五行 将X分k折，y标签为样本对应index，fold 在 0~5</p><p>得到df[“kfold”] 列包含 属于第几折的 valid数据</p><p>通过下面的函数直接选择<strong>非本折的数据作为train</strong>，其他的就是valid</p><p><code>df_train = df[df.kfold != fold].reset_index(drop=True) df_valid = df[df.kfold == fold].reset_index(drop=True)</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_loaders</span>(<span class="params">fold</span>):</span><br><span class="line">    df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG[<span class="string">&#x27;tokenizer&#x27;</span>], max_length=CONFIG[<span class="string">&#x27;max_length&#x27;</span>])</span><br><span class="line">    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG[<span class="string">&#x27;tokenizer&#x27;</span>], max_length=CONFIG[<span class="string">&#x27;max_length&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(train_dataset, batch_size=CONFIG[<span class="string">&#x27;train_batch_size&#x27;</span>], </span><br><span class="line">                              num_workers=<span class="number">2</span>, shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[<span class="string">&#x27;valid_batch_size&#x27;</span>], </span><br><span class="line">                              num_workers=<span class="number">2</span>, shuffle=<span class="literal">False</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_loader, valid_loade</span><br></pre></td></tr></table></figure><h1 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><code>tqdm</code></h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bar = tqdm(enumerate(dataloader), total=len(dataloader))</span><br></pre></td></tr></table></figure><p>单个epoch下面对bar做如下设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,</span><br><span class="line">                        LR=optimizer.param_groups[0][&#x27;lr&#x27;])  </span><br></pre></td></tr></table></figure><h1 id="Weights-amp-Biases-W-amp-B"><a href="#Weights-amp-Biases-W-amp-B" class="headerlink" title="Weights &amp; Biases (W&amp;B) "></a><code>Weights &amp; Biases (W&amp;B) </code></h1><ul><li><p>hash 一个项目id</p></li><li><p>train valid 定义一个 1个epoch 的函数 返回 分别其中的loss</p></li><li><p>wandb.log({“Train Loss”: train_epoch_loss}) 使用 log 方式记录 损失函数</p></li><li><pre><code class="py">run = wandb.init(project=&#39;Jigsaw&#39;,                      config=CONFIG,                     job_type=&#39;Train&#39;,                     group=CONFIG[&#39;group&#39;],                     tags=[&#39;roberta-base&#39;, f&#39;&#123;HASH_NAME&#125;&#39;, &#39;margin-loss&#39;],                     name=f&#39;&#123;HASH_NAME&#125;-fold-&#123;fold&#125;&#39;,                     anonymous=&#39;must&#39;)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TRAIN PART</span><br></pre></td></tr></table></figure>run.finish()<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">显示如下</span><br><span class="line">&#x27;hash--------name&#x27;</span><br><span class="line">Syncing run k5nu8k69390a-fold-0 to Weights &amp; Biases (docs).</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre></li></ul><h1 id="流程训练提炼"><a href="#流程训练提炼" class="headerlink" title="流程训练提炼"></a>流程训练提炼</h1><ul><li>for fold in range(0, CONFIG[‘n_fold’])</li><li>wandb.init</li><li>prepare_loaders、fetch_scheduler</li><li>run_training<ul><li>train_one_epoch、valid_one_epoch —-&gt; to got model, loss for wandb</li></ul></li></ul><p>中间掺杂 W&amp;B 的数据实时载入分析即可</p><p>df[‘y’].value_counts(normalize&#x3D;True) to got the percentage of each values</p><p><a href="https://www.kaggle.com/code/debarshichanda/pytorch-w-b-jigsaw-starter">原文链接</a></p>]]></content>
      
      
      <categories>
          
          <category> Trick </category>
          
      </categories>
      
      
        <tags>
            
            <tag> wandb </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV 02 Vit 叶子图片分类</title>
      <link href="/posts/28702.html"/>
      <url>/posts/28702.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Vit——Vision Transformer</p><p>这里通过<a href="https://www.kaggle.com/competitions/classify-leaves/data">kaggle的叶子分类任务</a>来使用预训练(Pre-train)模型Vit来提升我们的任务表示</p><h1 id="1-观察模型-amp-处理数据"><a href="#1-观察模型-amp-处理数据" class="headerlink" title="1.观察模型&amp;处理数据"></a>1.观察模型&amp;处理数据</h1><h2 id="1-1-模型探索"><a href="#1-1-模型探索" class="headerlink" title="1.1 模型探索"></a>1.1 模型探索</h2><p>无论是基于python的特性(适配各个领域的包)，还是NLP里大行其道的Pre-train范式，拥有快速了解一个包的特性以适用于我们工作的能力，将极大的提升我们工作的效率和结果。所以下面我们来快速体验一下<a href="https://huggingface.co/google/vit-base-patch16-224">HuggingFace给出的模型范例</a>，并针对我们的任务进行相应的数据处理。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ViTFeatureExtractor, ViTForImageClassification</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(requests.get(url, stream=<span class="literal">True</span>).raw)</span><br><span class="line"></span><br><span class="line">feature_extractor = ViTFeatureExtractor.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line">model = ViTForImageClassification.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line"></span><br><span class="line">inputs = feature_extractor(images=image, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">logits = outputs.logits</span><br><span class="line"><span class="comment"># model predicts one of the 1000 ImageNet classes</span></span><br><span class="line">predicted_class_idx = logits.argmax(-<span class="number">1</span>).item()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上面的代码可以自行运行</p><h3 id="1-1-1-示例解读"><a href="#1-1-1-示例解读" class="headerlink" title="1.1.1 示例解读"></a>1.1.1 示例解读</h3><ul><li><p>上十行代码: 首先通过requests库拿到一张图片并用image生成图片形式，下面两行加载了Vit16的特征提取器和HF特供的图片分类适配模型</p></li><li><p>下面我们看看 特征提取后的输入(inputs)</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># inputs 输出如下</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;pixel_values&#x27;: tensor([[[[ 0.1137,  0.1686,  0.1843,  ..., -0.1922, -0.1843, -0.1843],</span></span><br><span class="line"><span class="string">          [ 0.1373,  0.1686,  0.1843,  ..., -0.1922, -0.1922, -0.2078],</span></span><br><span class="line"><span class="string">          [ 0.1137,  0.1529,  0.1608,  ..., -0.2314, -0.2235, -0.2157],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.8353,  0.7882,  0.7333,  ...,  0.7020,  0.6471,  0.6157],</span></span><br><span class="line"><span class="string">          [ 0.8275,  0.7961,  0.7725,  ...,  0.5843,  0.4667,  0.3961],</span></span><br><span class="line"><span class="string">          [ 0.8196,  0.7569,  0.7569,  ...,  0.0745, -0.0510, -0.1922]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-0.8039, -0.8118, -0.8118,  ..., -0.8902, -0.8902, -0.8980],</span></span><br><span class="line"><span class="string">          [-0.7882, -0.7882, -0.7882,  ..., -0.8745, -0.8745, -0.8824],</span></span><br><span class="line"><span class="string">          [-0.8118, -0.8039, -0.7882,  ..., -0.8902, -0.8902, -0.8902],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [-0.2706, -0.3176, -0.3647,  ..., -0.4275, -0.4588, -0.4824],</span></span><br><span class="line"><span class="string">          [-0.2706, -0.2941, -0.3412,  ..., -0.4824, -0.5451, -0.5765],</span></span><br><span class="line"><span class="string">          [-0.2784, -0.3412, -0.3490,  ..., -0.7333, -0.7804, -0.8353]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-0.5451, -0.4667, -0.4824,  ..., -0.7412, -0.6941, -0.7176],</span></span><br><span class="line"><span class="string">          [-0.5529, -0.5137, -0.4902,  ..., -0.7412, -0.7098, -0.7412],</span></span><br><span class="line"><span class="string">          [-0.5216, -0.4824, -0.4667,  ..., -0.7490, -0.7490, -0.7647],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.5686,  0.5529,  0.4510,  ...,  0.4431,  0.3882,  0.3255],</span></span><br><span class="line"><span class="string">          [ 0.5451,  0.4902,  0.5137,  ...,  0.3020,  0.2078,  0.1294],</span></span><br><span class="line"><span class="string">          [ 0.5686,  0.5608,  0.5137,  ..., -0.2000, -0.4275, -0.5294]]]])&#125;</span></span><br><span class="line"><span class="string">          &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">inputs[<span class="string">&#x27;pixel_values&#x27;</span>].size()</span><br><span class="line"><span class="comment"># torch.Size([1, 3, 224, 224])</span></span><br></pre></td></tr></table></figure><p>可以看到是一个字典类型的tensor数据，其维度为(b, C, W, H)</p><p><strong>因此我们喂给模型的数据也得是四维的结构</strong></p></li><li><p>接下来看看模型吐出来的结果</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># outputs 输入如下</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">MaskedLMOutput(loss=tensor(0.4776, grad_fn=&lt;DivBackward0&gt;), logits=tensor([[[[-0.0630, -0.0475, -0.1557,  ...,  0.0950,  0.0216, -0.0084],</span></span><br><span class="line"><span class="string">          [-0.1219, -0.0329, -0.0849,  ..., -0.0152, -0.0143, -0.0663],</span></span><br><span class="line"><span class="string">          [-0.1063, -0.0925, -0.0350,  ...,  0.0238, -0.0206, -0.2159],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.2204,  0.0593, -0.2771,  ...,  0.0819,  0.0535, -0.1783],</span></span><br><span class="line"><span class="string">          [-0.0302, -0.1537, -0.1370,  ..., -0.1245, -0.1181, -0.0070],</span></span><br><span class="line"><span class="string">          [ 0.0875,  0.0626, -0.0693,  ...,  0.1331,  0.1088, -0.0835]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.1977, -0.2163,  0.0469,  ...,  0.0802, -0.0414,  0.0552],</span></span><br><span class="line"><span class="string">          [ 0.1125, -0.0369,  0.0175,  ...,  0.0598, -0.0843,  0.0774],</span></span><br><span class="line"><span class="string">          [ 0.1559, -0.0994, -0.0055,  ..., -0.0215,  0.2452, -0.0603],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [ 0.0603,  0.1887,  0.2060,  ...,  0.0415, -0.0383,  0.0990],</span></span><br><span class="line"><span class="string">          [ 0.2106,  0.0992, -0.1562,  ..., -0.1254, -0.0603,  0.0685],</span></span><br><span class="line"><span class="string">          [ 0.0256,  0.1578,  0.0304,  ..., -0.0894,  0.0659,  0.1493]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-0.0348, -0.0362, -0.1617,  ...,  0.0527,  0.1927,  0.1431],</span></span><br><span class="line"><span class="string">          [-0.0447,  0.0137, -0.0798,  ...,  0.1057, -0.0299, -0.0742],</span></span><br><span class="line"><span class="string">          [-0.0725,  0.1473, -0.0118,  ..., -0.1284,  0.0010, -0.0773],</span></span><br><span class="line"><span class="string">          ...,</span></span><br><span class="line"><span class="string">          [-0.0315,  0.1065, -0.1130,  ...,  0.0091, -0.0650,  0.0688],</span></span><br><span class="line"><span class="string">          [ 0.0314,  0.1034, -0.0964,  ...,  0.0144,  0.0532, -0.0415],</span></span><br><span class="line"><span class="string">          [-0.0205,  0.0046, -0.0987,  ...,  0.1317, -0.0065, -0.1617]]]],</span></span><br><span class="line"><span class="string">       grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>可以看到有loss、logits、hidden_states、attentions，而我们的范例只取了logits作为结果输出。这里并不是说其他的部分没用，是只取适配下游任务的输出即可。<a href="https://arxiv.org/abs/2010.11929">详情可研究Vit的论文</a></p></li><li><p>最后通过<code>argmax</code>函数和<code>model.config.id2label</code>得出标签相对应的文字</p><p>argmax就是返回最大值的位置下标、model.config.id2label配置了对应标签的名称，也知道了最后的classifier层是1000维的</p></li></ul><h3 id="1-1-2-小结"><a href="#1-1-2-小结" class="headerlink" title="1.1.2 小结"></a>1.1.2 小结</h3><p>通过以上探索，我们可以得出：</p><ol><li>输入的维度为(batch_size, 3, 224, 224)</li><li>最后的classifier需由1000改成我们叶子的类别数</li></ol><h2 id="1-2-数据处理"><a href="#1-2-数据处理" class="headerlink" title="1.2 数据处理"></a>1.2 数据处理</h2><p>接下来我们将探索数据的特性，并修改以适应我们的模型</p><h3 id="1-2-1-EDA"><a href="#1-2-1-EDA" class="headerlink" title="1.2.1 EDA"></a>1.2.1 EDA</h3><p>即Exploratory Data Analysis</p><p>首先导入所需包</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入各种包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastprogress.fastprogress <span class="keyword">import</span> master_bar, progress_bar</span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> CosineAnnealingLR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (AdamW, get_scheduler)</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ViTFeatureExtractor, ViTForImageClassification</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看初始数据</p><p><code>train_df = pd.read_csv(&#39;/kaggle/input/classify-leaves/train.csv&#39;)</code></p><img src="../../article_img/leaves classifier df 01.png" style="zoom:80%;" /><p>使用下面代码给分类配上序号</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">num_map</span>(<span class="params">file_path</span>):</span><br><span class="line">    data_df = pd.read_csv(<span class="string">&#x27;/kaggle/input/classify-leaves/train.csv&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    categories = data_df.label.unique().tolist()</span><br><span class="line">    categories_zip = <span class="built_in">list</span>(<span class="built_in">zip</span>( <span class="built_in">range</span>(<span class="built_in">len</span>(categories)) , categories))</span><br><span class="line">    categories_dict = &#123;v:k <span class="keyword">for</span> k, v <span class="keyword">in</span> categories_zip&#125;</span><br><span class="line">    </span><br><span class="line">    data_df[<span class="string">&#x27;num_label&#x27;</span>] = data_df.label.<span class="built_in">map</span>(categories_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_df</span><br><span class="line">show_df = num_map(<span class="string">&#x27;/kaggle/input/classify-leaves/train.csv&#x27;</span>)</span><br><span class="line">show_df.to_csv(<span class="string">&#x27;train_valid_dataset.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><img src="../../article_img/leaves classifier df 02.png" style="zoom:80%;" /><h3 id="1-2-2-图片数据查看"><a href="#1-2-2-图片数据查看" class="headerlink" title="1.2.2 图片数据查看"></a>1.2.2 图片数据查看</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">&#x27;/kaggle/input/classify-leaves/&#x27;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(path+train_df.image[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.figure(&quot;Image&quot;) # 图像窗口名称</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>) <span class="comment"># 关掉坐标轴为 off</span></span><br><span class="line">plt.title(<span class="string">&#x27;image&#x27;</span>) <span class="comment"># 图像题目</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="../../article_img/leaves classifier df 03 .png" style="zoom:80%;" /><p>这里我们做一下维度转换 即 [0, 1, 2]  换成 [2, 1, 0], 并只取某一个通道 看看</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.asarray(img).shape</span></span><br><span class="line"><span class="comment"># 可以看到图片反了，正确的顺序是.transpose([2, 0, 1])</span></span><br><span class="line">img_trans = np.asarray(img).transpose([<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">plt.imshow(img_trans[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="../../article_img/leaves classifier df 04 .png" style="zoom:80%;" /><h1 id="2-Preprocessing"><a href="#2-Preprocessing" class="headerlink" title="2.Preprocessing"></a>2.Preprocessing</h1><p>接下来我们分别要做 数据增强、数据类定义、数据加载器测试</p><h3 id="2-1-1-先来算个平均值标准差"><a href="#2-1-1-先来算个平均值标准差" class="headerlink" title="2.1.1 先来算个平均值标准差"></a>2.1.1 先来算个平均值标准差</h3><p>这里算的mean跟std是为了Normalize我们的数据使训练更稳定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_list</span>(<span class="params">img_dir, isclasses=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将图像的名称列表</span></span><br><span class="line"><span class="string">    args: img_dir:存放图片的目录</span></span><br><span class="line"><span class="string">          isclasses:图片是否按类别存放标志</span></span><br><span class="line"><span class="string">    return: 图片文件名称列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 路径下图像是否按类别分类存放</span></span><br><span class="line">    <span class="keyword">if</span> isclasses:</span><br><span class="line">        img_file = os.listdir(img_dir)</span><br><span class="line">        <span class="keyword">for</span> class_name <span class="keyword">in</span> img_file:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(os.path.join(img_dir, class_name)):     </span><br><span class="line">                class_img_list = os.listdir(os.path.join(img_dir, class_name))</span><br><span class="line">                img_list.extend(class_img_list)         </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_list = os.listdir(img_dir)</span><br><span class="line">    <span class="built_in">print</span>(img_list)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;image numbers: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_list)))</span><br><span class="line">    <span class="keyword">return</span> img_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_pixel_mean</span>(<span class="params">img_dir, img_list, img_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;求数据集图像的R、G、B均值</span></span><br><span class="line"><span class="string">    args: img_dir:</span></span><br><span class="line"><span class="string">          img_list:</span></span><br><span class="line"><span class="string">          img_size:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    R_sum = <span class="number">0</span></span><br><span class="line">    G_sum = <span class="number">0</span></span><br><span class="line">    B_sum = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环读取所有图片</span></span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> img_list:</span><br><span class="line">        img_path = os.path.join(img_dir, img_name)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(img_path):</span><br><span class="line">            image = cv2.imread(img_path)</span><br><span class="line">            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">            image = cv2.resize(image, (img_size, img_size))      <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">            R_sum += image[:, :, <span class="number">0</span>].mean()</span><br><span class="line">            G_sum += image[:, :, <span class="number">1</span>].mean()</span><br><span class="line">            B_sum += image[:, :, <span class="number">2</span>].mean()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    R_mean = R_sum / count</span><br><span class="line">    G_mean = G_sum / count</span><br><span class="line">    B_mean = B_sum / count</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;R_mean:&#123;&#125;, G_mean:&#123;&#125;, B_mean:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(R_mean,G_mean,B_mean))</span><br><span class="line">    RGB_mean = [R_mean, G_mean, B_mean]</span><br><span class="line">    <span class="keyword">return</span> RGB_mean</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_pixel_std</span>(<span class="params">img_dir, img_mean, img_list, img_size</span>):</span><br><span class="line">    R_squared_mean = <span class="number">0</span></span><br><span class="line">    G_squared_mean = <span class="number">0</span></span><br><span class="line">    B_squared_mean = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    image_mean = np.array(img_mean)</span><br><span class="line">    <span class="comment"># 循环读取所有图片</span></span><br><span class="line">    <span class="keyword">for</span> img_name <span class="keyword">in</span> img_list:</span><br><span class="line">        img_path = os.path.join(img_dir, img_name)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(img_path):</span><br><span class="line">            image = cv2.imread(img_path)    <span class="comment"># 读取图片</span></span><br><span class="line">            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">            image = cv2.resize(image, (img_size, img_size))      <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">            image = image - image_mean    <span class="comment"># 零均值</span></span><br><span class="line">            <span class="comment"># 求单张图片的方差</span></span><br><span class="line">            R_squared_mean += np.mean(np.square(image[:, :, <span class="number">0</span>]).flatten())</span><br><span class="line">            G_squared_mean += np.mean(np.square(image[:, :, <span class="number">1</span>]).flatten())</span><br><span class="line">            B_squared_mean += np.mean(np.square(image[:, :, <span class="number">2</span>]).flatten())</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    R_std = math.sqrt(R_squared_mean / count)</span><br><span class="line">    G_std = math.sqrt(G_squared_mean / count)</span><br><span class="line">    B_std = math.sqrt(B_squared_mean / count)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;R_std:&#123;&#125;, G_std:&#123;&#125;, B_std:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(R_std, G_std, B_std))</span><br><span class="line">    RGB_std = [R_std, G_std, B_std]</span><br><span class="line">    <span class="keyword">return</span> RGB_std</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    image_dir = <span class="string">&#x27;/图片文件路径&#x27;</span></span><br><span class="line">    image_list = get_image_list(image_dir, isclasses=<span class="literal">False</span>)</span><br><span class="line">    RGB_mean = get_image_pixel_mean(image_dir, image_list, img_size=<span class="number">224</span>)</span><br><span class="line">    get_image_pixel_std(image_dir, RGB_mean, image_list, img_size=<span class="number">224</span>)```</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-1-2-数据增强"><a href="#2-1-2-数据增强" class="headerlink" title="2.1.2 数据增强"></a>2.1.2 数据增强</h3><p><code>transforms.Compose</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])</span><br></pre></td></tr></table></figure><p><a href="https://pytorch.org/vision/stable/transforms.html#compositions-of-transforms">见官网</a></p><h3 id="2-2-1-Dataset"><a href="#2-2-1-Dataset" class="headerlink" title="2.2.1 Dataset"></a>2.2.1 Dataset</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">imgdataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, os_path, file_path, transform </span>):</span><br><span class="line">        self.os_path = os_path</span><br><span class="line">        self.data = pd.read_csv(file_path)</span><br><span class="line">        self.transform = transform</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img = Image.<span class="built_in">open</span>(self.os_path + self.data.image[idx])</span><br><span class="line">        label = self.data.num_label[idx]</span><br><span class="line">        </span><br><span class="line">        self.transform != <span class="literal">None</span>:</span><br><span class="line">        img = self.transform(img)</span><br><span class="line">        label = torch.tensor(label)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="2-2-2-模型测试"><a href="#2-2-2-模型测试" class="headerlink" title="2.2.2 模型测试"></a>2.2.2 模型测试</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = imgdataset(<span class="string">&#x27;/kaggle/input/classify-leaves/&#x27;</span>, </span><br><span class="line">                           <span class="string">&#x27;/kaggle/working/processed_train.csv&#x27;</span>,</span><br><span class="line">                          transform = transform)</span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size = <span class="number">1</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">samples = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line">samples[<span class="number">0</span>], samples[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">``` 输入如下</span><br><span class="line">(tensor([[[[<span class="number">0.7608</span>, <span class="number">0.7608</span>, <span class="number">0.7608</span>,  ..., <span class="number">0.8353</span>, <span class="number">0.8353</span>, <span class="number">0.8392</span>],</span><br><span class="line">           [<span class="number">0.7608</span>, <span class="number">0.7608</span>, <span class="number">0.7608</span>,  ..., <span class="number">0.8392</span>, <span class="number">0.8353</span>, <span class="number">0.8431</span>],</span><br><span class="line">           [<span class="number">0.7608</span>, <span class="number">0.7608</span>, <span class="number">0.7608</span>,  ..., <span class="number">0.8392</span>, <span class="number">0.8392</span>, <span class="number">0.8431</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>]],</span><br><span class="line"> </span><br><span class="line">          [[<span class="number">0.8118</span>, <span class="number">0.8118</span>, <span class="number">0.8118</span>,  ..., <span class="number">0.8588</span>, <span class="number">0.8588</span>, <span class="number">0.8627</span>],</span><br><span class="line">           [<span class="number">0.8118</span>, <span class="number">0.8118</span>, <span class="number">0.8118</span>,  ..., <span class="number">0.8627</span>, <span class="number">0.8588</span>, <span class="number">0.8667</span>],</span><br><span class="line">           [<span class="number">0.8118</span>, <span class="number">0.8118</span>, <span class="number">0.8118</span>,  ..., <span class="number">0.8627</span>, <span class="number">0.8627</span>, <span class="number">0.8667</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>]],</span><br><span class="line"> </span><br><span class="line">          [[<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.8510</span>, <span class="number">0.8510</span>, <span class="number">0.8549</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.8549</span>, <span class="number">0.8510</span>, <span class="number">0.8588</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.8549</span>, <span class="number">0.8549</span>, <span class="number">0.8588</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>],</span><br><span class="line">           [<span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>,  ..., <span class="number">0.7725</span>, <span class="number">0.7725</span>, <span class="number">0.7725</span>]]]]),</span><br><span class="line"> tensor([<span class="number">55</span>])) ```</span><br></pre></td></tr></table></figure><p>这里可以直接看到transforms的ToTensor方式已经将我们的数据修改乘(C, W, H)形式（原来的是 C在最后）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_ot = model(samples[<span class="number">0</span>])</span><br><span class="line">test_pred = test_ot.logits.argmax(-<span class="number">1</span>)</span><br><span class="line">test_pred, test_ot.logits</span><br></pre></td></tr></table></figure><h1 id="3-训练循环"><a href="#3-训练循环" class="headerlink" title="3. 训练循环"></a>3. 训练循环</h1><h2 id="3-1-plot"><a href="#3-1-plot" class="headerlink" title="3.1 plot"></a>3.1 plot</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_loss_update</span>(<span class="params">epoch, mb, train_loss, valid_loss</span>):</span><br><span class="line"></span><br><span class="line">    x = <span class="built_in">range</span>(<span class="number">1</span>, epoch+<span class="number">1</span>)</span><br><span class="line">    y = np.concatenate((train_loss, valid_loss))</span><br><span class="line">    graphs = [[x,train_loss], [x,valid_loss]]</span><br><span class="line">    x_margin = <span class="number">0.2</span></span><br><span class="line">    y_margin = <span class="number">0.05</span></span><br><span class="line">    x_bounds = [<span class="number">1</span>-x_margin, epochs+x_margin]</span><br><span class="line">    <span class="comment"># y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]  #边界换成0-1看看</span></span><br><span class="line">y_bounds = [<span class="number">0</span>-y_margin, <span class="number">1</span>+y_margin]</span><br><span class="line">    </span><br><span class="line">    mb.update_graph(graphs, x_bounds, y_bounds)</span><br></pre></td></tr></table></figure><p>上面是一个 在训练过程中绘制ACC的包 <a href="https://github.com/fastai/fastprogress">fastprogress</a></p><h2 id="3-2-train-valid"><a href="#3-2-train-valid" class="headerlink" title="3.2 train_valid"></a>3.2 train_valid</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>(<span class="params">net, device, criterion, opti, lr, lr_scheduler, batch_size, </span></span><br><span class="line"><span class="params">               train_loader, val_loader, epochs, model_name</span>):</span><br><span class="line">    </span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    train_acc, valid_acc = [], []</span><br><span class="line">    mb = master_bar(<span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> mb:</span><br><span class="line">        train_correct, valid_correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># train_part</span></span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> progress_bar(train_loader, parent=mb):</span><br><span class="line"></span><br><span class="line">            x, y = <span class="built_in">tuple</span>(k.to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch_data)</span><br><span class="line">            outputs = net(x).logits</span><br><span class="line">            loss = criterion(outputs, y)</span><br><span class="line">            train_correct += (outputs.argmax(dim=-<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            opti.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">            opti.step()</span><br><span class="line">        lr_now = lr_scheduler.get_lr()[<span class="number">0</span>]</span><br><span class="line">        train_acc.append(train_correct/(<span class="built_in">len</span>(train_loader)*batch_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># valid_part</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> progress_bar(val_loader, parent=mb):</span><br><span class="line"></span><br><span class="line">                x, y = <span class="built_in">tuple</span>(k.to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch_data)</span><br><span class="line">                outputs = net(x).logits</span><br><span class="line">                loss = criterion(outputs, y)</span><br><span class="line">                valid_correct += (outputs.argmax(dim=-<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            valid_acc.append(valid_correct/(<span class="built_in">len</span>(val_loader)*batch_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plot</span></span><br><span class="line">        plot_loss_update(epochs, mb, train_acc, valid_acc)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print info</span></span><br><span class="line">        train_loss_now = train_acc[-<span class="number">1</span>]</span><br><span class="line">        valid_loss_now = valid_acc[-<span class="number">1</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> complete! Train Acc : </span></span><br><span class="line"><span class="string">              <span class="subst">&#123;train_loss_now*<span class="number">100</span>:<span class="number">.5</span>f&#125;</span>% with lr <span class="subst">&#123;lr_now:<span class="number">.4</span>f&#125;</span>&quot;</span>, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> complete! Validation Acc : <span class="subst">&#123;valid_loss_now*<span class="number">100</span>:<span class="number">.5</span>f&#125;</span>%&quot;</span>, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> valid_loss_now</span><br></pre></td></tr></table></figure><p>上面我们定义两个数组保存ACC的值，以绘制图形</p><h2 id="3-3-kfold-save"><a href="#3-3-kfold-save" class="headerlink" title="3.3 kfold_save"></a>3.3 kfold_save</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kfold_loop</span>(<span class="params">data, save_path, config</span>):</span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> fold, (train_ids,valid_ids) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(data)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;FOLD <span class="subst">&#123;fold&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># config 数据配置</span></span><br><span class="line">        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)</span><br><span class="line">        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)</span><br><span class="line">        </span><br><span class="line">        config[<span class="string">&#x27;train_loader&#x27;</span>] = torch.utils.data.DataLoader(data, batch_size=<span class="number">32</span>, </span><br><span class="line">                                                 sampler=train_subsampler, num_workers=<span class="number">2</span>)</span><br><span class="line">        config[<span class="string">&#x27;val_loader&#x27;</span>] = torch.utils.data.DataLoader(data,batch_size=<span class="number">32</span>, </span><br><span class="line">                                                  sampler=valid_subsampler, num_workers=<span class="number">2</span>)</span><br><span class="line">        config[<span class="string">&#x27;opti&#x27;</span>]  = torch.optim.AdamW(model.parameters(), lr=lr)</span><br><span class="line">        config[<span class="string">&#x27;lr_scheduler&#x27;</span>] = CosineAnnealingLR(config[<span class="string">&#x27;opti&#x27;</span>],T_max=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        net.to(device)</span><br><span class="line">        valid_acc_now = train_loop(**config)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># save  保存最好的模型</span></span><br><span class="line">        <span class="keyword">if</span> valid_acc_now &gt; best_acc:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Best validation Acc improved from <span class="subst">&#123;best_acc:<span class="number">.5</span>f&#125;</span> to <span class="subst">&#123;valid_loss_now:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            net_copy = copy.deepcopy(net)</span><br><span class="line">            best_acc = valid_loss_now</span><br><span class="line"></span><br><span class="line">            save_path = config[<span class="string">&#x27;save_path&#x27;</span>]</span><br><span class="line">            path_to_model = <span class="string">f&#x27;<span class="subst">&#123;save_path&#125;</span>/<span class="subst">&#123;model_name&#125;</span>_lr_<span class="subst">&#123;lr_now:<span class="number">.5</span>f&#125;</span>_valid_acc_<span class="subst">&#123;best_acc:<span class="number">.5</span>f&#125;</span>.pt&#x27;</span></span><br><span class="line">            torch.save(net_copy.state_dict(), path_to_model)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;The model has been saved in <span class="subst">&#123;path_to_model&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这里我们进行k折验证</p><h2 id="3-4-config"><a href="#3-4-config" class="headerlink" title="3.4 config"></a>3.4 config</h2><p>最后我们定义超参数，以及其他构件</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">seed = <span class="number">1222</span></span><br><span class="line">bs = <span class="number">32</span></span><br><span class="line">lr = <span class="number">3e-4</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">warm_steps = <span class="number">122</span>*epochs</span><br><span class="line">total_steps = <span class="number">458</span>*epochs</span><br><span class="line"></span><br><span class="line">set_seed(seed)</span><br><span class="line"></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.08</span>, <span class="number">1.0</span>), ratio=(<span class="number">3.0</span> / <span class="number">4.0</span>, <span class="number">4.0</span> / <span class="number">3.0</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">os_path = <span class="string">&#x27;/kaggle/input/classify-leaves/&#x27;</span></span><br><span class="line">file_path = <span class="string">&#x27;/kaggle/working/train_valid_dataset.csv&#x27;</span></span><br><span class="line">dataset = imgdataset(os_path, file_path, train_transform)</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = ViTForImageClassification.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line">model.classifier = nn.Linear(<span class="number">768</span>, <span class="number">176</span>)</span><br><span class="line"><span class="keyword">for</span> idx, para <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.parameters()): <span class="comment">#冻结部分参数</span></span><br><span class="line">    para.requires_grad = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">197</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">criter = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><p>打包配置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;net&#x27;</span>:model,</span><br><span class="line">    <span class="string">&#x27;device&#x27;</span>:device,</span><br><span class="line">    </span><br><span class="line">    <span class="string">&#x27;lr&#x27;</span>:lr,</span><br><span class="line">    <span class="string">&#x27;opti&#x27;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;lr_scheduler&#x27;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;criterion&#x27;</span>:criter,</span><br><span class="line">    </span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>:bs, </span><br><span class="line">    <span class="string">&#x27;train_loader&#x27;</span>:<span class="number">1</span>, </span><br><span class="line">    <span class="string">&#x27;val_loader&#x27;</span>:<span class="number">1</span>, </span><br><span class="line">    <span class="string">&#x27;epochs&#x27;</span>:epochs, </span><br><span class="line">    <span class="string">&#x27;model_name&#x27;</span>:<span class="string">&#x27;leaves_classifier_model&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="4-训练-amp-结果分析"><a href="#4-训练-amp-结果分析" class="headerlink" title="4. 训练 &amp; 结果分析"></a>4. 训练 &amp; 结果分析</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!mkdir model_save</span><br><span class="line">save_dir = os.getcwd()+<span class="string">&#x27;/model_save&#x27;</span></span><br><span class="line"></span><br><span class="line">kfold_loop(dataset, save_dir, config)</span><br></pre></td></tr></table></figure><blockquote><p>这里第一个fold 出了点问题，总之valid_acc应该是从6%到了23% 后面就是跟下图一样了</p></blockquote><img src="../../article_img/1669374036209.jpg" style="zoom:80%;" /><p>这里我截取了两个fold进行数据查看 (1fold在p100上训练大概40分钟左右）</p><ul><li>随着模型在训练集上的准确率上升，模型在验证集上的准确率也跟train_acc逐步拟合，当然由于验证集的数据没有训练过，中间有一些抖动。但是模型最后还是学到东西了的。</li></ul><p><strong>小结</strong>：</p><p>由于硬件资源的限制，就不再进行训练(模型还是在继续提升的)，我们省略了模型融合和提交结果的验证，这里简单提下</p><ul><li>以投票方式的模型融合为例，Vit的投票结果占权重0.4，剩下的ResNeSt和ResNeXt各占0.25,  VGG占0.1，最后决定输出的标签</li><li>test上就是valid部分 只输出176维度里最大值的位置即可</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>此次我们学习了<strong>Pre-train</strong>的范式、<strong>K-fold验证</strong>、<strong>DataAugment</strong>。</p><ul><li>重点是理解‘拿来主义’，总之拿来就用</li><li>k折交叉验证只是验证的一种方式</li><li>数据增强则需要在理解数据集的基础上进行，是炼丹师必修的一门，当然也有非常多中增强数据的方式</li></ul><p>之后我们将进行对比学习的讲解</p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> HuggingFace </tag>
            
            <tag> Trick </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV 01 CNN MNIST识别</title>
      <link href="/posts/53023.html"/>
      <url>/posts/53023.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将通过CNN 让计算机识别MNIST数据集中的手写数字 以此来介绍Pytorch的基本使用方法：</p><ul><li>Pytorch中的数据类型——tensor</li><li>Pytorch中的数据集、数据加载器——Dataset、DataLoader</li><li>Pytorch中的基础类模型——torch.nn.Module</li></ul><p>以及程序设计上的一些小技巧。</p><h1 id="1-tensor"><a href="#1-tensor" class="headerlink" title="1. tensor"></a>1. tensor</h1><h2 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h2><p>tensor一词译为<strong>张量</strong>，一般我们所接触的矩阵是二维的，称为二阶张量、向量称为一阶张量、标量称为零阶张量。接下来我们通过Numpy库了解一下张量。（这里并非数学上严格的定义，感性理解一下即可）</p><h3 id="1-1-1-二阶张量-矩阵"><a href="#1-1-1-二阶张量-矩阵" class="headerlink" title="1.1.1 二阶张量 矩阵"></a>1.1.1 二阶张量 矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先我们举一个三行八列的矩阵</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">1</span>,<span class="number">25</span>).reshape(<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 1,  2,  3,  4,  5,  6,  7,  8],</span></span><br><span class="line"><span class="string">       [ 9, 10, 11, 12, 13, 14, 15, 16],</span></span><br><span class="line"><span class="string">       [17, 18, 19, 20, 21, 22, 23, 24]])&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">b = np.ones_like(a).T</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1],</span></span><br><span class="line"><span class="string">           [1, 1, 1]])&#x27;&#x27;&#x27;</span></span><br><span class="line">          </span><br><span class="line"><span class="comment"># 我们创建以上两个矩阵，接下来我们把他们做点乘</span></span><br><span class="line">a@b</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[ 36,  36,  36],</span></span><br><span class="line"><span class="string">           [100, 100, 100],</span></span><br><span class="line"><span class="string">           [164, 164, 164]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>上面我们创建了两个矩阵a为三行八列，b为八行三列，两者做点积得到一个三行三列的矩阵。</p><p>我们拉到列表的角度解释这个矩阵，我们将所有数据都包含在一个大列表之内，大列表里有三个小列表，每个列表内有八个元素，</p><p>即<strong>三个小列表代表三行，每个列表的八个元素代表八个维度</strong>。</p><blockquote><p>这里举个小栗子帮助理解一下维度：</p><p>我们在三年级二班给各位同学做信息登记，每个人所需要填写【姓名、年龄、身高、体重】四项内容，我们把每个人的信息记为一条数据，那么我们就可以说这条数据有四个特征，它的维度为四。</p><p>通常来讲我们把特征记为<strong>feature</strong>，称这条数据有四个特征。</p><p>现在整个班级的信息都填写好了应该是如何的形状，我们假设有32个人：</p><p>【【张三、7、130、 70】</p><p>​【李四、7、131、 71】</p><p>​  。。。</p><p>​【李小明、7、129、70】】 如何我们得到一个32行4列的矩阵，记为（32,4）</p><p>接下来我们把视角拉倒整个三年级，我们假设有7个班级：</p><p>那么我们得到的数据维度应该是（7,32,4），表示我们有7个班级的数据，每个班级的数据维度（32,4）。</p><p>此后我们都称这个“班级为<strong>batch</strong>“ ，至此我们从二维的矩阵上升到三维的张量。</p></blockquote><h3 id="1-1-2-张量"><a href="#1-1-2-张量" class="headerlink" title="1.1.2 张量"></a>1.1.2 张量</h3><p>下面我们将介绍常用的张量形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先介绍下一张图片通常的数据格式，我们使用Numpy来伪造一下数据</span></span><br><span class="line"></span><br><span class="line">np.random.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[[[-1.03301822, -0.26956785, -1.81246034, ..., -0.2025034 ,</span></span><br><span class="line"><span class="string">          -0.24770628,  0.45183312],</span></span><br><span class="line"><span class="string">         [ 1.09102807,  0.92955389,  0.07537901, ...,  0.69203358,</span></span><br><span class="line"><span class="string">          -0.17726632, -0.74610015],</span></span><br><span class="line"><span class="string">         [ 0.40508712, -1.2507095 ,  0.68702445, ..., -0.12526432,</span></span><br><span class="line"><span class="string">           0.0390443 ,  1.00993313],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 1.96843042, -2.43286678,  0.08543089, ..., -1.57232148,</span></span><br><span class="line"><span class="string">           0.92844287, -0.25532137],</span></span><br><span class="line"><span class="string">         [ 0.46919141, -0.13700029,  1.78645959, ...,  0.01334257,</span></span><br><span class="line"><span class="string">           1.31030895, -0.22523819],</span></span><br><span class="line"><span class="string">         [ 0.63897933,  0.54846445, -0.64030391, ...,  0.92298892,</span></span><br><span class="line"><span class="string">          -0.50840421,  1.34232325]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.01892086,  0.1456131 , -0.08903806, ...,  1.68250139,</span></span><br><span class="line"><span class="string">           1.2097305 , -0.2680935 ],</span></span><br><span class="line"><span class="string">         [ 0.92759263,  0.22665021,  1.28734004, ...,  0.09925943,</span></span><br><span class="line"><span class="string">           1.30039407,  3.34710594],</span></span><br><span class="line"><span class="string">         [ 0.53486942, -0.56230181, -1.92117215, ...,  1.33047469,</span></span><br><span class="line"><span class="string">          -1.19211895, -0.03081918],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 0.2539067 , -2.13160564,  0.27519544, ..., -0.62223126,</span></span><br><span class="line"><span class="string">           0.5818296 ,  0.07102949],</span></span><br><span class="line"><span class="string">         [-0.7524386 , -0.71244818,  0.88997093, ...,  0.16566338,</span></span><br><span class="line"><span class="string">           0.80577231, -3.35350436],</span></span><br><span class="line"><span class="string">         [ 0.99558393, -2.32335969, -2.87512549, ...,  1.16290939,</span></span><br><span class="line"><span class="string">           2.24089232,  0.22083378]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.35970859,  0.7961136 ,  0.09896652, ...,  1.82609401,</span></span><br><span class="line"><span class="string">          -0.49607535,  0.23424012],</span></span><br><span class="line"><span class="string">         [-0.22283053, -1.35535905, -0.55896315, ...,  1.68093489,</span></span><br><span class="line"><span class="string">           0.80969216,  0.63538616],</span></span><br><span class="line"><span class="string">         [-0.88285682,  0.59389887, -1.05559301, ..., -0.00719476,</span></span><br><span class="line"><span class="string">          -0.25654492, -1.40716977],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 0.44508688, -0.05650302, -2.97674436, ...,  1.25730001,</span></span><br><span class="line"><span class="string">          -1.66409024,  0.96057644],</span></span><br><span class="line"><span class="string">         [-1.3237267 , -0.27798159, -1.8947621 , ...,  1.96216661,</span></span><br><span class="line"><span class="string">          -0.10569547, -0.8446272 ],</span></span><br><span class="line"><span class="string">         [ 0.22525617,  0.75040916,  0.72823974, ..., -1.93525763,</span></span><br><span class="line"><span class="string">          -0.74464397,  0.55771249]]]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>上面就是一张图片**W(width)<strong>为28，</strong>H(heigh)**为28，有RGB三个通道，batch为1的图片(这个batch里面只有一张图片)的数据表示形式。</p><p>当然上面的数据太过复杂，我们以下面W和H都为4的数据继续讲解一下各个数据的意义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;array([[[[ 0.43748082, -0.65000689,  0.13972451, -0.40213376],</span></span><br><span class="line"><span class="string">            [ 0.09342289, -0.83655856,  0.51844492,  0.96505144],</span></span><br><span class="line"><span class="string">            [ 0.68421876,  1.05527391, -0.30821748, -1.89826909],</span></span><br><span class="line"><span class="string">            [-0.36654524,  0.22642376,  0.16545107,  0.00401234]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [[-0.13032482,  0.68182741, -0.52511016,  0.75875314],</span></span><br><span class="line"><span class="string">             [-1.39072336, -0.22848391, -1.64733525,  0.3339502 ],</span></span><br><span class="line"><span class="string">             [-1.06568103, -0.58455172, -0.02874822, -0.64499225],</span></span><br><span class="line"><span class="string">             [-0.23380602, -0.74809941, -0.71214339, -0.44950305]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            [[-1.51112191,  0.49145194, -0.01839728, -1.52788219],</span></span><br><span class="line"><span class="string">             [ 0.93370593,  0.96444176, -0.67434299, -1.8492484 ],</span></span><br><span class="line"><span class="string">             [ 0.51140855, -0.58682968, -1.16261225, -0.65782238],</span></span><br><span class="line"><span class="string">             [ 0.8643421 ,  0.79983446, -0.92330871, -2.45649675]]]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><p>一号位<strong>batch</strong>&#x3D;1表示只有一张图片</p><ul><li>若batch&#x3D;3，我们下面降到的模型依次取本批次内【0】号(3,4,4)、【1】(3,4,4)、【2】(3,4,4)进行处理</li></ul></li><li><p>二号位<strong>Channel</strong>&#x3D;3 表示有三个通道分别是RGB</p><ul><li>如上面 0.43748082….0.00401234，就表示在R通道内，这张图片的颜色数据</li><li>G和B通道同理，让三者叠加就可以表示颜色的明暗，从而勾勒画面，渲染颜色</li></ul></li><li><p>最后的两位就表示长宽，每个元素表示像素点的明暗程度。如R通道的第一个元素0.43748082就表示这个像素点有多红</p><p>(红也有程度对吧)</p></li></ul><h2 id="1-2-Pytorch中tensor的API"><a href="#1-2-Pytorch中tensor的API" class="headerlink" title="1.2 Pytorch中tensor的API"></a>1.2 Pytorch中tensor的API</h2><p>Pytorch中tensor号称是跟Numpy及其相似的操作方式，有Numpy的学习基础的话几乎不用付出学习成本来适应tensor。但是实际情况就经常出现各种警告。无论如何，tensor可以享受到GPU的加速运算，总的来说也够友好，下面我们将介绍其常用的API</p><p>首先是随机函数，基本跟Numpy是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Random Tensor:</span></span><br><span class="line"><span class="string"> tensor([[0.7453, 0.7993, 0.8484],</span></span><br><span class="line"><span class="string">        [0.3592, 0.3243, 0.7226]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ones Tensor:</span></span><br><span class="line"><span class="string"> tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Zeros Tensor:</span></span><br><span class="line"><span class="string"> tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>接下来介绍tensor对象的一些属性，其中device默认就是使用cpu，表示我们数据在cpu上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8165, 0.1909, 0.6631, 0.3062],</span></span><br><span class="line"><span class="string">           [0.0178, 0.5158, 0.0267, 0.9819],</span></span><br><span class="line"><span class="string">           [0.6103, 0.7354, 0.7933, 0.2770]]) &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">tensor.shape   <span class="comment"># 将返回 torch.Size([3, 4])</span></span><br><span class="line">tensor.dtype<span class="comment"># 将返回 torch.float32</span></span><br><span class="line">tensor.device<span class="comment"># 将返回 cpu</span></span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span><span class="comment"># 这句话经常来指定数据处理的设备</span></span><br></pre></td></tr></table></figure><p>torch.cat也是一个常用的函数，用来链接数据。</p><p>下面以第一行为例，cat函数将<code>[0.8165, 0.1909, 0.6631, 0.3062]</code>与<code>[0.8165, 0.1909, 0.6631, 0.3062]</code>、<code>[0.8165, 0.1909, 0.6631, 0.3062,]</code>连接，这是因为dim&#x3D;1表示在第一维度，其视角内的可操作单位为<code>0.8165, 0.1909, 0.6631, 0.3062</code>这些元素，dim&#x3D;0则可操作的基本单位为tensor(这里的tensor表示上面的三行四列的实例张量)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8165, 0.1909, 0.6631, 0.3062, 0.8165, 0.1909, 0.6631, 0.3062, 0.8165,</span></span><br><span class="line"><span class="string">         0.1909, 0.6631, 0.3062],</span></span><br><span class="line"><span class="string">        [0.0178, 0.5158, 0.0267, 0.9819, 0.0178, 0.5158, 0.0267, 0.9819, 0.0178,</span></span><br><span class="line"><span class="string">         0.5158, 0.0267, 0.9819],</span></span><br><span class="line"><span class="string">        [0.6103, 0.7354, 0.7933, 0.2770, 0.6103, 0.7354, 0.7933, 0.2770, 0.6103,</span></span><br><span class="line"><span class="string">         0.7354, 0.7933, 0.2770]]) &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>另外介绍一下常用的类型转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">n = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">t.to_list()<span class="comment"># 将tensor类型转换为list</span></span><br><span class="line">t.numpy()<span class="comment"># 转换为Numpy类型</span></span><br><span class="line">torch.from_numpy(n)<span class="comment"># 从Numpy转换为tensor</span></span><br></pre></td></tr></table></figure><p>最后最常用的就是下面两句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">torch.tensor(data) <span class="comment"># 返回tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])可使用dtype=torch.float32换成浮点数</span></span><br><span class="line">torch.Tensor(data)<span class="comment"># 返回 tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])</span></span><br></pre></td></tr></table></figure><h1 id="2-Dataset-DataLoader"><a href="#2-Dataset-DataLoader" class="headerlink" title="2. Dataset DataLoader"></a>2. Dataset DataLoader</h1><p> 都说数据科学家一般的时间都花在数据处理上，一点不假。前面花了这么大篇幅讲价tensor，接下来我们将介绍Pytorch中存储数据的</p><p>Dataset和数据加载器DataLoader</p><h2 id="2-1-你的数据类"><a href="#2-1-你的数据类" class="headerlink" title="2.1 你的数据类"></a>2.1 你的数据类</h2><p>虽然我们使用的MNIST数据集已经可以直接通过Pytorch的API调用，如下</p><p><code>from torchvision import datasets</code></p><p><code>datasets.MNIST(root=&#39;../dataset/mnist/&#39;, train=True, download=True, transform=transform)</code></p><blockquote><p>root表示存储或者加载数据的路径</p><p>train表示是否只加载训练部分的数据集，不设定默认加载全部数据集</p><p>download字面意思</p><p>transform指代这批数据使用什么转换形式，一般来说是一种数据增强方式，以后会专门介绍</p></blockquote><p>我们还是来具体解释下通常要自定义使用的dataset。</p><p>定义符合你要求的数据集有三步必须操作：</p><ul><li>定义你自己的数据集类并继承自<code>torch.utils.data.Dataset</code></li><li>需要包含<code>__len__</code>方法返回长度</li><li>需要包含<code>__getitem__</code>方法，按照下标取得数据</li></ul><p>以上配置也都是为了配合DataLoader的使用，下面我们定义一个dataset类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TitanicDataSets</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,flag</span>):</span><br><span class="line">        xy = preprocess(pd.read_csv(<span class="string">&quot;Titanic.csv&quot;</span>), flag=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> flag == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.x_data = xy.iloc[:, :-<span class="number">1</span>][:<span class="number">800</span>]</span><br><span class="line">            self.y_data = xy.iloc[:, -<span class="number">1</span>][:<span class="number">800</span>]</span><br><span class="line">            self.<span class="built_in">len</span> = self.x_data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> flag == <span class="string">&quot;valid&quot;</span>:</span><br><span class="line">            self.x_data = xy[<span class="number">0</span>][<span class="number">800</span>:<span class="number">892</span>]</span><br><span class="line">            self.y_data = xy[<span class="number">1</span>][<span class="number">800</span>:<span class="number">892</span>]</span><br><span class="line">            self.<span class="built_in">len</span> = self.x_data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br></pre></td></tr></table></figure><p>上面我们引入kaggle著名的<a href="https://www.kaggle.com/competitions/titanic/data">泰坦尼克号幸存者预测比赛使用的数据集</a>，其中x_data获得的是前八百行乘客的信息，y_data记录的就是是否存活。</p><blockquote><p>一般来说我们也<strong>将数据集分为train和valid两部分</strong>，因为最后我们需要预测的数据集并不会有是否存活的标签，所以<strong>通过训练模型参数以拟合train部分的数据，以valid为本次训练的结果导向以修正模型参数</strong>，最终预测，就是我们的目的。</p></blockquote><p>如上面所示，Python中的语句就是这么简洁明了，我们在初始部分读取数据集，然后根据传入的flag决定是处理train还是valid的部分数据，最后我们赋予这个类像列表那样的获取下标和切片能力(<code>__getitem__</code>方法)、以及返回长度的能力(<code>__len__</code>方法)</p><h2 id="2-2-数据加载器"><a href="#2-2-数据加载器" class="headerlink" title="2.2 数据加载器"></a>2.2 数据加载器</h2><p>Pytorch的数据加载器DataLoader简单易用，下面介绍它部分常用参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = TitanicDataSets(flag=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>dataset 表示它所处理的数据，一般是你定义的dataset类，或者具有下标取值，和返回长度的数据类型也可以</li><li>batch_size 表示一词传给模型多少条数据</li><li>shuffle 表示是否打乱</li><li>num_workers 表示使用你cpu的几个核进行读取</li></ul><p>可以使用下面的语句查看dataloader返回给你的数据形状</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">samples = next(iter(train_loader))</span><br><span class="line">samples[:2]# 查看本批次(batch)的前两个样本([0]号，[1]号)</span><br></pre></td></tr></table></figure><h1 id="3-模型"><a href="#3-模型" class="headerlink" title="3. 模型"></a>3. 模型</h1><p>对于模型，以我的理解，数据虽然是死的，但是理解它的方式是活的；模型是活的，但是组合它的方式并没有那么灵活。这里之所以说是组合，说点题外话，是因为如今预训练模型大行其道，大模型在各个任务上不断刷新纪录(SOTA)，小型机构很难有力量去训练这种大模型，于是在大模型上修修改改以适应下游任务的方式，只能使用这种像是Transformer的方式不断变形组合，总感觉缺了点活力。(奠定Pre-train的Bert就是在Transformer基础上提出来的)。</p><h2 id="3-1-模型定义"><a href="#3-1-模型定义" class="headerlink" title="3.1 模型定义"></a>3.1 模型定义</h2><p>下面开始定义我们的CNN模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># flatten data from (n,1,28,28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># -1 此处自动算出的是320</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><ul><li>初始化部分就是定义这个模型的各种方法</li><li>forward向前传播，应用初始化部分定义的函数</li></ul><p>因为MNIST数据集是黑白图片所以只有一个通道(以灰度grey刻画图像即可)</p><h2 id="3-2-模型功能细节"><a href="#3-2-模型功能细节" class="headerlink" title="3.2 模型功能细节"></a>3.2 模型功能细节</h2><p><code>torch.nn.Conv2d</code>即convolution（卷积层）</p><ul><li><p>第一个参数表示进入卷积层数据的channel数</p></li><li><p>第二个参数表示完成卷积后数据的channel数</p></li><li><p>padding&#x3D;1 即在图形周围填充一圈为0的数据(一般来说是有些图形在某些情况下不padding将会取不到原本边界上的值)</p></li><li><p>kernel_size表示卷积核大小(3,3)，如图所示（5,5）的图形padding之后变为（7,7），其经过卷积核映射成（3,3）的形状</p></li></ul><p><img src="/../../article_img/Convolution_arithmetic_-_Padding_strides.gif"></p><blockquote><p>卷积核进行的操作是elementwise multiplication，就是元素与核上对应元素相乘之后加起来就可以了</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d">这里也请各位查看Pytoch文档查看更多参数的详细解释</a></p></blockquote><p><code>torch.nn.MaxPool2d</code></p><p>也就是在一个设定的核的窗口内取最大值</p><p><strong>注意maxpool就是为了取得此区域最大值作为特征输出给下一层的所以不会有overlap的地方</strong></p><p><img src="/../../article_img/maxpool.gif"></p><p><code>激活函数relu</code></p><p>直接上图，置于sigma函数、softmax函数、tanh函数之后会开文讲</p><p><img src="/../../article_img/1_DfMRHwxY1gyyDmrIAd-gjQ.png"></p><p><code>torch.nn.Linear</code></p><p>就是全连接层。</p><p>对于经过卷积、池化、激活的数据，维度为(batch_size, b, c, d)，</p><p>我们将其压缩为(batch_size, n) 最后送给全连接层做n到10的映射，最后变为(batch_size, 10)，以最大数的下标作为我们模型的预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如batch_size = 2</span></span><br><span class="line"></span><br><span class="line">tensor = torch.rand(<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[0.1022, 0.3252, 0.8618, 0.1433, 0.8307, 0.8538, 0.1535, 0.1760, 0.0021,0.9704],</span></span><br><span class="line"><span class="string"> [0.6809, 0.6555, 0.1134, 0.3555, 0.4866, 0.5923, 0.5204, 0.9048, 0.5630,0.4472]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">tensor.argmax(dim=-<span class="number">1</span>)   <span class="comment"># 获得 [9, 7] 第一个列表最大值的下标是9，第二个是7</span></span><br></pre></td></tr></table></figure><h1 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h1><h2 id="4-1-一般流程"><a href="#4-1-一般流程" class="headerlink" title="4.1 一般流程"></a>4.1 一般流程</h2><p>首先将模型实例化，并引入损失函数、优化器和设备。（关于损失函数和优化器之后也会开文讲）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Net()</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p>加载数据，这里函数名都是见名知意，很好理解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"> </span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>下面定义训练循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">       <span class="comment"># 注意这里因为是MNIST数据集，所以自动返回tensor类型，这才有to()方法</span></span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        target = target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"> </span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"> </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span>*correct/total))</span><br></pre></td></tr></table></figure><p>以上模型准确率在98%</p><h2 id="4-2-看看准不准"><a href="#4-2-看看准不准" class="headerlink" title="4.2 看看准不准"></a>4.2 看看准不准</h2><p>以下内容使用jupyter notebook查看</p><p>单个查看(train_loader就是上面流程中定义的)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_pred</span>():</span><br><span class="line">    samples = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line">    x = samples[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    pred = model(x).argmax(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred.item())</span><br><span class="line"></span><br><span class="line">    plt.imshow(x.squeeze().numpy())</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_pred()</span><br></pre></td></tr></table></figure><p><img src="/../../article_img/Mnist_pred.png"></p><p>批量查看(train_loader就是上面流程中定义的)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_batch_pred</span>():</span><br><span class="line">    samples = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line">    x = samples[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i, imgs <span class="keyword">in</span> <span class="built_in">enumerate</span>(x[:<span class="number">10</span>], <span class="number">0</span>):</span><br><span class="line">        npimg = imgs.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">10</span>, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(npimg, cmap=plt.cm.binary)</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    pred = model(x[:<span class="number">10</span>]).argmax(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(pred.numpy().tolist())</span><br><span class="line">    </span><br><span class="line">show_batch_pred()</span><br></pre></td></tr></table></figure><p><img src="/../../article_img/Mnist_batch.png"></p><p>可以看到还是错了一个的，倒数第三应该是4 (要不就是我看错了)</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>下面我们来总结一下训练一个模型的pipeline，我认为总结让我们的pipeline获得一定的泛化能力</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"> </span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># flatten data from (n,1,28,28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># -1 此处自动算出的是320</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> x </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练和验证</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        target = target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"> </span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"> </span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span>*correct/total))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(epochs)):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 模块定义，一般这里会加入超参数的定义</span></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">main(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><ul><li><p>回顾整个流程: 准备数据—&gt; 定义模型—&gt; 训练循环设计—&gt; 超参数—&gt; 训练并分析结果。各个环节细节的设计请各位参照<a href="https://pytorch.org/docs/stable/index.html">Pytoch官方文档研究</a></p></li><li><p>以上就是整个数据到模型到结果的流程，下节我们将介绍VGG、ResNet50等预训练模型</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Pytorch 基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python基础01 数据类型</title>
      <link href="/posts/12763.html"/>
      <url>/posts/12763.html</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文介绍Python中基本的数据类型：</p><ul><li>字符串、数字</li><li>列表</li><li>字典</li><li>集合</li><li>元组</li></ul><p>以及一些常用的处理小技巧。</p><h1 id="1-字符串、数字"><a href="#1-字符串、数字" class="headerlink" title="1. 字符串、数字"></a>1. 字符串、数字</h1><p>Python中字符串（str）的处理对于没有任何变成经验的同学可能有些苦恼，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">s1 = <span class="string">&#x27;1222&#x27;</span></span><br><span class="line">i = <span class="number">1222</span></span><br><span class="line"></span><br><span class="line">add_up = s1 + i <span class="comment"># 这段函数就会报错，因为无法将 int类型 与 str类型相加</span></span><br></pre></td></tr></table></figure><ul><li><p>int ：整数类型，将小数抹除</p><ul><li>float ：浮点数类型，因为二进制进位的关系数据并不准确的</li></ul></li><li><p>以上就是提醒各位，对数据处理的时候，一定要留心数据的类型</p></li></ul><h2 id="1-1-字符串中的序号"><a href="#1-1-字符串中的序号" class="headerlink" title="1.1 字符串中的序号"></a>1.1 字符串中的序号</h2><p>字符串的序号可以让你快速取得一串字符中任意位置的任意字符，有如下三种基本方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27;Attention Is A Talent&#x27;</span> <span class="comment"># 首先创建一个字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种 取单个字符</span></span><br><span class="line">s[<span class="number">0</span>] <span class="comment"># 将获得 &#x27;A&#x27;</span></span><br><span class="line">s[<span class="number">20</span>]<span class="comment"># 将获得 &#x27;t&#x27;  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种 取多个字符</span></span><br><span class="line">s[<span class="number">0</span>:<span class="number">2</span>]  <span class="comment"># 将获得 &#x27;At&#x27; </span></span><br><span class="line">s[:<span class="number">20</span>]<span class="comment"># 将获得 &#x27;Attention Is A Talen&#x27;</span></span><br><span class="line">s[::<span class="number">2</span>]  <span class="comment"># 将获得 &#x27;AtninI  aet&#x27;</span></span><br><span class="line">s[::<span class="number">1</span>]  <span class="comment"># 将获得 &#x27;Attention Is A Talent&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种 倒序</span></span><br><span class="line">s[-<span class="number">1</span>]   <span class="comment"># 将获得 &#x27;t&#x27; </span></span><br><span class="line">s[-<span class="number">21</span>:]<span class="comment"># 将获得 &#x27;Attention Is A Talent&#x27;</span></span><br></pre></td></tr></table></figure><p>有如下需要注意的几个地方：</p><ol><li><p>在python中我们使用引号包裹需要的字符串内容</p></li><li><p>字符串的序号是从 0 开始定义的，所以上面的s有21个字符，而<strong>方括号内</strong>的取值范围是[0,20], 细心你的肯定发现了，空格数也算进去了。没错<strong>空格也算一种特殊的字符</strong>。</p></li><li><p>第二种取值方式我们称之为<strong>切片</strong>，python中的切片方式等价数学上的**左闭右开~[x, y)**，上面字符s[:20]，是取得s[20]号位左边的全部值，但是不会包含s[20]。</p></li></ol><p>​当然是用s[0:20] 也是等价的。</p><p>​<strong>步长</strong> 即第二种方法的第三个式子，是用步长就是字面意思，每走n步取值。</p><p>​s[::2]就是 s[0]第一步， s[1]第二步(存储)，s[2]第三步，s[3]第四步(存储)….</p><ol start="4"><li><p>倒序是字符串的另一套序号，它有很多应用场景，比如定义一个很长的字符串你可能需要用s[1222222]才能取得这个值，但是是用s[-1]就很方便。</p><p>当然，需要注意<strong>倒序是从[-1]开始的</strong>。</p></li></ol><h2 id="1-2-特殊字符"><a href="#1-2-特殊字符" class="headerlink" title="1.2 特殊字符"></a>1.2 特殊字符</h2><p><code>&#39;\n&#39;(换行)    &#39;\b&#39;(回退)    &#39;\r&#39;(光标回到本行行首)  &#39;\t&#39;(相当于八个空格，两个table)</code></p><p>以上就是几个常见的特殊字符，其中特别需要注意的是路径中的斜杠如遇到 \nigger 计算机可能就无法明白你输入的是 ‘igger’，还是含有n的字符串。有以下两种处理方式：</p><ul><li><code>&#39;\\nigger&#39;</code> </li><li><code>r&#39;\nigger&#39;</code></li></ul><h2 id="1-3-字符串的运算以及常用函数"><a href="#1-3-字符串的运算以及常用函数" class="headerlink" title="1.3 字符串的运算以及常用函数"></a>1.3 字符串的运算以及常用函数</h2><p><strong>运算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1 + s2               <span class="comment"># 将两个字符串连接</span></span><br><span class="line">s1*n                  <span class="comment"># 将s1复制n次</span></span><br><span class="line">s1 <span class="keyword">in</span> s2              <span class="comment"># 如果s1是s2的字串 则返回 True 否 False</span></span><br></pre></td></tr></table></figure><p><strong>函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27; Attention,Is,A,Talent &#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split函数</span></span><br><span class="line">s.split(<span class="string">&#x27;,&#x27;</span>)<span class="comment"># 输出为 [&#x27; Attention&#x27;, &#x27;Is&#x27;, &#x27;A&#x27;, &#x27;Talent &#x27;]</span></span><br><span class="line">                   <span class="comment"># split函数以逗号为标志，返回一个分隔后的列表</span></span><br><span class="line"><span class="comment"># count函数                   </span></span><br><span class="line">s.count(<span class="string">&#x27;A&#x27;</span>)                    <span class="comment"># 统计A在s中的次数，本例中将返回int类型的2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># upper，lower函数</span></span><br><span class="line">s.upper() / s1.lower()          <span class="comment"># 将字符串转化为对应的大小写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replace函数</span></span><br><span class="line">s.replace(<span class="string">&#x27;tion&#x27;</span>, <span class="string">&#x27;&#x27;</span>)  <span class="comment"># 将字符串中的&#x27;tion&#x27;替换为&#x27;&#x27;，即没有东西，相当于删除</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># center函数</span></span><br><span class="line">s.center(<span class="number">30</span>, <span class="string">&#x27;=&#x27;</span>)               <span class="comment"># 将s放在中间，左右两侧平均填充等于号至总字符数为30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># strip函数</span></span><br><span class="line">s.strip(<span class="string">&#x27; &#x27;</span>)                    <span class="comment"># s两侧删除空格，以及其他不可读符号如&#x27;\n&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># join函数</span></span><br><span class="line"><span class="string">&#x27;,&#x27;</span>.join(s)                     <span class="comment"># s中每个字符间填充逗号</span></span><br><span class="line"><span class="comment"># 返回 &#x27;A,t,t,e,n,t,i,o,n,,,I,s,,,A,,,T,a,l,e,n,t&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># len函数</span></span><br><span class="line"><span class="built_in">len</span>(s)<span class="comment"># 返回s的长度</span></span><br></pre></td></tr></table></figure><ol><li>上面我们以逗号为分隔符使用split函数，但是注意<strong>中英文的逗号是有区别的</strong>，其他有些符号也一样，需要注意。</li><li>上述中的replace几乎可以代替center函数，但是注意<strong>strip只能处理字符串的两端</strong></li><li>如join函数返回的结果，再次提醒空格也算是字符</li><li>最后，<strong>上述操作产生都是一个新的对象</strong>，即调用s后返回的是原本的字符串，并不是函数作用后的结果。<br>需要<code>s = s.replace(&#39;tion&#39;, &#39;&#39;)  </code> 赋值才‘生效’。</li></ol><h2 id="1-4-数字函数"><a href="#1-4-数字函数" class="headerlink" title="1.4 数字函数"></a>1.4 数字函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pow</span>(x, n)    <span class="comment">#为x的n次方</span></span><br><span class="line"><span class="built_in">divmod</span>(<span class="number">10</span>, <span class="number">3</span>)    <span class="comment"># 输出为（3， 1）</span></span><br><span class="line"><span class="built_in">abs</span>()       <span class="comment">#返回值为绝对值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">int</span>(<span class="number">12.34</span>)   <span class="comment">#输出 12</span></span><br><span class="line"><span class="built_in">float</span>(<span class="number">12</span>), <span class="built_in">float</span>(<span class="string">&#x27;12.23&#x27;</span>)  <span class="comment">#输出为 12.0 和 12.23</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">round</span>(<span class="number">1.2345</span>， <span class="number">2</span>)   <span class="comment">#保留两位小数</span></span><br><span class="line"><span class="built_in">max</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)   <span class="comment">#返回值为3</span></span><br><span class="line"><span class="built_in">min</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)   <span class="comment">#返回值为1</span></span><br></pre></td></tr></table></figure><h2 id="1-5-格式化字符串"><a href="#1-5-格式化字符串" class="headerlink" title="1.5 格式化字符串"></a>1.5 格式化字符串</h2><p>在我们得到一个数据之后，经常需要对其做保留多少位小数、居中打印、靠右输出、等操作，我们一般叫使其格式化。在python中有三种格式化填充字符串的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># format方式</span></span><br><span class="line">a, b, c = <span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span>, <span class="number">0.12222</span></span><br><span class="line"><span class="string">&#x27;Attention &#123;a&#125; A &#123;b&#125; version &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(a, b, c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将得到如下输出 &#x27;Attention Is A Talent version0.12&#x27;</span></span><br></pre></td></tr></table></figure><p>format格式化将按照顺序填入上面字符串{}的空位，{:.2f}表示此处保留两位小数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f方式</span></span><br><span class="line">a, b, c = <span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span>, <span class="number">0.12222</span></span><br><span class="line"><span class="string">f&#x27;Attention <span class="subst">&#123;a&#125;</span> A <span class="subst">&#123;b&#125;</span> version <span class="subst">&#123;c:<span class="number">.2</span>f&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将得到如下输出 &#x27;Attention Is A Talent version0.12&#x27;</span></span><br></pre></td></tr></table></figure><p>f格式化就是对format方式的简化版</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %方式</span></span><br><span class="line">a, b, c = <span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span></span><br><span class="line"><span class="string">&#x27;Attention %s A %s &#x27;</span>% (<span class="string">&#x27;Is&#x27;</span>, <span class="string">&#x27;Talent&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将得到如下输出 &#x27;Attention Is A Talent &#x27;</span></span><br></pre></td></tr></table></figure><p>这种方式很老了，推荐使用f方式，非常简洁。</p><p><strong>以下不是必看内容：format填充方式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chr</span>(Unicode)<span class="comment">#返回Unicode对应的字符</span></span><br><span class="line"><span class="built_in">ord</span>(<span class="string">&#x27;字&#x27;</span>)   <span class="comment">#返回对应的编码，如chr(ord(&#x27;a&#x27;)+i ) 即可遍历26字母</span></span><br></pre></td></tr></table></figure><p><strong>填充物若为chr(12222)，等特殊字符</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">chr</span>(<span class="number">12222</span>):^<span class="number">10</span>&#125;</span>&#x27;</span></span><br><span class="line"><span class="comment"># 输出为 &#x27;    ⾾     &#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">f&#x27;&#x27;</span>&#123;<span class="built_in">chr</span>(<span class="number">12222</span>):=^<span class="number">10</span>&#125;<span class="string">&#x27;</span></span><br><span class="line"><span class="string">&#x27;</span>====⾾=====<span class="string">&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">f&#x27;</span><span class="string">&#x27;&#123;chr(12222):=&gt;10&#125;&#x27;</span></span><br><span class="line"><span class="string">&#x27;=========⾾&#x27;</span></span><br></pre></td></tr></table></figure><p>如上{chr(12222):^10} 将chr(12222)对应的字符输出在中间，左右两侧填充空格。</p><p>也可用等号等其他符号填充，或者使用&gt;大于号使结果置右。</p><h1 id="2-列表"><a href="#2-列表" class="headerlink" title="2. 列表"></a>2. 列表</h1><p>列表跟上文中提到的字符串很像，或者说字符串是一种特殊的列表，其所有元素都是字符串。</p><p>python中的列表（list），在我的印象里几乎可以装任何的东西: 字符串、数字、甚至你定义的函数…</p><p><strong>列表是一种非常好用的数据类型，也是我们最常使用数据类型</strong>，以下概要对其简要介绍并补充几个<strong>判断符</strong>。</p><h2 id="2-1-概要"><a href="#2-1-概要" class="headerlink" title="2.1 概要"></a>2.1 概要</h2><p>列表同字符串也有<strong>正反序号下标，切片操作</strong>，不同的是列表可以含有各种类型的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="string">&#x27;Attention Is A Talent&#x27;</span>, <span class="number">100</span>, <span class="string">&#x27;% &#x27;</span>]</span><br><span class="line"></span><br><span class="line">ls[<span class="number">0</span>] == ls[-<span class="number">3</span>] <span class="comment"># 返回True</span></span><br><span class="line"><span class="string">&#x27;A&#x27;</span> <span class="keyword">in</span> ls<span class="comment"># 返回True</span></span><br><span class="line"><span class="built_in">str</span>(ls[<span class="number">1</span>]) + ls[<span class="number">2</span>] + ls[<span class="number">0</span>]<span class="comment"># &#x27;100% Attention Is A Talent&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><p>上面我们使用<strong>双等号</strong>作为判断符，等价询问python 是否 ls[0] &#x3D; ls[-3]</p><ul><li>还有 !&#x3D; 、&gt;&#x3D;、&lt;&#x3D;、等</li></ul></li><li><p>上面我们使用<strong>in</strong>作为判断词，等价询问python 是否 ‘A’ 在 ls</p><ul><li>还有 not in，or，and等</li></ul></li><li><p>第二点，我们使用了str()，它是一个函数，将对传给它的值做字符串化的处理</p><ul><li>int类型的 100 ——&gt; ‘100’ 即数字100变成字符串了</li></ul></li></ul><h2 id="2-2-列表的运算以及常用函数"><a href="#2-2-列表的运算以及常用函数" class="headerlink" title="2.2 列表的运算以及常用函数"></a>2.2 列表的运算以及常用函数</h2><p><strong>运算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="string">&#x27;Attention Is A Talent&#x27;</span>, <span class="number">100</span>, <span class="string">&#x27;% &#x27;</span>]</span><br><span class="line"></span><br><span class="line">ls * <span class="number">2</span> <span class="comment"># 将返回 [&#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;, &#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;]</span></span><br><span class="line"></span><br><span class="line">ls + ls[:<span class="number">1</span>]<span class="comment"># 将返回 [&#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;, &#x27;Attention Is A Talent&#x27;]</span></span><br></pre></td></tr></table></figure><ul><li>注意，<strong>列表的加法操作只能在列表跟列表之间</strong>。<ul><li>如上图使用 ls + ls[0] 将会报错</li></ul></li></ul><p><strong>函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面x表示单个元素、ls表示列表0号、ls1表示列表1号</span></span><br><span class="line"></span><br><span class="line">ls.append(x)<span class="comment"># 给ls尾添加x元素</span></span><br><span class="line">ls.remove(x)<span class="comment"># 将ls中出现的第一个x删除，如要删除所有x可以用（while+flag）或者set集合类型除重</span></span><br><span class="line">ls.extend(ls1)<span class="comment"># 将ls后面连接ls1</span></span><br><span class="line">ls.reverse()<span class="comment"># 将列表的元素逆置</span></span><br><span class="line">ls.insert(i,x)<span class="comment"># 在i位置 插入x</span></span><br><span class="line">ls.pop(i)<span class="comment"># i位置元素出栈，删除</span></span><br></pre></td></tr></table></figure><p>(这里加些列表复杂一点的方法，如果没有了解python中的字典、元组数据类型，可以在下文中了解后再看)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ls = [(<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;jerry&#x27;</span>, <span class="number">80</span>), (<span class="string">&#x27;mike&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;john&#x27;</span>, <span class="number">70</span>)]</span><br><span class="line">ls.sort()</span><br><span class="line">ls.sort(reverse = ture)<span class="comment"># 逆排序</span></span><br><span class="line">ls.sort(key= <span class="keyword">lambda</span> x:x[<span class="number">1</span>])<span class="comment"># 按值排序 </span></span><br><span class="line">-------------------------------</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(ls)<span class="comment"># 会自动把序列从小到大排序</span></span><br><span class="line"><span class="built_in">sorted</span>(ls, reverse = true)</span><br><span class="line"><span class="built_in">sorted</span>(ls, <span class="keyword">lambda</span> x:x[<span class="number">0</span>])<span class="comment"># 以lambda函数作为值排序</span></span><br><span class="line">-------------------------------</span><br><span class="line"></span><br><span class="line">seasons = [<span class="string">&#x27;Spring&#x27;</span>, <span class="string">&#x27;Summer&#x27;</span>, <span class="string">&#x27;Fall&#x27;</span>, <span class="string">&#x27;Winter&#x27;</span>]<span class="comment"># enumerate()的对象必须是可以迭代的类型(iterable)</span></span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">enumerate</span>(seasons))</span><br><span class="line">[(<span class="number">0</span>, <span class="string">&#x27;Spring&#x27;</span>), (<span class="number">1</span>, <span class="string">&#x27;Summer&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;Fall&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;Winter&#x27;</span>)]</span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">enumerate</span>(seasons, start=<span class="number">1</span>))</span><br><span class="line">[(<span class="number">1</span>, <span class="string">&#x27;Spring&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;Summer&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;Fall&#x27;</span>), (<span class="number">4</span>, <span class="string">&#x27;Winter&#x27;</span>)]</span><br></pre></td></tr></table></figure><ul><li><p>第一部分中为<strong>ls的sort方法</strong>配置参数</p><ul><li>reverse 表示逆置，如果对象没有‘大小’，则按照原来的顺序直接逆置</li><li>key 参数表示排序根据此值的大小，这里我们就是以每个元组的第二个值作为value排序</li></ul></li><li><p>第二部分是使用<strong>python中的sorted和sort函数</strong></p><ul><li><p>第一个参数表示传入的可迭代数据类型(就是列表这种含有很多元素，可以一个一个出来的数据类型)</p></li><li><p>第二个参数 同上面的key</p></li><li><blockquote><p><strong>sort 与 sorted 区别：</strong></p><p>sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。</p><p>list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。</p></blockquote></li></ul></li><li><p>第三部分使用了<strong>enumerate函数</strong>，这个函数主要是为元素添加下标，方便一些特殊场景处理</p><ul><li>enumerate函数返回一个含有位置下标的元组类型，为(index，element)形式</li></ul></li></ul><h2 id="2-3-列表应用的例子"><a href="#2-3-列表应用的例子" class="headerlink" title="2.3 列表应用的例子"></a>2.3 列表应用的例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="string">&#x27;Alice&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>]</span><br><span class="line"><span class="built_in">list</span>(lt) = ls[<span class="number">0</span>]<span class="comment"># [0]号为字符，导入list中会分割成[&#x27;a&#x27;, &#x27;l&#x27;, &#x27;i&#x27;, &#x27;c&#x27;, &#x27;e&#x27;]</span></span><br><span class="line">------------------------------------</span><br><span class="line">ls = [<span class="string">&#x27;Ali:ce&#x27;</span>, <span class="string">&#x27;Bo:b&#x27;</span>]</span><br><span class="line">lt = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ls:</span><br><span class="line">    elem = i.split(<span class="string">&#x27;:&#x27;</span>)[-<span class="number">1</span>]<span class="comment"># ls[i]为字符串使用split分割-&gt;[&#x27;Ali&#x27;,&#x27;ce&#x27;]取[-1]</span></span><br><span class="line">    lt.append(elem)<span class="comment"># 直接+=会变成[&#x27;c&#x27;,&#x27;e&#x27;],所以使用list的append,则直接将str的ce加入</span></span><br><span class="line"><span class="comment"># lt = [ce, b]  干净的字符串列表</span></span><br></pre></td></tr></table></figure><h1 id="3-字典"><a href="#3-字典" class="headerlink" title="3. 字典"></a>3. 字典</h1><p>Python字典（dict）是另一种<strong>可变容器模型</strong>,可存储任意类型对象。如字符串、数字、元组等其他容器模型<br><strong>因为字典是无序的所以不支持索引和切片</strong></p><p>注意：</p><ul><li>key不可以重复,否则只会保留第一个;</li><li>value值可以重复;</li><li>key可以是任意的数据类型,但不能出现可变的数据类型,保证key唯一;</li><li>key一般形式为字符串。</li></ul><h2 id="3-1-基本属性"><a href="#3-1-基本属性" class="headerlink" title="3.1 基本属性"></a>3.1 基本属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dic.keys()<span class="comment"># 返回字典中所有的key</span></span><br><span class="line">dic.values()<span class="comment"># 返回包含value的列表</span></span><br><span class="line">dic.items()<span class="comment"># 返回包含(键值,实值)元组的列表</span></span><br></pre></td></tr></table></figure><h2 id="3-2-基本函数"><a href="#3-2-基本函数" class="headerlink" title="3.2 基本函数"></a>3.2 基本函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dic.setdefault(k,value)</span><br><span class="line"><span class="comment">#如果key值存在,那么返回对应字典的value,不会用到自己设置的value;</span></span><br><span class="line"><span class="comment">#如果key值不存在.返回None,并且把新设置的key和value保存在字典中;</span></span><br><span class="line"><span class="comment">#如果key值不存在,但设置了value,则返回设置的value;</span></span><br><span class="line"></span><br><span class="line">dic.get(k,value)</span><br><span class="line"><span class="comment">#如果key值存在,那么返回对应字典的value,不会用到自己设置的value;</span></span><br><span class="line"><span class="comment">#如果key值不存在.返回None,但是不会把新设置的key和value保存在字典中;</span></span><br><span class="line"><span class="comment">#如果key值不存在,但设置了value,则返回设置的value;</span></span><br><span class="line"></span><br><span class="line">dic.items()</span><br><span class="line"><span class="comment">#打印字典中的所有元组</span></span><br></pre></td></tr></table></figure><p><strong>以下不是必看内容：dic.get(key, init_value)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ls = [(<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;tom&#x27;</span>, <span class="number">95</span>), (<span class="string">&#x27;jerry&#x27;</span>, <span class="number">80</span>), (<span class="string">&#x27;mike&#x27;</span>, <span class="number">99</span>), (<span class="string">&#x27;john&#x27;</span>, <span class="number">70</span>)]</span><br><span class="line"></span><br><span class="line">ls1,dic = [], &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> ls:</span><br><span class="line">    ls1.append(i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> ls1:</span><br><span class="line">dic[i] = dic.get(i, <span class="number">0</span>) + <span class="number">1</span>         </span><br><span class="line">dic</span><br></pre></td></tr></table></figure><p>get第一个参数为对应的键，第二个参数为键对应的初始值。</p><p>get方法将键对应的值初始化为0，以后每见一次加一次，在文本统计时经常使用。</p><p>相对于dic[i]，dic.get(i)不会报错，而它每见一次加一次而不是覆盖，明显速度会比前者慢。(但是不是大量数据都差不多。)</p><h1 id="4-集合"><a href="#4-集合" class="headerlink" title="4.集合"></a>4.集合</h1><p>在Python中集合（set）元素之间无序，<strong>每个元素唯一</strong>，不存在相同元素<strong>集合元素不可更改</strong>，不能是可变数据类型</p><h2 id="4-1-创建和运算"><a href="#4-1-创建和运算" class="headerlink" title="4.1 创建和运算"></a>4.1 创建和运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">使用两种方式建立</span><br><span class="line">A = &#123;<span class="string">&quot;python&quot;</span>, <span class="number">123</span>, (<span class="string">&quot;python&quot;</span>,<span class="number">123</span>)&#125; <span class="comment"># 使用&#123;&#125;建立集合</span></span><br><span class="line">输出为&#123;<span class="number">123</span>, <span class="string">&#x27;python&#x27;</span>, (<span class="string">&#x27;python&#x27;</span>, <span class="number">123</span>)&#125;</span><br><span class="line">B = <span class="built_in">set</span>(<span class="string">&quot;pypy123&quot;</span>) <span class="comment"># 使用set()建立集合</span></span><br><span class="line">输出为&#123;<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;y&#x27;</span>&#125;</span><br><span class="line">C = &#123;<span class="string">&quot;python&quot;</span>, <span class="number">123</span>, <span class="string">&quot;python&quot;</span>,<span class="number">123</span>&#125;<span class="comment"># 去重</span></span><br><span class="line">输出为&#123;<span class="string">&#x27;python&#x27;</span>, <span class="number">123</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">S | T 并，返回一个新集合，包括在集合S和T中的所有元素 </span><br><span class="line">S - T 差，返回一个新集合，包括在集合S但不在T中的元素 </span><br><span class="line">S &amp; T 交，返回一个新集合，包括同时在集合S和T中的元素 </span><br><span class="line">S ^ T 补，返回一个新集合，包括集合S和T中的非相同元素 </span><br><span class="line">S &lt;= T 或 S &lt; T     返回<span class="literal">True</span>/<span class="literal">False</span>，判断S和T的子集关系 </span><br><span class="line">S &gt;= T 或 S &gt; T     返回<span class="literal">True</span>/<span class="literal">False</span>，判断S和T的包含关系</span><br></pre></td></tr></table></figure><h2 id="4-2-函数"><a href="#4-2-函数" class="headerlink" title="4.2 函数"></a>4.2 函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(s) <span class="comment">#返回序列s的长度，即元素个数</span></span><br><span class="line"><span class="built_in">min</span>(s) <span class="comment">#返回序列s的最小元素，s中元素需要可比较</span></span><br><span class="line"><span class="built_in">max</span>(s) <span class="comment">#返回序列s的最大元素，s中元素需要可比较</span></span><br><span class="line">s.index(x) / s.index(x, i, j)     <span class="comment">#返回序列s从i开始到j位置中第一次出现元素x的位置</span></span><br><span class="line">s.count(x) <span class="comment">#返回序列s中出现x的总次数</span></span><br></pre></td></tr></table></figure><p>其中len()、min()、max()函数是内置的通用函数</p><h1 id="5-元组"><a href="#5-元组" class="headerlink" title="5. 元组"></a>5. 元组</h1><p>python中的元组（tuple）是一种序列类型，一旦创建就<strong>不能被修改</strong> ，使用<strong>小括号 () 或 tuple() 创建</strong>，元素间用逗号 , 分隔 。</p><h2 id="5-1-创建和取值"><a href="#5-1-创建和取值" class="headerlink" title="5.1 创建和取值"></a>5.1 创建和取值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">creature = <span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;tiger&quot;</span>,<span class="string">&quot;human&quot;</span></span><br><span class="line">creature = (<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;tiger&#x27;</span>, <span class="string">&#x27;human&#x27;</span>) <span class="comment"># 可以使用括号和不带括号的两两种</span></span><br><span class="line">color = (<span class="number">0x001100</span>, <span class="string">&quot;blue&quot;</span>, creature)<span class="comment"># 输出会得到（，，（））</span></span><br></pre></td></tr></table></figure><p>元组的取值操作跟列表一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">creature = <span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;tiger&quot;</span>,<span class="string">&quot;human&quot;</span></span><br><span class="line">creature[::-<span class="number">1</span>]<span class="comment"># 输出为(&#x27;human&#x27;, &#x27;tiger&#x27;, &#x27;dog&#x27;, &#x27;cat&#x27;)</span></span><br><span class="line">color = (<span class="number">0x001100</span>, <span class="string">&quot;blue&quot;</span>, creature)</span><br><span class="line">color[-<span class="number">1</span>][<span class="number">2</span>]<span class="comment"># 输出为&#x27;tiger&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="5-2-函数"><a href="#5-2-函数" class="headerlink" title="5.2 函数"></a>5.2 函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span>(x) <span class="comment"># 将其他类型变量x转变为集合类型</span></span><br><span class="line">t.discard(x)     <span class="comment"># 移除S中元素x，如果x不在集合S中，不报错</span></span><br><span class="line">t.remove(x) <span class="comment"># 移除S中元素x，如果x不在集合S中，产生KeyError异常</span></span><br><span class="line">t.pop() <span class="comment"># 随机返回S的一个元素，更新S，若S为空产生KeyError异常</span></span><br><span class="line">t.add(x) <span class="comment"># 如果x不在集合S中，将x增加到S</span></span><br><span class="line">t.clear() <span class="comment"># 移除S中所有元素</span></span><br><span class="line">x <span class="keyword">in</span> t<span class="comment"># 判断S中元素x，x在集合S中，返回True，否则返回False</span></span><br><span class="line">x <span class="keyword">not</span> <span class="keyword">in</span> t <span class="comment"># 判断S中元素x，x不在集合S中，返回True，否则返回False</span></span><br><span class="line">t.copy() <span class="comment"># 返回集合S的一个副本</span></span><br><span class="line"><span class="built_in">len</span>(t) <span class="comment"># 返回集合S的元素个数</span></span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>同上文我们定义的字符串、列表、字典，我们按照<strong>见名知意</strong>的原则，将其赋值给s、ls、dic。这样做是为了程序的可读性。今后我们在写程序的时候会定义很多变量，变量一多了就容易搞混了，顺藤摸瓜找着效率又很低，所以给你的变量取一个好名字，是一个很划算的决定。</p><ul><li>举个例子<code>train_df = pandas.DataFrame(&#39;../train.csv&#39;)</code>中的 train_df表示这个是个训练数据，其数据类型为pandas的DataFrame结构。</li></ul></li><li><p>此外在操作数据的时候一定要注意数据类型，根据数据的特性选择合适的数据类型。</p></li><li><p>最后，python最好用的地方在于有各种各样的库供你选择，而掌握它基础的数据结构是第一步，下面我们将讲解函数以及类，最后介绍两个常用的库，介绍一些如何快速掌握一个库的通法。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer &amp; Self-Attention</title>
      <link href="/posts/4330.html"/>
      <url>/posts/4330.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>失效图片处理</li></ul><p><a href="https://jalammar.github.io/illustrated-transformer/">阿三博客地址</a></p><ul><li><p><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=6997c0a04f6a78d03d30de86e9b949d9">李沐老师 48分钟讲解 encoder-decoder中(KV–Q)的运算</a>: </p><ul><li><p><strong>KQ相乘就是单个q对所有k的相似度作为attention score(给这个K值多少注意力)，与单个v做加权和(权值来自KQ)</strong></p><p>再通过<strong>注意力分数</strong>与<strong>V向量相乘</strong>，<strong>得到每个V应该多大的缩放</strong>， 进行相加后就得到了最终V应该是什么样子了</p></li></ul></li></ul><p><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=6997c0a04f6a78d03d30de86e9b949d9">李沐老师 56分 对multi-head输出和linear层相较于RNN的讲解</a>：</p><ul><li><p>词向量经过Attention层抓取全局信息，汇聚之后，在每个点上都有了所需要的信息</p><p>(权重不同，每个输出的向量的重点在不同的position编码位置上)，因此只需要做linear transformation。</p></li><li><p><strong>bert中transformer参数计算</strong>:</p></li></ul><blockquote><p>embedding: vocab_size&#x3D;30522, max_position_embeddings&#x3D;512, token_type_embeddings&#x3D;2(就进行两句分别标记，多了截断)</p><p>​（30522+512+2）*768 &#x3D; 23835648 (23M)</p><p>self-attention: 768&#x2F;12 &#x3D; 64 (多头每头分64维度的向量) ，64*768(每个64映射回768)，QKV三个矩阵, </p><p>​  最后一层 786(64 *12的拼接)-&gt;768的线性变换</p><p>​(768&#x2F;12 * 768 <em>3 ) * 12 + (768</em>768) &#x3D; 2359296</p><p>​经过12个transformer</p><p>​2359296*12 &#x3D; 28311552 (28M)</p><p>feedfoward: 自注意力层之后 分别在 encoder 和 decoder 中有个一个全连接层</p><p>​维度从 768-&gt;4*768_768-&gt;768</p><p>​(768*4 * 768 )*2 &#x3D; 4718592</p><p>​(768*4 * 768 )*2  * 12 &#x3D; 56623104 (56M)</p><p>layernorm: 有伽马和贝塔两个参数，embedding层（768 * 2），12层的self-attention，</p><p>​768 * 2 + 768 * 2 * 2 * 12 &#x3D; 38400</p><p>总计: 23835648+28311552+56623104+38400 &#x3D; 108808704      (108M)</p><p>每一层的参数为:  多头注意力的参数 + 拼接线性变换的参数 + feed-forward的参数 + layer-norm的参数</p><p>768 * 768 &#x2F; 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2 &#x3D; 7080960  (7M)</p></blockquote><h1 id="Encoder-编码阶段"><a href="#Encoder-编码阶段" class="headerlink" title="Encoder 编码阶段"></a>Encoder 编码阶段</h1><h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p>多头注意力机制将一个词向量留过八个 self-attention 头生成八个词向量 vector，</p><p>将八个词向量拼接，通过 fc 层进行 softmax 输出。</p><p>例如：</p><p>词向量为 (1,4) –&gt; </p><p>经过 QKV 矩阵(系数) 得到 (1,3) 八个 (1,3)*8 –&gt;</p><p>将输出拼接成 (8,3) 矩阵与全连接层的系数矩阵进行相乘再 softmax <strong>确定最后输出的</strong> 词向量 –&gt; (1,4)</p><h3 id="注意-QKV矩阵怎么来的-attention分数-，最后为什么要拼接，以及FC层的系数"><a href="#注意-QKV矩阵怎么来的-attention分数-，最后为什么要拼接，以及FC层的系数" class="headerlink" title="注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数"></a>注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数</h3><ol><li><p>qk相乘得到，词向量与其他词的attention分数( q1*(k1,k2,k3) )</p></li><li><p>多头注意力机制让一份词向量产生了多份答案，将每一份注意力机制的产物拼接，</p><p>获得了词向量在不同注意力矩阵运算后的分数，进行拼接后，softmax输出<strong>最注意的词</strong>，即是注意力机制。</p></li><li><p><strong>多头注意力机制，将向量复制n份(n为多头头数)，投影到如512&#x2F;8 &#x3D; 64的64维的低维空间，最后将每一层的输出结果</strong></p><p><strong>此处为八层，8*64&#x3D;512 拼回512维的输出数据</strong></p><p><strong>由于Scale Dot Product 只是做乘法点积(向量变成qvk之后的attention运算)，没什么参数，因此重点学习的参数在Multi-Head的线性变换中，</strong></p><p><strong>即将 64*8的八份数据线性变换的下文中的W0，给模型八次机会希望能够学到什么，最后在拼接回来。</strong>&#x3D;&#x3D;</p></li></ol><hr><p><strong>注意力机制流程</strong>：</p><p><strong>q –&gt; 查询向量</strong></p><p><strong>set( k，v)    k –&gt;关键字 v—-&gt; 值</strong></p><p><strong>如果 q对k的相似度很高，则输出v的概率也变高</strong></p><img src="https://jalammar.github.io/images/t/self-attention-output.png" alt="img" style="zoom: 80%;" /><p><strong>’多头’注意力机制</strong> </p><p>请注意并推演其<strong>词向量维度与系数矩阵带的行数</strong></p><p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="img"></p><hr><h3 id="Scale-Dot-Product"><a href="#Scale-Dot-Product" class="headerlink" title="Scale Dot Product"></a>Scale Dot Product</h3><p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="img"></p><p><strong>step1</strong></p><p>QK做点积，则输出每一行，是q与所有k的相乘相加结果，</p><p>α1 &#x3D; （q11k11+q12k21+q13k31 ,  q11k12+q12k22+q13k32 )</p><p>α2同理。</p><p><strong>step2</strong></p><p>所以得到了query1对所有key的相似度，最后每一行做个softmax进行概率分布。</p><p>除以根号dk是为了平滑梯度，具体来说：<strong>当概率趋近于1的时候softmax函数的梯度很小</strong>，<strong>除以dk让数值接近函数中部，梯度会比较陡峭</strong></p><p><strong>step3</strong></p><p>将第二步的结果与V相乘得到最后的输出</p><hr><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>位置编码是 将embedding好的词向量加上 position embedding vector 将<strong>信息融合，在注意力机制中进行计算</strong>。</p><p>(原文是使用sin cos将词向量份两部分进行编码， 本文中将交替使用sin cos，即单数sin 双数cos)</p><p>位置嵌入编码，主要是为了编辑定位<strong>词向量的位置</strong>以及<strong>词向量间的相对距离</strong></p><blockquote><p>pos为 词的种类数，为行标号</p><p>i 为特征维度</p><p>len(pos) * len(i)  表示为一position embedding 矩阵， 每一行为词的位置信息，每一列表示在特征上偏置，</p><p><strong>将位置信息 融入 词向量信息 使词获得 时间上的相对信息</strong></p></blockquote><p><a href="https://www.imagehub.cc/image/JqzAac"><img src="https://s1.imagehub.cc/images/2022/11/09/image638960e7f1096a03.png" alt="image638960e7f1096a03.png" border="0" /></a></p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png" alt="img" style="zoom: 67%;" /><img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png" alt="img"  /><h2 id="Residual-细节"><a href="#Residual-细节" class="headerlink" title="Residual 细节"></a>Residual 细节</h2><p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt="img"></p><h1 id="Decoder-解码阶段"><a href="#Decoder-解码阶段" class="headerlink" title="Decoder 解码阶段"></a>Decoder 解码阶段</h1><h3 id="Mask-Multi-head"><a href="#Mask-Multi-head" class="headerlink" title="Mask Multi-head"></a>Mask Multi-head</h3><p>与encoder不同的是，解码器在工作时会引入 <strong>Mask Multi-head 机制</strong>，将右侧的词盖住(设为负无穷或者别的)。</p><p>具体来说:</p><ol><li><p><strong>encoder 将生成的K和V矩阵</strong>传入 decoder 的 self-attention 模块中，而 <strong>decoder 将 mask 后的Q矩阵</strong>与其做attention。</p></li><li><p>mask做的事情</p><p><a href="https://www.imagehub.cc/image/JPWaZm"><img src="https://s1.imagehub.cc/images/2022/10/31/IMG_235920221004-111643.jpg" alt="IMG_235920221004-111643.jpg" border="0" /></a></p></li></ol><p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p><p>解码还是得一个个来的</p><p><strong>时间维度</strong> </p><p>在时间序列的情况下，词向量表示为，t1时刻的vector，t2时刻的vector….</p><p>mask做的事情就是将后面(右边)的 tn个时刻都屏蔽掉，</p><p>而Qmatrix的形成 将vector含有了其之后词的信息(共享了系数矩阵)，所以将其右边屏蔽。</p><p>则剔除了后面词的信息，从而不进行考虑。</p><h3 id="Mask-细节"><a href="#Mask-细节" class="headerlink" title="Mask 细节"></a>Mask 细节</h3><p>mask就是为了阻止词知道后面的信息，具体来说就是QKV矩阵还相乘，但是引入-inf来阻止右边(后面的信息汇聚)</p><p><a href="https://www.imagehub.cc/image/JP7vvk"><img src="https://s1.imagehub.cc/images/2022/10/31/1a35106e841b162c68e41ceb2d8aafb.jpg" alt="1a35106e841b162c68e41ceb2d8aafb.jpg" border="0" style="zoom:50%;" /></a></p><p><strong>第一次点积：</strong>将Q和K矩阵相乘得到attention分数，</p><p>将右上角置零就会得到只含有本身信息和相对位置之前(左边)的信息，</p><p>且<strong>第二次点积:</strong> Mask(QK)与V相乘由下三角矩阵的性质，</p><p><a href="https://www.imagehub.cc/image/JPuD0r"><img src="https://s1.imagehub.cc/images/2022/10/31/752a1f3305f3573aacc1c7b92395faf.jpg" alt="752a1f3305f3573aacc1c7b92395faf.jpg" border="0" style="zoom:50%;" /></a></p><img src="https://z3.ax1x.com/2021/04/20/c7w7rD.png#shadow" alt="img" style="zoom:50%;" /><p><strong>注: mask去负无穷是因为 SoftMax中 e的指数形式只有在负无穷才为零，</strong></p><p><strong>这样相乘数据不会有一点影响，取其他值，都会影响softmax</strong></p><img src="https://z3.ax1x.com/2021/04/20/c7w48x.png#shadow" alt="img"  /><p><img src="https://s1.ax1x.com/2020/07/12/U3FCQ0.png" alt="img"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>特别注意理解 attention机制将词向量之间的联系， <strong>attention分数</strong></li><li>embedding方式为 <strong>词向量+位置编码向量</strong></li><li>引入了 <strong>Residual</strong></li><li><strong>encoder-decoder层的传入</strong>为KV矩阵，decoder生成Q矩阵</li><li><strong>Mask方式</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Dive Into Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention机制</title>
      <link href="/posts/54367.html"/>
      <url>/posts/54367.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>失效图片处理</li></ul><p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">博客地址</a></p><h2 id="传统Seq2Seq"><a href="#传统Seq2Seq" class="headerlink" title="传统Seq2Seq"></a>传统Seq2Seq</h2><p>​<a href="https://www.imagehub.cc/image/JJyQxx"><img src="https://s1.imagehub.cc/images/2022/10/29/image9776ac96fcdc3b7e.png" alt="image9776ac96fcdc3b7e.png" border="0" /></a></p><p><a href="https://jalammar.github.io/images/seq2seq_4.mp4">动画连接</a></p><p>左侧为 input 将句子一个一个投入到 encoder 中，</p><p>encoder整个处理其相关性得到 context，吐给 decoder，</p><p>decoder 进行一个一个解码输出，得到整个翻译后的句子。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>An attention model differs from a classic sequence-to-sequence model in two main ways:</p><ul><li>First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes <em>all</em> the hidden states to the decoder:</li></ul><p>​<strong>注意力机制将产生的隐藏层信息(时间步骤信息)，全部保留，一次性传给 Decoder。</strong></p><img src="https://img-blog.csdnimg.cn/20181119222424704.gif" alt="img" style="zoom: 80%;" /><ul><li><p>Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:</p><ol><li><p>Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence</p></li><li><p>Give each hidden state a score (let’s ignore how the scoring is done for now)</p></li><li><p>Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores</p></li></ol></li></ul><p>​<strong>decoder 将 encoder 输入的隐藏层的 vector 进行打分得到一个分数vector，</strong></p><p>​<strong>将分数 vector 做 softmax，得到一个权重 vector，</strong></p><p>​<strong>将权重 vector 与隐藏层 vector 相乘得到 注意力 vector，</strong></p><p>​<strong>最后把注意力 vector 进行相加就完成了。</strong></p><p><a href="https://www.imagehub.cc/image/JJ8xIK"><img src="https://s1.imagehub.cc/images/2022/10/29/imageb79497ae8b6c83ca.png" alt="imageb79497ae8b6c83ca.png" border="0" /></a></p><ul><li>注意: 将 encoder 的隐藏层信息传入 decoder之后，decoder 每一步都将使用其传入的隐藏层信息做 attention。</li></ul><p><img src="https://img-blog.csdnimg.cn/20181119222700993.gif" alt="img"></p><p>​<strong>由上图可以看到，输出时 Attention 机制就是将注意力放在分数最高的向量上，所以，称之为’注意力机制’</strong></p>]]></content>
      
      
      <categories>
          
          <category> Dive Into Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF 02</title>
      <link href="/posts/45348.html"/>
      <url>/posts/45348.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>示例详解</li></ul><h3 id="Transformer分两块BERT-amp-GPT都很能打"><a href="#Transformer分两块BERT-amp-GPT都很能打" class="headerlink" title="Transformer分两块BERT&amp;GPT都很能打"></a>Transformer分两块BERT&amp;GPT都很能打</h3><ol><li><p>BERT用的是transformer的encoder</p><blockquote><p>BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。</p></blockquote></li><li><p>GPT用的是transformer的decoder</p><blockquote><p>在它之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。</p></blockquote></li></ol><h2 id="Bert的embedding"><a href="#Bert的embedding" class="headerlink" title="Bert的embedding"></a>Bert的embedding</h2><p>Embedding由三种Embedding求和而成：</p><ol><li><p>Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务</p><blockquote><p>BERT在第一句前会加一个[CLS]标志，<strong>最后一层该位对应向量可以作为整句话的语义表示</strong>，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更<strong>“公平”地融合文本中各个词的语义信息</strong>，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。</p></blockquote></li><li><p>Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</p></li><li><p>Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的</p></li></ol><h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><h3 id="tokenizer"><a href="#tokenizer" class="headerlink" title="tokenizer"></a>tokenizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># config模块</span></span><br><span class="line">MODEL_NAME=<span class="string">&quot;bert-base-chinese&quot;</span></span><br><span class="line">config = AutoConfig.from_pretrained(MODEL_NAME) <span class="comment">#c onfig可以配置模型信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizer模块</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)</span><br><span class="line"></span><br><span class="line">tokenizer.all_special_ids <span class="comment"># 查看特殊符号的id [100, 102, 0, 101, 103]</span></span><br><span class="line">tokenizer.all_special_tokens <span class="comment"># 查看token  [&#x27;[UNK]&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;]</span></span><br><span class="line"></span><br><span class="line">tokenizer.vocab_size <span class="comment"># 词汇表大小</span></span><br><span class="line">tokenizer.vocab <span class="comment"># 词汇对应的dict形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## tokening</span></span><br><span class="line">text=<span class="string">&quot;我在北京工作&quot;</span></span><br><span class="line">token_ids=tokenizer.encode(text)</span><br><span class="line">token_ids <span class="comment"># [101, 2769, 1762, 1266, 776, 2339, 868, 102]</span></span><br><span class="line">tokenizer.convert_ids_to_tokens(token_ids) <span class="comment"># [&#x27;[CLS]&#x27;, &#x27;我&#x27;, &#x27;在&#x27;, &#x27;北&#x27;, &#x27;京&#x27;, &#x27;工&#x27;, &#x27;作&#x27;, &#x27;[SEP]&#x27;]</span></span><br><span class="line">  <span class="comment"># convert_tokens_to_ids(tokens) 为对应方法</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">## padding 做向量填充</span></span><br><span class="line">token_ids=tokenizer.encode(text,padding=<span class="literal">True</span>,max_length=<span class="number">30</span>,add_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## encode_plus</span></span><br><span class="line">token_ids=tokenizer.encode_plus(</span><br><span class="line">    text,padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">    return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">    return_attention_mask=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="使用pre-train模型载入数据"><a href="#使用pre-train模型载入数据" class="headerlink" title="使用pre_train模型载入数据"></a>使用pre_train模型载入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model=AutoModel.from_pretrained(MODEL_NAME)</span><br><span class="line">outputs=model(token_ids[<span class="string">&#x27;input_ids&#x27;</span>],token_ids[<span class="string">&#x27;attention_mask&#x27;</span>])</span><br></pre></td></tr></table></figure><h3 id="数据集dataset定义"><a href="#数据集dataset定义" class="headerlink" title="数据集dataset定义"></a>数据集dataset定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,item</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        item 为数据索引，迭代取第item条数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        </span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,  <span class="comment">#转为tensor</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"><span class="comment">#print(encoding[&#x27;input_ids&#x27;])</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>:text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>:encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>:encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten(),</span><br><span class="line">            <span class="comment"># toeken_type_ids:0</span></span><br><span class="line">            <span class="string">&#x27;labels&#x27;</span>:torch.tensor(label,dtype=torch.long)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HuggingFace </tag>
            
            <tag> Bert </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF 01</title>
      <link href="/posts/45347.html"/>
      <url>/posts/45347.html</url>
      
        <content type="html"><![CDATA[<h1 id="待完成"><a href="#待完成" class="headerlink" title="待完成"></a>待完成</h1><ul><li>示例详解</li><li></li></ul><p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Attention 原文</a></p><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><ul><li><p>全面拥抱Transformer：NLP三大特征抽取器(CNN&#x2F;RNN&#x2F;TF)中，近两年新欢<strong>Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。</strong></p></li><li><p>像Wordvec出现之后一样，在人工智能领域种各种目标皆可向量化，也就是我们经常听到的“<strong>万物皆可Embedding</strong>”。而Transformer模型和Bert模型的出现，更是NLP领域划时代的产物：将<strong>transformer和双向语言模型进行融合</strong>，便得到NLP划时代的，也是当下在各自NLP下流任务中获得state-of-the-art的模型-BERT</p></li><li><p><strong>BERT起源于预训练的上下文表示学习</strong>，与之前的模型不同，BERT是一种深度双向的、无监督的语言表示，且仅使用纯文本语料库进行预训练的模型。<strong>上下文无关模型（如word2vec或GloVe）为词汇表中的每个单词生成一个词向量表示，因此容易出现单词的歧义问题</strong>。BERT考虑到单词出现时的上下文。例如，词“水分”的word2vec词向量在“植物需要吸收水分”和“财务报表里有水分”是相同的，但BERT根据上下文的不同提供不同的词向量，词向量与句子表达的句意有关。</p></li></ul><h3 id="Embedding："><a href="#Embedding：" class="headerlink" title="Embedding："></a>Embedding：</h3><p><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/bert_img/embedding.png" alt="img"></p><ul><li>首先类似 word2vec 的 token化，再进行片段标记( segment )，最后 ids 的位置编码(  position )</li><li>编码后一个 ’词‘ 有三个信息，token、段落位置信息、绝对位置信息( id: 1、2、3…)</li></ul><h4 id="Embedding解决的问题"><a href="#Embedding解决的问题" class="headerlink" title="Embedding解决的问题:"></a>Embedding解决的问题:</h4><ul><li>首先是之前用的 <strong>One-Hot Key</strong>，高维度，离散的，低信息密度的储存形式</li><li>其次是更好的 <strong>Contextual Similarity</strong>，上下文相关相似性。</li></ul><h2 id="Preview-Api"><a href="#Preview-Api" class="headerlink" title="Preview Api"></a>Preview Api</h2><h3 id="前置查看："><a href="#前置查看：" class="headerlink" title="前置查看："></a>前置查看：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>) <span class="comment"># 获取相应模型的tokenizer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForMaskedLM</span><br><span class="line">model = AutoModelForMaskedLM.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>) <span class="comment">#查看模型的分层</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="函数调用："><a href="#函数调用：" class="headerlink" title="函数调用："></a>函数调用：</h3><h4 id="字典大小，token化，ids化"><a href="#字典大小，token化，ids化" class="headerlink" title="字典大小，token化，ids化"></a>字典大小，token化，ids化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vocab = tokenizer.vocab</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;字典大小：&quot;</span>, <span class="built_in">len</span>(vocab)) <span class="comment"># 查看字典大小</span></span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(text)<span class="comment"># 将文字分词</span></span><br><span class="line">ids = tokenizer.convert_tokens_to_ids(tokens)<span class="comment"># 将文字转化为数字，进行编码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;[CLS]&#x27;, &#x27;等&#x27;, &#x27;到&#x27;, &#x27;潮&#x27;, &#x27;水&#x27;, &#x27;[MASK]&#x27;, &#x27;了&#x27;, &#x27;，&#x27;, &#x27;就&#x27;, &#x27;知&#x27;] ...</span></span><br><span class="line"><span class="string">[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ... </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="Mask模型的使用"><a href="#Mask模型的使用" class="headerlink" title="Mask模型的使用"></a>Mask模型的使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertForMaskedLM</span><br><span class="line"><span class="comment"># 除了 tokens 以外我們還需要辨別句子的 segment ids</span></span><br><span class="line">tokens_tensor = torch.tensor([ids])  <span class="comment"># (1, seq_len)</span></span><br><span class="line">segments_tensors = torch.zeros_like(tokens_tensor)  <span class="comment"># (1, seq_len)</span></span><br><span class="line">maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 masked LM 估計 [MASK] 位置所代表的實際 token </span></span><br><span class="line">maskedLM_model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = maskedLM_model(tokens_tensor, segments_tensors)</span><br><span class="line">    predictions = outputs[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># (1, seq_len, num_hidden_units)</span></span><br><span class="line"><span class="keyword">del</span> maskedLM_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來</span></span><br><span class="line">masked_index = <span class="number">5</span></span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">probs, indices = torch.topk(torch.softmax(predictions[<span class="number">0</span>, masked_index], -<span class="number">1</span>), k)</span><br><span class="line">predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 top k 可能的字。一般我們就是取 top 1 当做预测值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;輸入 tokens ：&quot;</span>, tokens[:<span class="number">10</span>], <span class="string">&#x27;...&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> i, (t, p) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(predicted_tokens, probs), <span class="number">1</span>):</span><br><span class="line">    tokens[masked_index] = t</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Top &#123;&#125; (&#123;:2&#125;%)：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="built_in">int</span>(p.item() * <span class="number">100</span>), tokens[:<span class="number">10</span>]), <span class="string">&#x27;...&#x27;</span>)</span><br></pre></td></tr></table></figure><p>​輸入 tokens ： [‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘[MASK]’, ‘了’, ‘，’, ‘就’, ‘知’] …<br>​Top 1 (65%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘来’, ‘了’, ‘，’, ‘就’, ‘知’] …<br>​Top 2 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘过’, ‘了’, ‘，’, ‘就’, ‘知’] …<br>​Top 3 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘干’, ‘了’, ‘，’, ‘就’, ‘知’] …</p><h4 id="可视化模型-bertviz"><a href="#可视化模型-bertviz" class="headerlink" title="可视化模型: bertviz"></a>可视化模型: bertviz</h4><h2 id="Pandas预处理文本"><a href="#Pandas预处理文本" class="headerlink" title="Pandas预处理文本"></a>Pandas预处理文本</h2><ol><li>多使用自定义函数</li><li>nltk库的stopwords</li><li>textblob库的拼写检查、词干抽取、词性还原等</li></ol><h3 id="文本数据的基本体征提取"><a href="#文本数据的基本体征提取" class="headerlink" title="文本数据的基本体征提取"></a>文本数据的基本体征提取</h3><ul><li><p>词汇数量</p></li><li><p>字符数量</p></li><li><p>平均字长</p></li><li><p>停用词数量</p></li><li><p>特殊字符数量</p></li><li><p>数字数量</p></li><li><p>大写字母数量</p></li></ul><h3 id="文本数据的基本预处理"><a href="#文本数据的基本预处理" class="headerlink" title="文本数据的基本预处理"></a>文本数据的基本预处理</h3><ul><li>小写转换</li><li>去除标点符号</li><li>去除停用词</li><li>去除频现词</li><li>去除稀疏词</li><li>拼写校正</li><li>分词(tokenization)</li><li>词干提取(stemming)</li><li>词形还原(lemmatization)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Universe </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HuggingFace </tag>
            
            <tag> Bert </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>文章分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/categorybar.css"/>
      <url>/css/categorybar.css</url>
      
        <content type="html"><![CDATA[#categoryBar {  width: 100% !important;}ul.categoryBar-list {  margin: 5px 5px 0 5px !important;  padding: 0 !important;}li.categoryBar-list-item {  font-weight: bold;  display: inline-block;  height: 180px !important;  margin: 5px 0.5% 0 0.5% !important;  background-image: linear-gradient(rgba(0,0,0,0.4) 25%, rgba(16,16,16,0) 100%);  border-radius: 10px;  padding: 25px 0 25px 25px !important;  box-shadow: rgba(50,50,50,0.3) 50px 50px 50px 50px inset;  overflow: hidden;  background-size: 100% !important;  background-position: center !important;}li.categoryBar-list-item:hover {  background-size: 110% !important;  box-shadow: inset 500px 50px 50px 50px rgba(50,50,50,0.6);}li.categoryBar-list-item:hover span.categoryBar-list-descr {  transition: all 0.5s;  transform: translate(-100%, 0);}a.categoryBar-list-link {  color: #fff !important;  font-size: 20px !important;}a.categoryBar-list-link::before {  content: '|' !important;  color: #fff !important;  font-size: 20px !important;}a.categoryBar-list-link:after {  content: '';  position: relative;  width: 0;  bottom: 0;  display: block;  height: 3px;  border-radius: 3px;  background-color: #fff;}a.categoryBar-list-link:hover:after {  width: 90%;  left: 1%;  transition: all 0.5s;}span.categoryBar-list-count {  /* display: block !important; */  color: #fff !important;  font-size: 20px !important;}span.categoryBar-list-count::before {  content: '\f02d' !important;  padding-right: 15px !important;  display: inline-block;  font-weight: 600;  font-style: normal;  font-variant: normal;  font-family: 'Font Awesome 6 Free';  text-rendering: auto;  -webkit-font-smoothing: antialiased;}span.categoryBar-list-descr {  padding: 5px;  display: block !important;  color: #fff !important;  font-size: 20px !important;  position: relative;  right: -100%;}@media screen and (max-width: 650px) {  li.categoryBar-list-item {    width: 48% !important;    height: 150px !important;    margin: 5px 1% 0 1% !important;  }}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/light.js"/>
      <url>/js/light.js</url>
      
        <content type="html"><![CDATA[// 霓虹灯效果// 颜色数组var arr = ["#f14747", "#f1a247", "#f1ee47", "#b347f1", "#1edbff", "#ed709b", "#5636ed"];// 颜色索引var idx = 0;// 切换颜色function changeColor() {    // 仅夜间模式才启用    if (document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark') {        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = arr[idx] + " 0 0 10px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = arr[idx] + " 0 0 5px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = arr[idx] + " 0 0 12px";            document.getElementsByClassName("author-info__description")[0].style.textShadow = arr[idx] + " 0 0 12px";        } catch {                    }        idx++;        if (idx == 8) {            idx = 0;        }    } else {        // 白天模式恢复默认        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = "#1e1e1ee0 1px 1px 1px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = "";            document.getElementsByClassName("author-info__description")[0].style.textShadow = "";        } catch {                    }    }}// 开启计时器window.onload = setInterval(changeColor, 9900);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[/* 页脚与头图透明 */#footer {    background: transparent !important;  }  #page-header {    background: transparent !important;  }    /* 白天模式遮罩透明 */  #footer::before {    background: transparent !important;  }  /* #page-header::before {    background: transparent !important;  } */    /* 夜间模式遮罩透明 */  [data-theme="dark"] #footer::before {    background: transparent !important;  }  [data-theme="dark"] #page-header::before {    background: transparent !important;  }/* 一级菜单居中 *//* #nav .menus_items {    position: absolute !important;    width: fit-content !important;    left: 50% !important;    transform: translateX(-50%) !important;  }  子菜单横向展示 */  /* #nav .menus_items .menus_item:hover .menus_item_child {    display: flex !important;  }  /* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */  /* .menus_items .menus_item:nth-child(2) .menus_item_child {    left: -125px;  } */ *//* 夜间模式菜单栏发光字 */[data-theme="dark"] #nav .site-page,[data-theme="dark"] #nav .menus_items .menus_item .menus_item_child li a {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 手机端适配 */[data-theme="dark"] #sidebar #sidebar-menus .menus_items .site-page {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 侧边栏个人信息卡片动态渐变色 *//* #aside-content > .card-widget.card-info {   background-image: linear-gradient(  to right,   #ff8177 0%,  #ff867a 0%,  #ff8c7f 21%,  #f99185 52%,  #cf556c 78%,  #b12a5b 100%);;    box-shadow: 0 0 5px rgb(66, 68, 68);    position: relative;    background-size: 400% 400%;    -webkit-animation: Gradient 10s ease infinite;    -moz-animation: Gradient 10s ease infinite;    animation: Gradient 10s ease infinite !important;  }  @-webkit-keyframes Gradient {    0% {      background-position: 0% 50%;    }    50% {      background-position: 100% 50%;    }    100% {      background-position: 0% 50%;    }  }  @-moz-keyframes Gradient {    0% {      background-position: 0% 50%;    }    50% {      background-position: 100% 50%;    }    100% {      background-position: 0% 50%;    }  }  @keyframes Gradient {    0% {      background-position: 0% 50%;    }    50% {      background-position: 100% 50%;    }    100% {      background-position: 0% 50%;    }  }      /* 黑夜模式适配 */  [data-theme="dark"] #aside-content > .card-widget.card-info {    background: #191919dd;  }    /* 个人信息Follow me按钮 */  #aside-content > .card-widget.card-info > #card-info-btn {    background-color: #3eb8be;    border-radius: 8px;  }/* 翻页按钮居中 */#pagination {  width: 100%;  margin: auto;}/* 首页 卡片文章背景透明 */#recent-posts>.recent-post-item {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border-radius: 25px;  border: 1px solid rgb(169, 169, 169) }#aside-content .card-widget {  background: #ffffffee;   /* backdrop-filter: var(--backdrop-filter); */  border-radius: 18px;  transition: .3s;  border: 1px solid rgb(169, 169, 169) }div#post {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#page {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#archive {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#tag {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}div#category {  background: #ffffffee;  /* backdrop-filter: var(--backdrop-filter); */  border: 1px solid rgb(169, 169, 169) ;  border-radius: 20px}#page-header.nav-fixed #nav {  background: rgba(255,255,255,.75);  /* backdrop-filter: var(--backdrop-filter) */}[data-theme=dark] #page-header.nav-fixed #nav {  background: rgba(0,0,0,.7)!important}[data-theme=dark] #recent-posts>.recent-post-item {  background: #191919dd  }[data-theme=dark] #aside-content .card-widget {  background: #191919dd}[data-theme=dark] div#post {  background: #191919dd}[data-theme=dark] div#tag {  background: #191919dd}[data-theme=dark] div#archive {  background: #191919dd}[data-theme=dark] div#page {  background: #191919dd}[data-theme=dark] div#category {  background: #191919dd}[data-theme=dark] #footer::before {  background: 0 0!important}[data-theme=dark] #page-header::before {  background: 0 0!important}.read-mode #aside-content .card-widget {  background: rgba(158,204,171,.5)!important}.read-mode div#post {  background: rgba(158,204,171,.5)!important}[data-theme=dark] .read-mode #aside-content .card-widget {  background: rgba(25,25,25,.9)!important;  color: #fff}[data-theme=dark] .read-mode div#post {  background: rgba(25,25,25,.9)!important;  color: #fff}/* 隐藏分类磁贴数字 */span.categoryBar-list-count {  display: none !important;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>资源分享</title>
      <link href="/something/index.html"/>
      <url>/something/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>知识标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
