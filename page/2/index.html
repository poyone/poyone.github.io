<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/2/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-04 11:21:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/10656.html" title="句意相似度 PipeLine总结"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris41.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="句意相似度 PipeLine总结"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/10656.html" title="句意相似度 PipeLine总结">句意相似度 PipeLine总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-27T02:47:31.323Z" title="发表于 2022-11-27 10:47:31">2022-11-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-02T02:09:36.113Z" title="更新于 2022-12-02 10:09:36">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%8F%A5%E6%84%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6/">句意相似度</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Pipeline/">Pipeline</a></span></div><div class="content">主要进行训练框架优化

端到端 ML 实施（训练、验证、预测、评估）
轻松适应您自己的数据集
促进其他基于 BERT 的模型（BERT、ALBERT、…）的快速实验
使用有限的计算资源进行快速训练（混合精度、梯度累积……）
多 GPU 执行
分类决策的阈值选择（不一定是 0.5）
冻结 BERT 层，只更新分类层权重或更新所有权重
种子设置，可复现结果

PipeLine导包12345678910111213import torchimport torch.nn as nnimport osimport copyimport torch.optim as optimimport randomimport numpy as npimport pandas as pdfrom torch.utils.data import DataLoader, Datasetfrom torch.cuda.amp import autocast, GradScalerfrom tqdm.auto import tqdmfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmupfrom datasets import load_dataset, load_metric

Dataset123456789101112131415161718192021222324252627282930313233343536class CustomDataset(Dataset):    def __init__(self, data, maxlen, with_labels=True, bert_model=&#x27;albert-base-v2&#x27;):        self.data = data  # pandas dataframe        #Initialize the tokenizer        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)          self.maxlen = maxlen        self.with_labels = with_labels     def __len__(self):        return len(self.data)    def __getitem__(self, index):        #根据索引索取DataFrame中句子1余句子2        sent1 = str(self.data.loc[index, &#x27;sentence1&#x27;])        sent2 = str(self.data.loc[index, &#x27;sentence2&#x27;])        # 对句子对分词，得到input_ids、attention_mask和token_type_ids        encoded_pair = self.tokenizer(sent1, sent2,                                       padding=&#x27;max_length&#x27;,  # 填充到最大长度                                      truncation=True,  # 根据最大长度进行截断                                      max_length=self.maxlen,                                        return_tensors=&#x27;pt&#x27;)  # 返回torch.Tensor张量                token_ids = encoded_pair[&#x27;input_ids&#x27;].squeeze(0)  # tensor token ids        attn_masks = encoded_pair[&#x27;attention_mask&#x27;].squeeze(0)  # padded values对应为 &quot;0&quot; ，其他token为1        token_type_ids = encoded_pair[&#x27;token_type_ids&#x27;].squeeze(0)  #第一个句子的值为0，第二个句子的值为1 # 只有一句全为0        if self.with_labels:  # True if the dataset has labels            label = self.data.loc[index, &#x27;label&#x27;]            return token_ids, attn_masks, token_type_ids, label          else:            return token_ids, attn_masks, token_type_ids

建议，进行测试
12sample = next(iter(DataLoader(tr_dataset, batch_size=2)))sample

12tr_model = SentencePairClassifier(freeze_bert=True)tr_model(sample[0], sample[1], sample[2])

就是方便最后的维度转换，squeeze、flatten、view；甚至可以用reshape方法
模型定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SentencePairClassifier(nn.Module):    def __init__(self, bert_model=&quot;albert-base-v2&quot;, freeze_bert=False):        super(SentencePairClassifier, self).__init__()        #  初始化预训练模型Bert xxx        self.bert_layer = AutoModel.from_pretrained(bert_model)        #  encoder 隐藏层大小        if bert_model == &quot;albert-base-v2&quot;:  # 12M 参数            hidden_size = 768        elif bert_model == &quot;albert-large-v2&quot;:  # 18M 参数            hidden_size = 1024        elif bert_model == &quot;albert-xlarge-v2&quot;:  # 60M 参数            hidden_size = 2048        elif bert_model == &quot;albert-xxlarge-v2&quot;:  # 235M 参数            hidden_size = 4096        elif bert_model == &quot;bert-base-uncased&quot;: # 110M 参数            hidden_size = 768        elif bert_model == &quot;roberta-base&quot;: #             hidden_size = 768        # 固定Bert层 更新分类输出层        if freeze_bert:            for p in self.bert_layer.parameters():                p.requires_grad = False                        self.dropout = nn.Dropout(p=0.1)        # 分类输出        self.cls_layer = nn.Linear(hidden_size, 1)    @autocast()  # 混合精度训练    def forward(self, input_ids, attn_masks, token_type_ids):        &#x27;&#x27;&#x27;        Inputs:            -input_ids : Tensor  containing token ids            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2        &#x27;&#x27;&#x27;        # 输入给Bert，获取上下文表示        # cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)        # last_hidden_state,pooler_output,all_hidden_states 12层        # 将last layer hidden-state of the [CLS] 输入到 classifier layer        # - last_hidden_state 的向量平均        # - 取all_hidden_states最后四层，然后做平均 weighted 平均        # - last_hidden_state+lstm        # 获取输出        logits = self.cls_layer(self.dropout(outputs[&#x27;pooler_output&#x27;]))        return logits

固定随机种子12345678910def set_seed(seed):    &quot;&quot;&quot; 固定随机种子，保证结果复现    &quot;&quot;&quot;    torch.manual_seed(seed)    torch.cuda.manual_seed_all(seed)    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    np.random.seed(seed)    random.seed(seed)    os.environ[&#x27;PYTHONHASHSEED&#x27;] = str(seed)

训练和评估12!mkdir models 	#可以在之前补充绝对路径!mkdir results



123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):    best_loss = np.Inf    best_ep = 1    nb_iterations = len(train_loader)    print_every = nb_iterations // 5  # 打印频率    iters = []    train_losses = []    val_losses = []    scaler = GradScaler()    for ep in range(epochs):        net.train()        running_loss = 0.0        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):            # 转为cuda张量            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)                # 混合精度加速训练            with autocast():                # Obtaining the logits from the model                logits = net(seq, attn_masks, token_type_ids)                # Computing loss                loss = criterion(logits.squeeze(-1), labels.float())                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged            # Backpropagating the gradients            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.            scaler.scale(loss).backward()            if (it + 1) % iters_to_accumulate == 0:                # Optimization step                # scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.                # If these gradients do not contain infs or NaNs, opti.step() is then called,                # otherwise, opti.step() is skipped.                scaler.step(opti)                # Updates the scale for next iteration.                scaler.update()                # 根据迭代次数调整学习率。                lr_scheduler.step()                # 梯度清零                opti.zero_grad()            running_loss += loss.item()            if (it + 1) % print_every == 0:  # Print training loss information                print()                print(f&quot;Iteration &#123;it+1&#125;/&#123;nb_iterations&#125; of epoch &#123;ep+1&#125; complete. \                Loss : &#123;running_loss / print_every&#125; &quot;)                running_loss = 0.0        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss        print()        print(f&quot;Epoch &#123;ep+1&#125; complete! Validation Loss : &#123;val_loss&#125;&quot;)        if val_loss &lt; best_loss:            print(&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;.format(best_loss, val_loss))            print()            net_copy = copy.deepcopy(net)  # # 保存最优模型            best_loss = val_loss            best_ep = ep + 1    # 保存模型    path_to_model=f&#x27;models/&#123;bert_model&#125;_lr_&#123;lr&#125;_val_loss_&#123;round(best_loss, 5)&#125;_ep_&#123;best_ep&#125;.pt&#x27;    torch.save(net_copy.state_dict(), path_to_model)    print(&quot;The model has been saved in &#123;&#125;&quot;.format(path_to_model))    del loss    torch.cuda.empty_cache() # 清空显存    def evaluate_loss(net, device, criterion, dataloader):    &quot;&quot;&quot;    评估输出    &quot;&quot;&quot;    net.eval()    mean_loss = 0    count = 0    with torch.no_grad():        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)            logits = net(seq, attn_masks, token_type_ids)            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()            count += 1    return mean_loss / count


注意autocast和累计梯度 这两种加速计算的方法

evaluate的时候要注意数据的维度，标签的类型


超参数 &amp; 开始训练1234567bert_model = &quot;albert-base-v2&quot;  # &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;freeze_bert = False  # 是否冻结Bertmaxlen = 128  # 最大长度bs = 16  # batch sizeiters_to_accumulate = 2  # 梯度累加lr = 2e-5  # learning rateepochs = 2  # 训练轮数



123456789101112131415161718192021222324252627282930#  固定随机种子 便于复现set_seed(1) # 2022 # 创建训练集与验证集print(&quot;Reading training data...&quot;)train_set = CustomDataset(df_train, maxlen, bert_model)print(&quot;Reading validation data...&quot;)val_set = CustomDataset(df_val, maxlen, bert_model)# 常见训练集与验证集DataLoadertrain_loader = DataLoader(train_set, batch_size=bs, num_workers=0)val_loader = DataLoader(val_set, ba ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/26087.html" title="Weight &amp; Bias"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Weight &amp; Bias"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/26087.html" title="Weight &amp; Bias">Weight &amp; Bias</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-26T09:17:27.733Z" title="发表于 2022-11-26 17:17:27">2022-11-26</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-01T16:04:07.190Z" title="更新于 2022-12-02 00:04:07">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Trick/">Trick</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/wandb/">wandb</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/pytorch/">pytorch</a></span></div><div class="content">待完成
源码细节整理

torch.inference_mode()with no_gradient的一种加速  参考文档
 nn.MarginRankingLoss()文档 margin &#x3D; 0  x1大于x2 则去-y，viceversa 取 y
*loss(x1,x2,y)&#x3D;max(0,−y∗(x1−x2)+margin)*
这里最后的loss是平均后的
1234567891011121314151617181920212223loss = nn.MarginRankingLoss()input1 = torch.randn(3, requires_grad=True)input2 = torch.randn(3, requires_grad=True)target = torch.randn(3).sign()output = loss(input1, input2, target)output.backward()```input1, input2, target, output(tensor([ 0.0277, -0.3806,  1.0405], requires_grad=True), tensor([-0.9075,  0.3271,  0.1156], requires_grad=True), tensor([ 1., -1., -1.]), tensor(0.3083, grad_fn=&lt;MeanBackward0&gt;)) input1 - input2 , (input1 - input2) * (-target)(tensor([ 0.9352, -0.7077,  0.9249], grad_fn=&lt;SubBackward0&gt;), tensor([-0.9352, -0.7077,  0.9249], grad_fn=&lt;MulBackward0&gt;), loss = 0.9249/3 ```



gc.collect()清除内存
defaultdict获得创建key不给value也不报错的dict
12345from collections import defaultdicthistory = defaultdict(list)history[&#x27;Train Loss&#x27;].append(1.1)



StratifiedKFold()12345678from sklearn.model_selection import StratifiedKFold, KFoldskf = StratifiedKFold(n_splits=CONFIG[&#x27;n_fold&#x27;], shuffle=True, random_state=CONFIG[&#x27;seed&#x27;])for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):    df.loc[val_ , &quot;kfold&quot;] = int(fold)    df[&quot;kfold&quot;] = df[&quot;kfold&quot;].astype(int)

第五行 将X分k折，y标签为样本对应index，fold 在 0~5
得到df[“kfold”] 列包含 属于第几折的 valid数据
通过下面的函数直接选择非本折的数据作为train，其他的就是valid
df_train = df[df.kfold != fold].reset_index(drop=True) df_valid = df[df.kfold == fold].reset_index(drop=True)
12345678910111213def prepare_loaders(fold):    df_train = df[df.kfold != fold].reset_index(drop=True)    df_valid = df[df.kfold == fold].reset_index(drop=True)        train_dataset = JigsawDataset(df_train, tokenizer=CONFIG[&#x27;tokenizer&#x27;], max_length=CONFIG[&#x27;max_length&#x27;])    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG[&#x27;tokenizer&#x27;], max_length=CONFIG[&#x27;max_length&#x27;])    train_loader = DataLoader(train_dataset, batch_size=CONFIG[&#x27;train_batch_size&#x27;],                               num_workers=2, shuffle=True, pin_memory=True, drop_last=True)    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[&#x27;valid_batch_size&#x27;],                               num_workers=2, shuffle=False, pin_memory=True)        return train_loader, valid_loade



tqdm1bar = tqdm(enumerate(dataloader), total=len(dataloader))

单个epoch下面对bar做如下设置
12bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,                        LR=optimizer.param_groups[0][&#x27;lr&#x27;])  



Weights &amp; Biases (W&amp;B) 
hash 一个项目id

train valid 定义一个 1个epoch 的函数 返回 分别其中的loss

wandb.log({“Train Loss”: train_epoch_loss}) 使用 log 方式记录 损失函数

run = wandb.init(project=&#39;Jigsaw&#39;, 
                     config=CONFIG,
                     job_type=&#39;Train&#39;,
                     group=CONFIG[&#39;group&#39;],
                     tags=[&#39;roberta-base&#39;, f&#39;&#123;HASH_NAME&#125;&#39;, &#39;margin-loss&#39;],
                     name=f&#39;&#123;HASH_NAME&#125;-fold-&#123;fold&#125;&#39;,
                     anonymous=&#39;must&#39;)
1TRAIN PART

run.finish()

1234显示如下			&#x27;hash--------name&#x27;Syncing run k5nu8k69390a-fold-0 to Weights &amp; Biases (docs).



流程训练提炼
for fold in range(0, CONFIG[‘n_fold’])
wandb.init
prepare_loaders、fetch_scheduler
run_training
train_one_epoch、valid_one_epoch —-&gt; to got model, loss for wandb



中间掺杂 W&amp;B 的数据实时载入分析即可
df[‘y’].value_counts(normalize&#x3D;True) to got the percentage of each values
原文链接
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/12763.html" title="Python基础01 数据类型"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris34.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python基础01 数据类型"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/12763.html" title="Python基础01 数据类型">Python基础01 数据类型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-22T08:53:45.281Z" title="发表于 2022-11-22 16:53:45">2022-11-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T02:36:45.513Z" title="更新于 2022-11-27 10:36:45">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Python/">Python</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/python%E5%9F%BA%E7%A1%80/">python基础</a></span></div><div class="content">前言本文介绍Python中基本的数据类型：

字符串、数字
列表
字典
集合
元组

以及一些常用的处理小技巧。
1. 字符串、数字Python中字符串（str）的处理对于没有任何变成经验的同学可能有些苦恼，如下
1234s1 = &#x27;1222&#x27;i = 1222add_up = s1 + i # 这段函数就会报错，因为无法将 int类型 与 str类型相加


int ：整数类型，将小数抹除

float ：浮点数类型，因为二进制进位的关系数据并不准确的


以上就是提醒各位，对数据处理的时候，一定要留心数据的类型


1.1 字符串中的序号字符串的序号可以让你快速取得一串字符中任意位置的任意字符，有如下三种基本方式：
123456789101112131415s = &#x27;Attention Is A Talent&#x27; # 首先创建一个字符串# 第一种 取单个字符s[0] 	# 将获得 &#x27;A&#x27;s[20]	# 将获得 &#x27;t&#x27;  # 第二种 取多个字符s[0:2]  # 将获得 &#x27;At&#x27; s[:20]	# 将获得 &#x27;Attention Is A Talen&#x27;s[::2]  # 将获得 &#x27;AtninI  aet&#x27;s[::1]  # 将获得 &#x27;Attention Is A Talent&#x27;# 第三种 倒序s[-1]   # 将获得 &#x27;t&#x27; s[-21:]	# 将获得 &#x27;Attention Is A Talent&#x27;

有如下需要注意的几个地方：

在python中我们使用引号包裹需要的字符串内容

字符串的序号是从 0 开始定义的，所以上面的s有21个字符，而方括号内的取值范围是[0,20], 细心你的肯定发现了，空格数也算进去了。没错空格也算一种特殊的字符。

第二种取值方式我们称之为切片，python中的切片方式等价数学上的**左闭右开~[x, y)**，上面字符s[:20]，是取得s[20]号位左边的全部值，但是不会包含s[20]。


​		当然是用s[0:20] 也是等价的。
​		步长 即第二种方法的第三个式子，是用步长就是字面意思，每走n步取值。
​		s[::2]就是 s[0]第一步， s[1]第二步(存储)，s[2]第三步，s[3]第四步(存储)….

倒序是字符串的另一套序号，它有很多应用场景，比如定义一个很长的字符串你可能需要用s[1222222]才能取得这个值，但是是用s[-1]就很方便。
当然，需要注意倒序是从[-1]开始的。


1.2 特殊字符&#39;\n&#39;(换行)    &#39;\b&#39;(回退)    &#39;\r&#39;(光标回到本行行首)  &#39;\t&#39;(相当于八个空格，两个table)
以上就是几个常见的特殊字符，其中特别需要注意的是路径中的斜杠如遇到 \nigger 计算机可能就无法明白你输入的是 ‘igger’，还是含有n的字符串。有以下两种处理方式：

&#39;\\nigger&#39; 
r&#39;\nigger&#39;

1.3 字符串的运算以及常用函数运算
123s1 + s2               # 将两个字符串连接s1*n                  # 将s1复制n次s1 in s2              # 如果s1是s2的字串 则返回 True 否 False

函数
1234567891011121314151617181920212223242526s = &#x27; Attention,Is,A,Talent &#x27;# split函数s.split(&#x27;,&#x27;)					# 输出为 [&#x27; Attention&#x27;, &#x27;Is&#x27;, &#x27;A&#x27;, &#x27;Talent &#x27;]                   				# split函数以逗号为标志，返回一个分隔后的列表# count函数                   				s.count(&#x27;A&#x27;)                    # 统计A在s中的次数，本例中将返回int类型的2# upper，lower函数s.upper() / s1.lower()          # 将字符串转化为对应的大小写# replace函数s.replace(&#x27;tion&#x27;, &#x27;&#x27;)  			# 将字符串中的&#x27;tion&#x27;替换为&#x27;&#x27;，即没有东西，相当于删除# center函数s.center(30, &#x27;=&#x27;)               # 将s放在中间，左右两侧平均填充等于号至总字符数为30# strip函数s.strip(&#x27; &#x27;)                    # s两侧删除空格，以及其他不可读符号如&#x27;\n&#x27;# join函数&#x27;,&#x27;.join(s)                     # s中每个字符间填充逗号								# 返回 &#x27;A,t,t,e,n,t,i,o,n,,,I,s,,,A,,,T,a,l,e,n,t&#x27;# len函数len(s)							# 返回s的长度


上面我们以逗号为分隔符使用split函数，但是注意中英文的逗号是有区别的，其他有些符号也一样，需要注意。
上述中的replace几乎可以代替center函数，但是注意strip只能处理字符串的两端
如join函数返回的结果，再次提醒空格也算是字符
最后，上述操作产生都是一个新的对象，即调用s后返回的是原本的字符串，并不是函数作用后的结果。需要s = s.replace(&#39;tion&#39;, &#39;&#39;)   赋值才‘生效’。

1.4 数字函数12345678910pow(x, n)   					 #为x的n次方divmod(10, 3)   				 # 输出为（3， 1）abs()      						 #返回值为绝对值int(12.34)  					 #输出 12float(12), float(&#x27;12.23&#x27;) 		 #输出为 12.0 和 12.23round(1.2345， 2)  	 			#保留两位小数max(1, 2, 3)  					 #返回值为3min(1, 2, 3)  					 #返回值为1



1.5 格式化字符串在我们得到一个数据之后，经常需要对其做保留多少位小数、居中打印、靠右输出、等操作，我们一般叫使其格式化。在python中有三种格式化填充字符串的方式：
12345# format方式a, b, c = &#x27;Is&#x27;, &#x27;Talent&#x27;, 0.12222&#x27;Attention &#123;a&#125; A &#123;b&#125; version &#123;:.2f&#125;&#x27;.format(a, b, c)# 我们将得到如下输出 &#x27;Attention Is A Talent version0.12&#x27;

format格式化将按照顺序填入上面字符串{}的空位，{:.2f}表示此处保留两位小数
12345# f方式a, b, c = &#x27;Is&#x27;, &#x27;Talent&#x27;, 0.12222f&#x27;Attention &#123;a&#125; A &#123;b&#125; version &#123;c:.2f&#125;&#x27;# 我们将得到如下输出 &#x27;Attention Is A Talent version0.12&#x27;

f格式化就是对format方式的简化版
12345# %方式a, b, c = &#x27;Is&#x27;, &#x27;Talent&#x27;&#x27;Attention %s A %s &#x27;% (&#x27;Is&#x27;, &#x27;Talent&#x27;)# 我们将得到如下输出 &#x27;Attention Is A Talent &#x27;

这种方式很老了，推荐使用f方式，非常简洁。
以下不是必看内容：format填充方式
12chr(Unicode)					#返回Unicode对应的字符ord(&#x27;字&#x27;)					   #返回对应的编码，如chr(ord(&#x27;a&#x27;)+i ) 即可遍历26字母	

填充物若为chr(12222)，等特殊字符
12345678f&#x27;&#123;chr(12222):^10&#125;&#x27;# 输出为 &#x27;    ⾾     &#x27;f&#x27;&#x27;&#123;chr(12222):=^10&#125;&#x27;&#x27;====⾾=====&#x27;f&#x27;&#x27;&#123;chr(12222):=&gt;10&#125;&#x27;&#x27;=========⾾&#x27;

如上{chr(12222):^10} 将chr(12222)对应的字符输出在中间，左右两侧填充空格。
也可用等号等其他符号填充，或者使用&gt;大于号使结果置右。
2. 列表列表跟上文中提到的字符串很像，或者说字符串是一种特殊的列表，其所有元素都是字符串。
python中的列表（list），在我的印象里几乎可以装任何的东西: 字符串、数字、甚至你定义的函数…
列表是一种非常好用的数据类型，也是我们最常使用数据类型，以下概要对其简要介绍并补充几个判断符。
2.1 概要列表同字符串也有正反序号下标，切片操作，不同的是列表可以含有各种类型的数据
12345ls = [&#x27;Attention Is A Talent&#x27;, 100, &#x27;% &#x27;]ls[0] == ls[-3] 				# 返回True&#x27;A&#x27; in ls						# 返回Truestr(ls[1]) + ls[2] + ls[0]		# &#x27;100% Attention Is A Talent&#x27;


上面我们使用双等号作为判断符，等价询问python 是否 ls[0] &#x3D; ls[-3]

还有 !&#x3D; 、&gt;&#x3D;、&lt;&#x3D;、等


上面我们使用in作为判断词，等价询问python 是否 ‘A’ 在 ls

还有 not in，or，and等


第二点，我们使用了str()，它是一个函数，将对传给它的值做字符串化的处理

int类型的 100 ——&gt; ‘100’ 即数字100变成字符串了



2.2 列表的运算以及常用函数运算
12345ls = [&#x27;Attention Is A Talent&#x27;, 100, &#x27;% &#x27;]ls * 2 				# 将返回 [&#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;, &#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;]ls + ls[:1]			# 将返回 [&#x27;Attention Is A Talent&#x27;, 100, &#x27;%&#x27;, &#x27;Attention Is A Talent&#x27;]


注意，列表的加法操作只能在列表跟列表之间。
如上图使用 ls + ls[0] 将会报错



函数
12345678# 下面x表示单个元素、ls表示列表0号、ls1表示列表1号ls.append(x)					# 给ls尾添加x元素ls.remove(x)					# 将ls中出现的第一个x删除，如要删除所有x可以用（while+flag）或者set集合类型除重ls.extend(ls1)					# 将ls后面连接ls1ls.reverse()					# 将列表的元素逆置ls.insert(i,x)					# 在i位置 插入xls.pop(i)						# i位置元素出栈，删除

(这里加些列表复杂一点的方法，如果没有了解python中的字典、元组数据类型，可以在下文中了解后再看)
12345678910111213141516ls = [(&#x27;tom&#x27;, 95), (&#x27;jerry&#x27;, 80), (&#x27;mike&#x27;, 99), (&#x27;john&#x27;, 70)]ls.sort()ls.sort(reverse = ture)			# 逆排序ls.sort(key= lambda x:x[1])		# 按值排序 -------------------------------sorted(ls)						# 会自动把序列从小到大排序sorted(ls, reverse = true)sorted(ls, lambda x:x[0])		# 以lambda函数作为值排序-------------------------------seasons = [&#x27;Spring&#x27;, &#x27;Summer&#x27;, &#x27;Fall&#x27;, &#x27;Winter&#x27;]			# enumerate()的对象必须是可以迭代的类型(iterable)list(enumerate(seasons))[(0, &#x27;Spring&#x27;), (1, &#x27;Summer&#x27;), (2, &#x27;Fall&#x27;), (3, &#x27;Winter&#x27;)]list(enumerate(seasons, start=1))[(1, &#x27;Spring&#x27;), (2, &#x27;Summer&#x27;), (3, &#x27;Fall&#x27;), (4, &#x27;Winter&#x27;)]


第一部分中为ls的sort方法配置参数

reverse 表示逆置，如果对象没有‘大小’，则按照原来的顺序直接逆置
key 参数表示排序根据此值的大小，这里我们就是以每个元组的第二个值作为value排序


第二部分是使用python中的sorted和sort函数

第一个参数表示传入的可迭代数据类型(就是列表这种含有很多元素，可以一个一个出来的数据类型)

第二个参数 同上面的key


sort 与 sorted 区别：
sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。
list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。




第三部分使用了enumerate函数，这个函数主要是为元素添加下标，方便一些特殊场景处理

enumerate函数返回一个含有位置下标的元组类型，为(index，element)形式



2.3 列表应用的例子123456789ls = [&#x27;Alice&#x27;, &#x27;Bob&#x27;]list(lt) = ls[0]							# [0]号为字符，导入list中会分割成[&#x27;a&#x27;, &#x27;l&#x27;, &#x27;i&#x27;, &#x27;c&#x27;, &#x27;e&#x27;]------------------------------------ls = [&#x27;Ali:ce&#x27;, &#x27;Bo:b&#x27;]lt = []for i in ls:    elem = i.split(&#x27;:&#x27;)[-1]					# ls[i]为字符串使用split分割-&gt;[&#x27;Ali&#x27;,&#x27;ce&#x27;]取[-1]    lt.append(elem)							# 直接+=会变成[&#x27;c&#x27;,&#x27;e&#x27;],所以使用list的append,则直接将str的ce加入											# lt = [ce, b]  干净的字符串列表



3. 字典Python字典（dict）是另一种可变容器模型,可存储任意类型对象。如字符串、数字、元组等其他容器模型因为字典是无序的所以不支持索引和切片
注意：

key不可以重复,否则只会保留第一个;
value值可以重复;
key可以是任意的数据类型,但不能出现可变的数据类型,保证key唯一;
key一般形式为字符串。

3.1 基本属性123dic.keys()								# 返回字典中所有的keydic.values()							# 返回包含value的列表dic.items()								# 返回包含(键值,实值)元组的列表

3.2 基本函数123456789101112dic.setdefault(k,value)#如果key值存在,那么返回对应字典的value,不会用到自己设置的value;#如果key值不存在.返回None,并且把新设置的key和value保存在字典中;#如果key值不存在,但设置了value,则返回设置的value;dic.get(k,value)#如果key值存在,那么返回对应字典的value,不会用到自己设置的value;#如果key值不存在.返回None,但是不会把新设置的key和value保存在字典中;#如果key值不存在,但设置了value,则返回设置的value;dic.items()#打印字典中的所有元组

以下不是必看内容：dic.get(key, init_value)
123456789ls = [(&#x27;tom&#x27;, 95), (&#x27;tom&#x27;, 95), (&#x27;tom&#x27;, 95), (&#x27;jerry&#x27;, 80), (&#x27;mike&#x27;, 99), (&#x27;john&#x27;, 70)]ls1,dic = [], &#123;&#125;for i, j in ls:    ls1.append(i)for i  in ls1:	dic[i] = dic.get(i, 0) + 1         dic

get第一个参数为对应的键，第二个参数为键对应的初始值。
get方法将键对应的值初始化为0，以后每见一次加一次，在文本统计时经常使用。
相对于dic[i]，dic.get(i)不会报错，而它每见一次加一次而不是覆盖，明显速度会比前者慢。(但是不是大量数据都差不多。)
4.集合在Python中集合（set）元素之间无序，每个元素唯一，不存在相同元素集合元素不可更改，不能是可变数据类型
4.1 创建和运算1234567使用两种方式建立A = &#123;&quot;python&quot;, 123, (&quot;python&quot;,123)&#125; 				# 使用&#123;&#125;建立集合	输出为&#123;123, &#x27;python&#x27;, (&#x27;python&#x27;, 123)&#125;B = set(&quot;pypy123&quot;) 									# 使用set()建立集合	输出为&#123;&#x27;1&#x27;, &#x27;p&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;y&#x27;&#125;C = &#123;&quot;python&quot;, 123, &quot;python&quot;,123&#125;					# 去重	输出为&#123;&#x27;python&#x27;, 123&#125;

123456S | T 				并，返回一个新集合，包括在集合S和T中的所有元素 S - T 				差，返回一个新集合，包括在集合S但不在T中的元素 S &amp; T 				交，返回一个新集合，包括同时在集合S和T中的元素 S ^ T 				补，返回一个新集合，包括集合S和T中的非相同元素 S &lt;= T 或 S &lt; T 	    返回True/False，判断S和T的子集关系 S &gt;= T 或 S &gt; T 	    返回True/False，判断S和T的包含关系

4.2 函数12345len(s) 											#返回序列s的长度，即元素个数min(s) 											#返回序列s的最小元素，s中元素需要可比较max(s) 											#返回序列s的最大元素，s中元素需要可比较s.index(x) / s.index(x, i, j) 				    #返回序列s从i开始到j位置中第一次出现元素x的位置s.count(x) 										#返回序列s中出现x的总次数

其中len()、min()、max()函数是内置的通用函数
5. 元组python中的元组（tuple）是一种序列类型，一旦创建就不能被修改 ，使用小括号 () 或 tuple() 创建，元素间用逗号 , 分隔 。
5.1 创建和取值123creature = &quot;cat&quot;,&quot;dog&quot;,&quot;tiger&quot;,&quot;human&quot;creature = (&#x27;cat&#x27;, &#x27;dog&#x27;, &#x27;tiger&#x27;, &#x27;human&#x27;) 		# 可以使用括号和不带括号的两两种color = (0x001100, &quot;blue&quot;, creature)				# 输出会得到（，，（））

元组的取值操作跟列表一样
1234creature = &quot;cat&quot;,&quot;dog&quot;,&quot;tiger&quot;,&quot;human&quot; ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/4330.html" title="Transformer &amp; Self-Attention"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer &amp; Self-Attention"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/4330.html" title="Transformer &amp; Self-Attention">Transformer &amp; Self-Attention</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T09:00:10.398Z" title="发表于 2022-11-21 17:00:10">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-02T13:27:47.943Z" title="更新于 2022-12-02 21:27:47">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">阿三博客地址

李沐老师 48分钟讲解 encoder-decoder中(KV–Q)的运算: 

KQ相乘就是单个q对所有k的相似度作为attention score(给这个K值多少注意力)，与单个v做加权和(权值来自KQ)
再通过注意力分数与V向量相乘，得到每个V应该多大的缩放， 进行相加后就得到了最终V应该是什么样子了




李沐老师 56分 对multi-head输出和linear层相较于RNN的讲解：

词向量经过Attention层抓取全局信息，汇聚之后，在每个点上都有了所需要的信息
(权重不同，每个输出的向量的重点在不同的position编码位置上)，因此只需要做linear transformation。

bert中transformer参数计算:



embedding: vocab_size&#x3D;30522, max_position_embeddings&#x3D;512, token_type_embeddings&#x3D;2(就进行两句分别标记，多了截断)
​					（30522+512+2）*768 &#x3D; 23835648 (23M)
self-attention: 768&#x2F;12 &#x3D; 64 (多头每头分64维度的向量) ，64*768(每个64映射回768)，QKV三个矩阵, 
​						  最后一层 786(64 *12的拼接)-&gt;768的线性变换
​						(768&#x2F;12 * 768 3 ) * 12 + (768768) &#x3D; 2359296
​						经过12个transformer
​						2359296*12 &#x3D; 28311552 (28M)
feedfoward: 自注意力层之后 分别在 encoder 和 decoder 中有个一个全连接层
​						维度从 768-&gt;4*768_768-&gt;768
​						(768*4 * 768 )*2 &#x3D; 4718592
​						(768*4 * 768 )*2  * 12 &#x3D; 56623104 (56M)
layernorm: 有伽马和贝塔两个参数，embedding层（768 * 2），12层的self-attention，
​						768 * 2 + 768 * 2 * 2 * 12 &#x3D; 38400
总计: 23835648+28311552+56623104+38400 &#x3D; 108808704      				(108M)
每一层的参数为:  多头注意力的参数 + 拼接线性变换的参数 + feed-forward的参数 + layer-norm的参数
768 * 768 &#x2F; 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2 &#x3D; 7080960  (7M)

Encoder 编码阶段Multi-head Attention多头注意力机制将一个词向量留过八个 self-attention 头生成八个词向量 vector，
将八个词向量拼接，通过 fc 层进行 softmax 输出。
例如：
词向量为 (1,4) –&gt; 
经过 QKV 矩阵(系数) 得到 (1,3) 八个 (1,3)*8 –&gt;
将输出拼接成 (8,3) 矩阵与全连接层的系数矩阵进行相乘再 softmax 确定最后输出的 词向量 –&gt; (1,4)
注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数
qk相乘得到，词向量与其他词的attention分数( q1*(k1,k2,k3) )

多头注意力机制让一份词向量产生了多份答案，将每一份注意力机制的产物拼接，
获得了词向量在不同注意力矩阵运算后的分数，进行拼接后，softmax输出最注意的词，即是注意力机制。

多头注意力机制，将向量复制n份(n为多头头数)，投影到如512&#x2F;8 &#x3D; 64的64维的低维空间，最后将每一层的输出结果
此处为八层，8*64&#x3D;512 拼回512维的输出数据
由于Scale Dot Product 只是做乘法点积(向量变成qvk之后的attention运算)，没什么参数，因此重点学习的参数在Multi-Head的线性变换中，
即将 64*8的八份数据线性变换的下文中的W0，给模型八次机会希望能够学到什么，最后在拼接回来。&#x3D;&#x3D;



注意力机制流程：
q –&gt; 查询向量
set( k，v)    		k –&gt;关键字 v—-&gt; 值
如果 q对k的相似度很高，则输出v的概率也变高


’多头’注意力机制 
请注意并推演其词向量维度与系数矩阵带的行数


Scale Dot Product
step1
QK做点积，则输出每一行，是q与所有k的相乘相加结果，
α1 &#x3D; （q11k11+q12k21+q13k31 ,  q11k12+q12k22+q13k32 )
α2同理。
step2
所以得到了query1对所有key的相似度，最后每一行做个softmax进行概率分布。
除以根号dk是为了平滑梯度，具体来说：当概率趋近于1的时候softmax函数的梯度很小，除以dk让数值接近函数中部，梯度会比较陡峭
step3
将第二步的结果与V相乘得到最后的输出

Position Embedding位置编码是 将embedding好的词向量加上 position embedding vector 将信息融合，在注意力机制中进行计算。
(原文是使用sin cos将词向量份两部分进行编码， 本文中将交替使用sin cos，即单数sin 双数cos)
位置嵌入编码，主要是为了编辑定位词向量的位置以及词向量间的相对距离

pos为 词的种类数，为行标号
i 为特征维度
len(pos) * len(i)  表示为一position embedding 矩阵， 每一行为词的位置信息，每一列表示在特征上偏置，
将位置信息 融入 词向量信息 使词获得 时间上的相对信息










Residual 细节
Decoder 解码阶段Mask Multi-head与encoder不同的是，解码器在工作时会引入 Mask Multi-head 机制，将右侧的词盖住(设为负无穷或者别的)。
具体来说:

encoder 将生成的K和V矩阵传入 decoder 的 self-attention 模块中，而 decoder 将 mask 后的Q矩阵与其做attention。

mask做的事情




解码还是得一个个来的
时间维度 
在时间序列的情况下，词向量表示为，t1时刻的vector，t2时刻的vector….
mask做的事情就是将后面(右边)的 tn个时刻都屏蔽掉，
而Qmatrix的形成 将vector含有了其之后词的信息(共享了系数矩阵)，所以将其右边屏蔽。
则剔除了后面词的信息，从而不进行考虑。
Mask 细节mask就是为了阻止词知道后面的信息，具体来说就是QKV矩阵还相乘，但是引入-inf来阻止右边(后面的信息汇聚)

第一次点积：将Q和K矩阵相乘得到attention分数，
将右上角置零就会得到只含有本身信息和相对位置之前(左边)的信息，
且第二次点积: Mask(QK)与V相乘由下三角矩阵的性质，



注: mask去负无穷是因为 SoftMax中 e的指数形式只有在负无穷才为零，
这样相乘数据不会有一点影响，取其他值，都会影响softmax



总结
特别注意理解 attention机制将词向量之间的联系， attention分数
embedding方式为 词向量+位置编码向量
引入了 Residual
encoder-decoder层的传入为KV矩阵，decoder生成Q矩阵
Mask方式

</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/54367.html" title="Attention机制"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attention机制"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/54367.html" title="Attention机制">Attention机制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T09:00:10.386Z" title="发表于 2022-11-21 17:00:10">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T02:37:06.463Z" title="更新于 2022-11-27 10:37:06">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Attention/">Attention</a></span></div><div class="content">博客地址
传统Seq2Seq​	
动画连接
左侧为 input 将句子一个一个投入到 encoder 中，
encoder整个处理其相关性得到 context，吐给 decoder，
decoder 进行一个一个解码输出，得到整个翻译后的句子。
AttentionAn attention model differs from a classic sequence-to-sequence model in two main ways:

First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder:

​		注意力机制将产生的隐藏层信息(时间步骤信息)，全部保留，一次性传给 Decoder。



Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:

Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence

Give each hidden state a score (let’s ignore how the scoring is done for now)

Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores




​		decoder 将 encoder 输入的隐藏层的 vector 进行打分得到一个分数vector，
​		将分数 vector 做 softmax，得到一个权重 vector，
​		将权重 vector 与隐藏层 vector 相乘得到 注意力 vector，
​		最后把注意力 vector 进行相加就完成了。


注意: 将 encoder 的隐藏层信息传入 decoder之后，decoder 每一步都将使用其传入的隐藏层信息做 attention。


​			由上图可以看到，输出时 Attention 机制就是将注意力放在分数最高的向量上，所以，称之为’注意力机制’
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/45348.html" title="02 HuggingFace基础"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="02 HuggingFace基础"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45348.html" title="02 HuggingFace基础">02 HuggingFace基础</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.815Z" title="发表于 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-03T16:24:52.364Z" title="更新于 2022-12-04 00:24:52">2022-12-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Preprocessing/">Preprocessing</a></span></div><div class="content">待完成
示例详解

Transformer分两块BERT&amp;GPT都很能打
BERT用的是transformer的encoder

BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。


GPT用的是transformer的decoder

在它之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。



Bert的embeddingEmbedding由三种Embedding求和而成：

Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务

BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。


Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务

Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的


APItokenizer12345678910111213141516171819202122232425262728293031323334from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging# config模块MODEL_NAME=&quot;bert-base-chinese&quot;config = AutoConfig.from_pretrained(MODEL_NAME) #c onfig可以配置模型信息# tokenizer模块tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)tokenizer.all_special_ids # 查看特殊符号的id [100, 102, 0, 101, 103]tokenizer.all_special_tokens # 查看token  [&#x27;[UNK]&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;]tokenizer.vocab_size # 词汇表大小tokenizer.vocab # 词汇对应的dict形式## tokeningtext=&quot;我在北京工作&quot;token_ids=tokenizer.encode(text)token_ids # [101, 2769, 1762, 1266, 776, 2339, 868, 102]tokenizer.convert_ids_to_tokens(token_ids) # [&#x27;[CLS]&#x27;, &#x27;我&#x27;, &#x27;在&#x27;, &#x27;北&#x27;, &#x27;京&#x27;, &#x27;工&#x27;, &#x27;作&#x27;, &#x27;[SEP]&#x27;]		  # convert_tokens_to_ids(tokens) 为对应方法    ## padding 做向量填充token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)## encode_plustoken_ids=tokenizer.encode_plus(    text,padding=&quot;max_length&quot;,    max_length=30,    add_special_tokens=True,    return_tensors=&#x27;pt&#x27;,    return_token_type_ids=True,    return_attention_mask=True)



使用pre_train模型载入数据12model=AutoModel.from_pretrained(MODEL_NAME)outputs=model(token_ids[&#x27;input_ids&#x27;],token_ids[&#x27;attention_mask&#x27;])



数据集dataset定义12345678910111213141516171819202122232425262728293031323334class EnterpriseDataset(Dataset):    def __init__(self,texts,labels,tokenizer,max_len):        self.texts=texts        self.labels=labels        self.tokenizer=tokenizer        self.max_len=max_len    def __len__(self):        return len(self.texts)        def __getitem__(self,item):        &quot;&quot;&quot;        item 为数据索引，迭代取第item条数据        &quot;&quot;&quot;        text=str(self.texts[item])        label=self.labels[item]                encoding=self.tokenizer.encode_plus(            text,            add_special_tokens=True,            max_length=self.max_len,            return_token_type_ids=True,            pad_to_max_length=True,            return_attention_mask=True,            return_tensors=&#x27;pt&#x27;,  #转为tensor        )        #print(encoding[&#x27;input_ids&#x27;])        return &#123;            &#x27;texts&#x27;:text,            &#x27;input_ids&#x27;:encoding[&#x27;input_ids&#x27;].flatten(),            &#x27;attention_mask&#x27;:encoding[&#x27;attention_mask&#x27;].flatten(),            # toeken_type_ids:0            &#x27;labels&#x27;:torch.tensor(label,dtype=torch.long)        &#125;

</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客 <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">13</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">24k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-04T03:21:33.417Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">拥有一切 却变成太空</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>