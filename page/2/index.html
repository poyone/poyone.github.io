<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/2/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-11 00:26:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/4330.html" title="Transformer &amp; Self-Attention"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer &amp; Self-Attention"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/4330.html" title="Transformer &amp; Self-Attention">Transformer &amp; Self-Attention</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T09:00:10.398Z" title="发表于 2022-11-21 17:00:10">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-09T07:40:28.351Z" title="更新于 2022-12-09 15:40:28">2022-12-09</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">待完成
失效图片处理

阿三博客地址

李沐老师 48分钟讲解 encoder-decoder中(KV–Q)的运算: 

KQ相乘就是单个q对所有k的相似度作为attention score(给这个K值多少注意力)，与单个v做加权和(权值来自KQ)
再通过注意力分数与V向量相乘，得到每个V应该多大的缩放， 进行相加后就得到了最终V应该是什么样子了




李沐老师 56分 对multi-head输出和linear层相较于RNN的讲解：

词向量经过Attention层抓取全局信息，汇聚之后，在每个点上都有了所需要的信息
(权重不同，每个输出的向量的重点在不同的position编码位置上)，因此只需要做linear transformation。

bert中transformer参数计算:



embedding: vocab_size&#x3D;30522, max_position_embeddings&#x3D;512, token_type_embeddings&#x3D;2(就进行两句分别标记，多了截断)
​					（30522+512+2）*768 &#x3D; 23835648 (23M)
self-attention: 768&#x2F;12 &#x3D; 64 (多头每头分64维度的向量) ，64*768(每个64映射回768)，QKV三个矩阵, 
​						  最后一层 786(64 *12的拼接)-&gt;768的线性变换
​						(768&#x2F;12 * 768 3 ) * 12 + (768768) &#x3D; 2359296
​						经过12个transformer
​						2359296*12 &#x3D; 28311552 (28M)
feedfoward: 自注意力层之后 分别在 encoder 和 decoder 中有个一个全连接层
​						维度从 768-&gt;4*768_768-&gt;768
​						(768*4 * 768 )*2 &#x3D; 4718592
​						(768*4 * 768 )*2  * 12 &#x3D; 56623104 (56M)
layernorm: 有伽马和贝塔两个参数，embedding层（768 * 2），12层的self-attention，
​						768 * 2 + 768 * 2 * 2 * 12 &#x3D; 38400
总计: 23835648+28311552+56623104+38400 &#x3D; 108808704      				(108M)
每一层的参数为:  多头注意力的参数 + 拼接线性变换的参数 + feed-forward的参数 + layer-norm的参数
768 * 768 &#x2F; 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2 &#x3D; 7080960  (7M)

Encoder 编码阶段Multi-head Attention多头注意力机制将一个词向量留过八个 self-attention 头生成八个词向量 vector，
将八个词向量拼接，通过 fc 层进行 softmax 输出。
例如：
词向量为 (1,4) –&gt; 
经过 QKV 矩阵(系数) 得到 (1,3) 八个 (1,3)*8 –&gt;
将输出拼接成 (8,3) 矩阵与全连接层的系数矩阵进行相乘再 softmax 确定最后输出的 词向量 –&gt; (1,4)
注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数
qk相乘得到，词向量与其他词的attention分数( q1*(k1,k2,k3) )

多头注意力机制让一份词向量产生了多份答案，将每一份注意力机制的产物拼接，
获得了词向量在不同注意力矩阵运算后的分数，进行拼接后，softmax输出最注意的词，即是注意力机制。

多头注意力机制，将向量复制n份(n为多头头数)，投影到如512&#x2F;8 &#x3D; 64的64维的低维空间，最后将每一层的输出结果
此处为八层，8*64&#x3D;512 拼回512维的输出数据
由于Scale Dot Product 只是做乘法点积(向量变成qvk之后的attention运算)，没什么参数，因此重点学习的参数在Multi-Head的线性变换中，
即将 64*8的八份数据线性变换的下文中的W0，给模型八次机会希望能够学到什么，最后在拼接回来。&#x3D;&#x3D;



注意力机制流程：
q –&gt; 查询向量
set( k，v)    		k –&gt;关键字 v—-&gt; 值
如果 q对k的相似度很高，则输出v的概率也变高


’多头’注意力机制 
请注意并推演其词向量维度与系数矩阵带的行数


Scale Dot Product
step1
QK做点积，则输出每一行，是q与所有k的相乘相加结果，
α1 &#x3D; （q11k11+q12k21+q13k31 ,  q11k12+q12k22+q13k32 )
α2同理。
step2
所以得到了query1对所有key的相似度，最后每一行做个softmax进行概率分布。
除以根号dk是为了平滑梯度，具体来说：当概率趋近于1的时候softmax函数的梯度很小，除以dk让数值接近函数中部，梯度会比较陡峭
step3
将第二步的结果与V相乘得到最后的输出

Position Embedding位置编码是 将embedding好的词向量加上 position embedding vector 将信息融合，在注意力机制中进行计算。
(原文是使用sin cos将词向量份两部分进行编码， 本文中将交替使用sin cos，即单数sin 双数cos)
位置嵌入编码，主要是为了编辑定位词向量的位置以及词向量间的相对距离

pos为 词的种类数，为行标号
i 为特征维度
len(pos) * len(i)  表示为一position embedding 矩阵， 每一行为词的位置信息，每一列表示在特征上偏置，
将位置信息 融入 词向量信息 使词获得 时间上的相对信息










Residual 细节
Decoder 解码阶段Mask Multi-head与encoder不同的是，解码器在工作时会引入 Mask Multi-head 机制，将右侧的词盖住(设为负无穷或者别的)。
具体来说:

encoder 将生成的K和V矩阵传入 decoder 的 self-attention 模块中，而 decoder 将 mask 后的Q矩阵与其做attention。

mask做的事情




解码还是得一个个来的
时间维度 
在时间序列的情况下，词向量表示为，t1时刻的vector，t2时刻的vector….
mask做的事情就是将后面(右边)的 tn个时刻都屏蔽掉，
而Qmatrix的形成 将vector含有了其之后词的信息(共享了系数矩阵)，所以将其右边屏蔽。
则剔除了后面词的信息，从而不进行考虑。
Mask 细节mask就是为了阻止词知道后面的信息，具体来说就是QKV矩阵还相乘，但是引入-inf来阻止右边(后面的信息汇聚)

第一次点积：将Q和K矩阵相乘得到attention分数，
将右上角置零就会得到只含有本身信息和相对位置之前(左边)的信息，
且第二次点积: Mask(QK)与V相乘由下三角矩阵的性质，



注: mask去负无穷是因为 SoftMax中 e的指数形式只有在负无穷才为零，
这样相乘数据不会有一点影响，取其他值，都会影响softmax



总结
特别注意理解 attention机制将词向量之间的联系， attention分数
embedding方式为 词向量+位置编码向量
引入了 Residual
encoder-decoder层的传入为KV矩阵，decoder生成Q矩阵
Mask方式

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/54367.html" title="Attention机制"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attention机制"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/54367.html" title="Attention机制">Attention机制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T09:00:10.386Z" title="发表于 2022-11-21 17:00:10">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-09T07:40:19.051Z" title="更新于 2022-12-09 15:40:19">2022-12-09</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Attention/">Attention</a></span></div><div class="content">待完成
失效图片处理

博客地址
传统Seq2Seq​	
动画连接
左侧为 input 将句子一个一个投入到 encoder 中，
encoder整个处理其相关性得到 context，吐给 decoder，
decoder 进行一个一个解码输出，得到整个翻译后的句子。
AttentionAn attention model differs from a classic sequence-to-sequence model in two main ways:

First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder:

​		注意力机制将产生的隐藏层信息(时间步骤信息)，全部保留，一次性传给 Decoder。



Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:

Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence

Give each hidden state a score (let’s ignore how the scoring is done for now)

Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores




​		decoder 将 encoder 输入的隐藏层的 vector 进行打分得到一个分数vector，
​		将分数 vector 做 softmax，得到一个权重 vector，
​		将权重 vector 与隐藏层 vector 相乘得到 注意力 vector，
​		最后把注意力 vector 进行相加就完成了。


注意: 将 encoder 的隐藏层信息传入 decoder之后，decoder 每一步都将使用其传入的隐藏层信息做 attention。


​			由上图可以看到，输出时 Attention 机制就是将注意力放在分数最高的向量上，所以，称之为’注意力机制’
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/18130.html" title="Huggingface Course 03 微调范式"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface Course 03 微调范式"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/18130.html" title="Huggingface Course 03 微调范式">Huggingface Course 03 微调范式</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-06T10:52:16.506Z" title="发表于 2022-12-06 18:52:16">2022-12-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T11:09:23.577Z" title="更新于 2022-12-06 19:09:23">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">dataIn this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We’ve selected it for this chapter because it’s a small dataset, so it’s easy to experiment with training on it.
 let’s focus on the MRPC dataset! This is one of the 10 datasets composing the GLUE benchmark

使用的是MRPC，很小很好实验

它属于 GLUE


接下来查看下数据
12345678910111213141516171819from datasets import load_datasetraw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)raw_datasets&#x27;&#x27;&#x27;DatasetDict(&#123;    train: Dataset(&#123;        features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;],        num_rows: 3668    &#125;)    validation: Dataset(&#123;        features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;],        num_rows: 408    &#125;)    test: Dataset(&#123;        features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;],        num_rows: 1725    &#125;)&#125;)&#x27;&#x27;&#x27;



1234567raw_train_dataset = raw_datasets[&quot;train&quot;]raw_train_dataset[0]&#x27;&#x27;&#x27;&#123;&#x27;idx&#x27;: 0, &#x27;label&#x27;: 1, &#x27;sentence1&#x27;: &#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;, &#x27;sentence2&#x27;: &#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;&#125;&#x27;&#x27;&#x27;



123456raw_train_dataset.features&#x27;&#x27;&#x27;&#123;&#x27;sentence1&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;sentence2&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;not_equivalent&#x27;, &#x27;equivalent&#x27;], names_file=None, id=None), &#x27;idx&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#125;&#x27;&#x27;&#x27;



预处理tokenizer方法
123456tokenized_dataset = tokenizer(    raw_datasets[&quot;train&quot;][&quot;sentence1&quot;],    raw_datasets[&quot;train&quot;][&quot;sentence2&quot;],    padding=True,    truncation=True,)

This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the 🤗 Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).

占内存

dataset.map 方法
1234567891011121314151617181920def tokenize_function(example):    return tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True)    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)tokenized_datasets&#x27;&#x27;&#x27;DatasetDict(&#123;    train: Dataset(&#123;        features: [&#x27;attention_mask&#x27;, &#x27;idx&#x27;, &#x27;input_ids&#x27;, &#x27;label&#x27;, &#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;token_type_ids&#x27;],        num_rows: 3668    &#125;)    validation: Dataset(&#123;        features: [&#x27;attention_mask&#x27;, &#x27;idx&#x27;, &#x27;input_ids&#x27;, &#x27;label&#x27;, &#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;token_type_ids&#x27;],        num_rows: 408    &#125;)    test: Dataset(&#123;        features: [&#x27;attention_mask&#x27;, &#x27;idx&#x27;, &#x27;input_ids&#x27;, &#x27;label&#x27;, &#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;token_type_ids&#x27;],        num_rows: 1725    &#125;)&#125;)&#x27;&#x27;&#x27;



since the tokenizer works on lists of pairs of sentences, as seen before. This will allow us to use the option batched=True in our call to map(), which will greatly speed up the tokenization. The tokenizer is backed by a tokenizer written in Rust from the 🤗 Tokenizers library. This tokenizer can be very fast, but only if we give it lots of inputs at once.

batch

This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!

batch内最长padding，将在下一小节介绍DataCollatorWithPadding

You can even use multiprocessing when applying your preprocessing function with map() by passing along a num_proc argument. We didn’t do this here because the 🤗 Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.

多线程


but note that if you’re training on a TPU it can cause problems — TPUs prefer fixed shapes, even when that requires extra padding.

tpu更喜欢恒定形状，所以你很少数据with large pad 也没事


Dynamic paddingThe function that is responsible for putting together samples inside a batch is called a collate function. It’s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won’t be possible in our case since the inputs we have won’t all be of the same size.

DataCollator可以看成是一个函数，可以传入pytorch的dataloader的collate_fn参数

123from transformers import DataCollatorWithPaddingdata_collator = DataCollatorWithPadding(tokenizer=tokenizer) # 有model参数，可以把model也让collator知道



军火展示
12345678910111213samples = tokenized_datasets[&quot;train&quot;][:8]samples = &#123;k: v for k, v in samples.items() if k not in [&quot;idx&quot;, &quot;sentence1&quot;, &quot;sentence2&quot;]&#125;[len(x) for x in samples[&quot;input_ids&quot;]]&#x27;&#x27;&#x27;[50, 59, 47, 67, 59, 50, 62, 32]&#x27;&#x27;&#x27;batch = data_collator(samples)&#123;k: v.shape for k, v in batch.items()&#125;&#x27;&#x27;&#x27;&#123;&#x27;attention_mask&#x27;: torch.Size([8, 67]), &#x27;input_ids&#x27;: torch.Size([8, 67]), &#x27;token_type_ids&#x27;: torch.Size([8, 67]), &#x27;labels&#x27;: torch.Size([8])&#125;&#x27;&#x27;&#x27;



Fine-tuneTrainer首先是TrainingArguments的定义
The first step before we can define our Trainer is to define a TrainingArguments class that will contain all the hyperparameters the Trainer will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning.
123456from transformers import TrainingArgumentsfrom transformers import AutoModelForSequenceClassificationtraining_args = TrainingArguments(&quot;test-trainer&quot;)model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)



You will notice that unlike in Chapter 2, you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now.

使用细分模型 (automode后带任务名称的)不会得到警告，是因为他会加载 最后面那个多分类的权重给你，这样预训练就又快了些，上面写的head 应该是指classifier的那几层吧。

接下来可以train了
123456789101112from transformers import Trainertrainer = Trainer(    model,    training_args,    train_dataset=tokenized_datasets[&quot;train&quot;],    eval_dataset=tokenized_datasets[&quot;validation&quot;],    data_collator=data_collator,    tokenizer=tokenizer,)trainer.train() # 调用训练

Note that when you pass the tokenizer as we did here, the default data_collator used by the Trainer will be a DataCollatorWithPadding as defined previously, so you can skip the line data_collator=data_collator in this call.
This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps.

不写collate的话默认就是DataCollatorWithPadding，不过声明一下，比较好，为了可读性
每500步给你一个loss返回，看看need loss有多大

Evaluation1234predictions = trainer.predict(tokenized_datasets[&quot;validation&quot;])print(predictions.predictions.shape, predictions.label_ids.shape)# (408, 2) (408,)


predict的结果就是二分类的logits 接下来做个argmax 取位置信息即可

1234567891011import numpy as npimport evaluatepreds = np.argmax(predictions.predictions, axis=-1)metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)metric.compute(predictions=preds, references=predictions.label_ids)&#x27;&#x27;&#x27;&#123;&#x27;accuracy&#x27;: 0.8578431372549019, &#x27;f1&#x27;: 0.8996539792387542&#125;&#x27;&#x27;&#x27;

The table in the BERT paper reported an F1 score of 88.9 for the base model. That was the uncased model while we are currently using the cased model, which explains the better result.

上面说 微调的power！

12345def compute_metrics(eval_preds):    metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)    logits, labels = eval_preds    predictions = np.argmax(logits, axis=-1)    return metric.compute(predicti ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/64185.html" title="Huggingface Course 02 API概要"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface Course 02 API概要"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/64185.html" title="Huggingface Course 02 API概要">Huggingface Course 02 API概要</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-06T10:52:16.501Z" title="发表于 2022-12-06 18:52:16">2022-12-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T11:09:18.692Z" title="更新于 2022-12-06 19:09:18">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">outputsNote that the outputs of 🤗 Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[&quot;last_hidden_state&quot;]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).

HF的输入返回 大多以是元组或字典形式出，处理的时候要注意。

从二分类到多分类，多分类中每个类别分别作二分类，是否属于这个类别进行输出
[[0.2,0.8]、[0.4,0.6]、[0.7,0.3]]  &#x3D; [[1]、[1]、[0]]
Config12345678910111213141516171819from transformers import BertConfig, BertModel# Building the configconfig = BertConfig()# Building the model from the configmodel = BertModel(config)config&#x27;&#x27;&#x27;BertConfig &#123;  [...]  &quot;hidden_size&quot;: 768,  &quot;intermediate_size&quot;: 3072,  &quot;max_position_embeddings&quot;: 512,  &quot;num_attention_heads&quot;: 12,  &quot;num_hidden_layers&quot;: 12,  [...]&#125;&#x27;&#x27;&#x27;



环境变量The weights have been downloaded and cached (so future calls to the from_pretrained() method won’t re-download them) in the cache folder, which defaults to ~&#x2F;.cache&#x2F;huggingface&#x2F;transformers. You can customize your cache folder by setting the HF_HOME environment variable.

配置你当前的环境变量 os.environ[&#39;HF_HOME&#39;]= &#39;~/.cache/huggingface/transformers&#39; 

Saving12345678model.save_pretrained(&quot;directory_on_my_computer&quot;)&#x27;&#x27;&#x27;This saves two files to your disk:ls directory_on_my_computerconfig.json pytorch_model.bin&#x27;&#x27;&#x27;


If you take a look at the config.json file, you’ll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what 🤗 Transformers version you were using when you last saved the checkpoint

The pytorch_model.bin file is known as the state dictionary; it contains all your model’s weights. The two files go hand in hand; the configuration is necessary to know your model’s architecture, while the model weights are your model’s parameters.


Tokenizerencode通过tokenizer.tokenize(sequence)查看分词后的结果
12345678910from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)sequence = &quot;Using a Transformer network is simple&quot;tokens = tokenizer.tokenize(sequence)print(tokens)&#x27;&#x27;&#x27;[&#x27;Using&#x27;, &#x27;a&#x27;, &#x27;transform&#x27;, &#x27;##er&#x27;, &#x27;network&#x27;, &#x27;is&#x27;, &#x27;simple&#x27;]&#x27;&#x27;&#x27;



ids = tokenizer.convert_tokens_to_ids(tokens)
还可以反过来得到token，也就是跟上面的 tokenize(seq) 一样的效果
123ids = tokenizer.convert_tokens_to_ids(tokens)print(ids)



decode12345decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])print(decoded_string)&#x27;&#x27;&#x27;&#x27;Using a Transformer network is simple&#x27;&#x27;&#x27;&#x27;



padding查看tokenizer.pad_token_id pad的id
123456789# Will pad the sequences up to the maximum sequence lengthmodel_inputs = tokenizer(sequences, padding=&quot;longest&quot;)# Will pad the sequences up to the model max length# (512 for BERT or DistilBERT)model_inputs = tokenizer(sequences, padding=&quot;max_length&quot;)# Will pad the sequences up to the specified max lengthmodel_inputs = tokenizer(sequences, padding=&quot;max_length&quot;, max_length=8)


直接max_length是到模型的最大长度，longest是到批次里句子的最大长度

1234567891011sequence1_ids = [[200, 200, 200]]sequence2_ids = [[200, 200]]batched_ids = [    [200, 200, 200],    [200, 200, tokenizer.pad_token_id],]&#x27;&#x27;&#x27;tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward&gt;)tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward&gt;)tensor([[ 1.5694, -1.3895],        [ 1.3373, -1.2163]], grad_fn=&lt;AddmmBackward&gt;)&#x27;&#x27;&#x27;



This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.

这里两条单独的数据的结果跟组合起来的是不同的，是因为pad的位置也分散了模型的注意力，这不是我们希望模型学习的地方

ATTENTION MASK123456789101112131415batched_ids = [    [200, 200, 200],    [200, 200, tokenizer.pad_token_id],]attention_mask = [    [1, 1, 1],    [1, 1, 0],]outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))print(outputs.logits)&#x27;&#x27;&#x27;tensor([[ 1.5694, -1.3895],        [ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward&gt;)&#x27;&#x27;&#x27;



长句子处理Models have different supported sequence lengths, and some specialize in handling very long sequences. Longformer is one example, and another is LED. If you’re working on a task that requires very long sequences, we recommend you take a look at those models.
一般是在tokenizer里设置truncation max_len，这里没讲，可以去看看模型。
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/16149.html" title="Huggingface Course 01 基础概念"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface Course 01 基础概念"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/16149.html" title="Huggingface Course 01 基础概念">Huggingface Course 01 基础概念</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-06T10:52:16.495Z" title="发表于 2022-12-06 18:52:16">2022-12-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T11:12:26.942Z" title="更新于 2022-12-06 19:12:26">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">Some of the currently available pipelines are:

feature-extraction (get the vector representation of a text)
fill-mask
ner (named entity recognition)
question-answering
sentiment-analysis
summarization
text-generation
translation
zero-shot-classification

pipeline12345678from transformers import pipelinegenerator = pipeline(&quot;text-generation&quot;, model=&quot;distilgpt2&quot;)generator(    &quot;In this course, we will teach you how to&quot;,    max_length=30,    num_return_sequences=2,)





model模型发展时间史

June 2018: GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results

October 2018: BERT, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)

February 2019: GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns

October 2019: DistilBERT, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance

October 2019: BART and T5, two large pretrained models using the same architecture as the original Transformer model (the first to do so)

May 2020, GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning)

GPT-like (also called auto-regressive Transformer models)

BERT-like (also called auto-encoding Transformer models)

BART&#x2F;T5-like (also called sequence-to-sequence Transformer models)


encoder-decoder特攻类编码器的主要用处

Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.
ALBERT
BERT
DistilBERT
ELECTRA
RoBERTa


Decoder-only models: Good for generative tasks such as text generation
CTRL
GPT
GPT-2
Transformer XL.


Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.
BART
mBART
Marian
T5



cross-attention层使得decoder能查看整个句意，以调整顺序翻译输出
Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.
架构和检查点

Architecture: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.
Checkpoints: These are the weights that will be loaded in a given architecture.
Model: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.

For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”
即使使用干净的词库，也可能产生性别歧视，种族歧视
When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter&#x2F;waitress). The others are work occupations usually associated with one specific gender — and yes, prostitute ended up in the top 5 possibilities the model associates with “woman” and “work.” This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it’s trained on the English Wikipedia and BookCorpus datasets).
When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won’t make this intrinsic bias disappear.
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/2534.html" title="22-12-3 e.g etc. 详解"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/made in Abyss/nanachi02.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="22-12-3 e.g etc. 详解"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/2534.html" title="22-12-3 e.g etc. 详解">22-12-3 e.g etc. 详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-04T16:08:37.456Z" title="发表于 2022-12-05 00:08:37">2022-12-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-04T16:38:46.207Z" title="更新于 2022-12-05 00:38:46">2022-12-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E8%8B%B1%E8%AF%AD/">英语</a></span></div><div class="content">
“e.g.”是拉丁文“exempli gratia”的縮寫，意思是「例如、舉例來說」，就是英文的for example或for instance。

ex.的意思是「範例或練習」，也就是英文的example或exercise。所以別再把ex.放到句子裡當「例如」用了喔。

還有“i.e.”也是非常讓人困惑的縮寫，“i.e.”其實是拉丁文“id est”的縮寫，意指「換句話說」。英文就是that is &#x2F; in other words，目的是給予更多資訊來闡述前面所說的話。

除此之外，在舉例的時候還要特別注意etc.的正確用法，etc.是“et cetera”的縮寫，意思是「等等」，相當於“…and so on.”。
提醒大家，“e.g.”和“etc.”不能出現在同一個句子中喔。因為e.g.是舉例來說，意旨舉幾個例子，所以其中就已經有「等等」的含意，因此若再加上etc.就是畫蛇添足了。


</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客 <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">20</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">33.1k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-10T16:26:55.466Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">8</span><span class="categoryBar-list-descr">拥有一切 却变成太空</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>