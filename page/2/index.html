<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/2/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-11-27 20:19:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/4330.html" title="Transformer &amp; Self-Attention"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer &amp; Self-Attention"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/4330.html" title="Transformer &amp; Self-Attention">Transformer &amp; Self-Attention</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T09:00:10.398Z" title="发表于 2022-11-21 17:00:10">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T02:37:17.317Z" title="更新于 2022-11-27 10:37:17">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">阿三博客地址
李沐老师 48分钟讲解 encoder-decoder中(KV–Q)的运算: 
KQ相乘就是单个q对所有k的相似度作为attention score(给这个K值多少注意力)，与单个v做加权和(权值来自KQ)
再通过注意力分数与V向量相乘，得到每个V应该多大的缩放， 进行相加后就得到了最终V应该是什么样子了
李沐老师 56分 对multi-head输出和linear层相较于RNN的讲解：
词向量经过Attention层抓取全局信息，汇聚之后，在每个点上都有了所需要的信息
(权重不同，每个输出的向量的重点在不同的position编码位置上)，因此只需要做linear transformation。
bert中transformer参数计算:
embedding: vocab_size&#x3D;30522, max_position_embeddings&#x3D;512, token_type_embeddings&#x3D;2(就进行两句分别标记，多了截断)
​					（30522+512+2）*768 &#x3D; 23835648 (23M)
self-attention: 768&#x2F;12 &#x3D; 64 (多头每头分64维度的向量) ，64*768(每个64映射回768)，QKV三个矩阵, 
​						  最后一层 786(64 *12的拼接)-&gt;768的线性变换
​						(768&#x2F;12 * 768 3 ) * 12 + (768768) &#x3D; 2359296
​						经过12个transformer
​						2359296*12 &#x3D; 28311552 (28M)
feedfoward: 自注意力层之后 分别在 encoder 和 decoder 中有个一个全连接层
​						维度从 768-&gt;4*768_768-&gt;768
​						(768*4 * 768 )*2 &#x3D; 4718592
​						(768*4 * 768 )*2  * 12 &#x3D; 56623104 (56M)
layernorm: 有伽马和贝塔两个参数，embedding层（768 * 2），12层的self-attention，
​						768 * 2 + 768 * 2 * 2 * 12 &#x3D; 38400
总计: 23835648+28311552+56623104+38400 &#x3D; 108808704      				(108M)
每一层的参数为:  多头注意力的参数 + 拼接线性变换的参数 + feed-forward的参数 + layer-norm的参数
768 * 768 &#x2F; 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2 &#x3D; 7080960  (7M)
Encoder 编码阶段Multi-head Attention多头注意力机制将一个词向量留过八个 self-attention 头生成八个词向量 vector，
将八个词向量拼接，通过 fc 层进行 softmax 输出。
例如：
词向量为 (1,4) –&gt; 
经过 QKV 矩阵(系数) 得到 (1,3) 八个 (1,3)*8 –&gt;
将输出拼接成 (8,3) 矩阵与全连接层的系数矩阵进行相乘再 softmax 确定最后输出的 词向量 –&gt;
(1,4)
注意 QKV矩阵怎么来的(attention分数)，最后为什么要拼接，以及FC层的系数
qk相乘得到，词向量与其他词的attention分数( q1*(k1,k2,k3) )

多头注意力机制让一份词向量产生了多份答案，将每一份注意力机制的产物拼接，
获得了词向量在不同注意力矩阵运算后的分数，进行拼接后，softmax输出最注意的词，即是注意力机制。

多头注意力机制，将向量复制n份(n为多头头数)，投影到如512&#x2F;8 &#x3D; 64的64维的低维空间，最后将每一层的输出结果
此处为八层，8*64&#x3D;512 拼回512维的输出数据
由于Scale Dot Product 只是做乘法点积(向量变成qvk之后的attention运算)，没什么参数，因此重点学习的参数在Multi-Head的线性变换中，
即将 64*8的八份数据线性变换的下文中的W0，给模型八次机会希望能够学到什么，最后在拼接回来。&#x3D;&#x3D;


注意力机制流程：
q –&gt; 查询向量
set( k，v)    		k –&gt;关键字 v—-&gt; 值
如果 q对k的相似度很高，则输出v的概率也变高


’多头’注意力机制 
请注意并推演其词向量维度与系数矩阵带的行数

Scale Dot Product
step1
QK做点积，则输出每一行，是q与所有k的相乘相加结果，
α1 &#x3D; （q11k11+q12k21+q13k31 ,  q11k12+q12k22+q13k32 )
α2同理。
step2
所以得到了query1对所有key的相似度，最后每一行做个softmax进行概率分布。
除以根号dk是为了平滑梯度，具体来说：当概率趋近于1的时候softmax函数的梯度很小，除以dk让数值接近函数中部，梯度会比较陡峭
step3
将第二步的结果与V相乘得到最后的输出
Position Embedding位置编码是 将embedding好的词向量加上 position embedding vector 将信息融合，在注意力机制中进行计算。
(原文是使用sin cos将词向量份两部分进行编码， 本文中将交替使用sin cos，即单数sin 双数cos)
位置嵌入编码，主要是为了编辑定位词向量的位置以及词向量间的相对距离

pos为 词的种类数，为行标号
i 为特征维度
len(pos) * len(i)  表示为一position embedding 矩阵， 每一行为词的位置信息，每一列表示在特征上偏置，
将位置信息 融入 词向量信息 使词获得 时间上的相对信息





Residual 细节
Decoder 解码阶段Mask Multi-head与encoder不同的是，解码器在工作时会引入 Mask Multi-head 机制，将右侧的词盖住(设为负无穷或者别的)。
具体来说:

encoder 将生成的K和V矩阵传入 decoder 的 self-attention 模块中，而 decoder 将产生 mask 后的Q矩阵与其做attention。

mask做的事情




没太懂，这张图，这里可以实现并行化嘛？&#x3D;&#x3D;解释在Mask细节&#x3D;&#x3D;
按照小蛮的矩阵，右侧的是可以屏蔽的，但是扩展成n为的词向量怎么屏蔽呢？
在生成z矩阵屏蔽？ 不可能，已经参加计算了。
所以不能并行输出？ 只能一个一个吐？等下看下小蛮视频、huggingface教程也行
时间维度 
在时间序列的情况下，词向量表示为，t1时刻的vector，t2时刻的vector….
mask做的事情就是将后面(右边)的 tn个时刻都屏蔽掉，
而Qmatrix的形成 将vector含有了其之后词的信息(共享了系数矩阵)，所以将其右边屏蔽。
则剔除了后面词的信息，从而不进行考虑。
Mask 细节mask就是为了阻止词知道后面的信息，具体来说就是QKV矩阵还相乘，但是引入-inf来阻止右边(后面的信息汇聚)

同小蛮的流程，第一次点积：将Q和K矩阵相乘得到attention分数，
将右上角置零就会得到只含有本身信息和相对位置之前(左边)的信息，
且第二次点积: Mask(QK)与V相乘由下三角矩阵的性质，

第一行(t1时刻)只考虑第一个值的输出
第一行(t2时刻)考虑第一个和第二个值的输出
….
这样就可以实现 tn时刻只考虑 t1 到 tn-1的输出
如此便可实现并行化。 (encode到decode还是串行的)


注: mask去负无穷是因为 SoftMax中 e的指数形式只有在负无穷才为零，
这样相乘数据不会有一点影响，取其他值，都会影响softmax



总结
特别注意理解 attention机制将词向量之间的联系， attention分数
embedding方式为 词向量+位置编码向量
引入了 Residual
encoder-decoder层的传入为KV矩阵，decoder生成Q矩阵
Mask方式

尚未明晰:
multi-head 将输入的向量均分八等分？分别做 self-attention？ 减少参数加快运算，结果还差不多？
multi-head 处理的向量是在不同维度处理？ 比如 head1是词义，head2是位置等等。

(来自小蛮视频)
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/54367.html" title="Attention机制"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attention机制"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/54367.html" title="Attention机制">Attention机制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T09:00:10.386Z" title="发表于 2022-11-21 17:00:10">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T02:37:06.463Z" title="更新于 2022-11-27 10:37:06">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Attention/">Attention</a></span></div><div class="content">博客地址
传统Seq2Seq​	
动画连接
左侧为 input 将句子一个一个投入到 encoder 中，
encoder整个处理其相关性得到 context，吐给 decoder，
decoder 进行一个一个解码输出，得到整个翻译后的句子。
AttentionAn attention model differs from a classic sequence-to-sequence model in two main ways:

First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder:

​		注意力机制将产生的隐藏层信息(时间步骤信息)，全部保留，一次性传给 Decoder。



Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:

Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence

Give each hidden state a score (let’s ignore how the scoring is done for now)

Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores




​		decoder 将 encoder 输入的隐藏层的 vector 进行打分得到一个分数vector，
​		将分数 vector 做 softmax，得到一个权重 vector，
​		将权重 vector 与隐藏层 vector 相乘得到 注意力 vector，
​		最后把注意力 vector 进行相加就完成了。


注意: 将 encoder 的隐藏层信息传入 decoder之后，decoder 每一步都将使用其传入的隐藏层信息做 attention。


​			由上图可以看到，输出时 Attention 机制就是将注意力放在分数最高的向量上，所以，称之为’注意力机制’
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/45348.html" title="02 HuggingFace基础"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris44.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="02 HuggingFace基础"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45348.html" title="02 HuggingFace基础">02 HuggingFace基础</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.815Z" title="发表于 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T05:53:39.615Z" title="更新于 2022-11-27 13:53:39">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Preprocessing/">Preprocessing</a></span></div><div class="content">Transformer分两块BERT&amp;GPT都很能打
BERT用的是transformer的encoder

BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。


GPT用的是transformer的decoder

在它之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。



Bert的embeddingEmbedding由三种Embedding求和而成：

Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务

BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。


Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务

Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的


APItokenizer12345678910111213141516171819202122232425262728293031323334from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging# config模块MODEL_NAME=&quot;bert-base-chinese&quot;config = AutoConfig.from_pretrained(MODEL_NAME) #c onfig可以配置模型信息# tokenizer模块tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)tokenizer.all_special_ids # 查看特殊符号的id [100, 102, 0, 101, 103]tokenizer.all_special_tokens # 查看token  [&#x27;[UNK]&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;]tokenizer.vocab_size # 词汇表大小tokenizer.vocab # 词汇对应的dict形式## tokeningtext=&quot;我在北京工作&quot;token_ids=tokenizer.encode(text)token_ids # [101, 2769, 1762, 1266, 776, 2339, 868, 102]tokenizer.convert_ids_to_tokens(token_ids) # [&#x27;[CLS]&#x27;, &#x27;我&#x27;, &#x27;在&#x27;, &#x27;北&#x27;, &#x27;京&#x27;, &#x27;工&#x27;, &#x27;作&#x27;, &#x27;[SEP]&#x27;]		  # convert_tokens_to_ids(tokens) 为对应方法    ## padding 做向量填充token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)## encode_plustoken_ids=tokenizer.encode_plus(    text,padding=&quot;max_length&quot;,    max_length=30,    add_special_tokens=True,    return_tensors=&#x27;pt&#x27;,    return_token_type_ids=True,    return_attention_mask=True)



使用pre_train模型载入数据12model=AutoModel.from_pretrained(MODEL_NAME)outputs=model(token_ids[&#x27;input_ids&#x27;],token_ids[&#x27;attention_mask&#x27;])



数据集dataset定义12345678910111213141516171819202122232425262728293031323334class EnterpriseDataset(Dataset):    def __init__(self,texts,labels,tokenizer,max_len):        self.texts=texts        self.labels=labels        self.tokenizer=tokenizer        self.max_len=max_len    def __len__(self):        return len(self.texts)        def __getitem__(self,item):        &quot;&quot;&quot;        item 为数据索引，迭代取第item条数据        &quot;&quot;&quot;        text=str(self.texts[item])        label=self.labels[item]                encoding=self.tokenizer.encode_plus(            text,            add_special_tokens=True,            max_length=self.max_len,            return_token_type_ids=True,            pad_to_max_length=True,            return_attention_mask=True,            return_tensors=&#x27;pt&#x27;,  #转为tensor        )        #print(encoding[&#x27;input_ids&#x27;])        return &#123;            &#x27;texts&#x27;:text,            &#x27;input_ids&#x27;:encoding[&#x27;input_ids&#x27;].flatten(),            &#x27;attention_mask&#x27;:encoding[&#x27;attention_mask&#x27;].flatten(),            # toeken_type_ids:0            &#x27;labels&#x27;:torch.tensor(label,dtype=torch.long)        &#125;

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/45347.html" title="01 HuggingFace基础"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka2.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="01 HuggingFace基础"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45347.html" title="01 HuggingFace基础">01 HuggingFace基础</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.804Z" title="发表于 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T05:53:35.623Z" title="更新于 2022-11-27 13:53:35">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Preprocessing/">Preprocessing</a></span></div><div class="content">Attention 原文
Why
全面拥抱Transformer：NLP三大特征抽取器(CNN&#x2F;RNN&#x2F;TF)中，近两年新欢Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。

像Wordvec出现之后一样，在人工智能领域种各种目标皆可向量化，也就是我们经常听到的“万物皆可Embedding”。而Transformer模型和Bert模型的出现，更是NLP领域划时代的产物：将transformer和双向语言模型进行融合，便得到NLP划时代的，也是当下在各自NLP下流任务中获得state-of-the-art的模型-BERT

BERT起源于预训练的上下文表示学习，与之前的模型不同，BERT是一种深度双向的、无监督的语言表示，且仅使用纯文本语料库进行预训练的模型。上下文无关模型（如word2vec或GloVe）为词汇表中的每个单词生成一个词向量表示，因此容易出现单词的歧义问题。BERT考虑到单词出现时的上下文。例如，词“水分”的word2vec词向量在“植物需要吸收水分”和“财务报表里有水分”是相同的，但BERT根据上下文的不同提供不同的词向量，词向量与句子表达的句意有关。


Embedding：

首先类似 word2vec 的 token化，再进行片段标记( segment )，最后 ids 的位置编码(  position )
编码后一个 ’词‘ 有三个信息，token、段落位置信息、绝对位置信息( id: 1、2、3…)

Embedding解决的问题:
首先是之前用的 One-Hot Key，高维度，离散的，低信息密度的储存形式
其次是更好的 Contextual Similarity，上下文相关相似性。

Preview Api前置查看：123456from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained(&quot;bert-base-chinese&quot;) # 获取相应模型的tokenizerfrom transformers import AutoTokenizer, AutoModelForMaskedLMmodel = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-chinese&quot;) #查看模型的分层



函数调用：字典大小，token化，ids化12345678910vocab = tokenizer.vocabprint(&quot;字典大小：&quot;, len(vocab)) 	# 查看字典大小text = &quot;[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。&quot;tokens = tokenizer.tokenize(text)				# 将文字分词ids = tokenizer.convert_tokens_to_ids(tokens)	# 将文字转化为数字，进行编码&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;等&#x27;, &#x27;到&#x27;, &#x27;潮&#x27;, &#x27;水&#x27;, &#x27;[MASK]&#x27;, &#x27;了&#x27;, &#x27;，&#x27;, &#x27;就&#x27;, &#x27;知&#x27;] ...[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ... &#x27;&#x27;&#x27;

Mask模型的使用1234567891011121314151617181920212223242526from transformers import BertForMaskedLM# 除了 tokens 以外我們還需要辨別句子的 segment idstokens_tensor = torch.tensor([ids])  # (1, seq_len)segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)# 使用 masked LM 估計 [MASK] 位置所代表的實際 token maskedLM_model.eval()with torch.no_grad():    outputs = maskedLM_model(tokens_tensor, segments_tensors)    predictions = outputs[0]    # (1, seq_len, num_hidden_units)del maskedLM_model# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來masked_index = 5k = 3probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())# 顯示 top k 可能的字。一般我們就是取 top 1 当做预测值print(&quot;輸入 tokens ：&quot;, tokens[:10], &#x27;...&#x27;)print(&#x27;-&#x27; * 50)for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):    tokens[masked_index] = t    print(&quot;Top &#123;&#125; (&#123;:2&#125;%)：&#123;&#125;&quot;.format(i, int(p.item() * 100), tokens[:10]), &#x27;...&#x27;)

​	輸入 tokens ： [‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘[MASK]’, ‘了’, ‘，’, ‘就’, ‘知’] …​	Top 1 (65%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘来’, ‘了’, ‘，’, ‘就’, ‘知’] …​	Top 2 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘过’, ‘了’, ‘，’, ‘就’, ‘知’] …​	Top 3 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘干’, ‘了’, ‘，’, ‘就’, ‘知’] …
可视化模型: bertvizPandas预处理文本
多使用自定义函数
nltk库的stopwords
textblob库的拼写检查、词干抽取、词性还原等

文本数据的基本体征提取
词汇数量

字符数量

平均字长

停用词数量

特殊字符数量

数字数量

大写字母数量


文本数据的基本预处理
小写转换
去除标点符号
去除停用词
去除频现词
去除稀疏词
拼写校正
分词(tokenization)
词干提取(stemming)
词形还原(lemmatization)

</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客 <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">10</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">18.8k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-11-27T12:19:03.080Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">拥有一切 却变成太空</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>