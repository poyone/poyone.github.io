<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/2/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-16 19:17:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/5596.html" title="HF Course 05 faiss æœç´¢å¼•æ“"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 05 faiss æœç´¢å¼•æ“"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/5596.html" title="HF Course 05 faiss æœç´¢å¼•æ“">HF Course 05 faiss æœç´¢å¼•æ“</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T09:15:41.640Z" title="å‘è¡¨äº 2022-12-11 17:15:41">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:27:52.986Z" title="æ›´æ–°äº 2022-12-12 22:27:52">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">åœ¨æˆ‘ä»¬åˆ›å»ºå¥½è‡ªå·±çš„æ•°æ®é›†åï¼Œå¯ä»¥ç”¨faiss å’Œ hf æ¥æœç´¢ä¸€äº›æ•°æ®ã€‚
æˆ‘ä»¬é€šè¿‡multi-qa-mpnet-base-dot-v1æ¨¡å‹embeddingæˆ‘ä»¬çš„æ•°æ®ï¼Œç„¶åé€šè¿‡ faissç»™æ¯ä¸ªembeddingå¾—åˆ°index
æœ€åå°†æˆ‘ä»¬çš„query ç»™tokenizerè½¬æ¢ä¹‹åå–‚ç»™æ¨¡å‹ï¼Œå¾—åˆ°æœ€åŒ¹é…æˆ‘ä»¬é—®é¢˜çš„æ•°æ®ã€‚

Fortunately, thereâ€™s a library called sentence-transformers that is dedicated to creating embeddings. As described in the libraryâ€™s documentation, our use case is an example of asymmetric semantic search because we have a short query whose answer weâ€™d like to find in a longer document, like a an issue comment. The handy model overview table in the documentation indicates that the multi-qa-mpnet-base-dot-v1 checkpoint has the best performance for semantic search, so weâ€™ll use that for our application.
æˆ‘ä»¬ä¸»è¦ä½¿ç”¨äº†sentence-transformers faissä¸¤ä¸ªé¢å¤–åº“å¤„ç†

åŠ è½½æ¨¡å‹12345from transformers import AutoTokenizer, AutoModelmodel_ckpt = &quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;tokenizer = AutoTokenizer.from_pretrained(model_ckpt)model = AutoModel.from_pretrained(model_ckpt)



æ•°æ®å¤„ç†123456789101112131415import torchdevice = torch.device(&quot;cuda&quot;)model.to(device)def cls_pooling(model_output):    return model_output.last_hidden_state[:, 0]    def get_embeddings(text_list):    encoded_input = tokenizer(        text_list, padding=True, truncation=True, return_tensors=&quot;pt&quot;    )    encoded_input = &#123;k: v.to(device) for k, v in encoded_input.items()&#125;    model_output = model(**encoded_input)    return cls_pooling(model_output)



åŠ å…¥ faiss çš„index12345embeddings_dataset = comments_dataset.map(    lambda x: &#123;&quot;embeddings&quot;: get_embeddings(x[&quot;text&quot;]).detach().cpu().numpy()[0]&#125;)embeddings_dataset.add_faiss_index(column=&quot;embeddings&quot;)



æµ‹è¯•1234question = &quot;How can I load a dataset offline?&quot;question_embedding = get_embeddings([question]).cpu().detach().numpy()question_embedding.shape# torch.Size([1, 768])



123scores, samples = embeddings_dataset.get_nearest_examples(    &quot;embeddings&quot;, question_embedding, k=5)



æŸ¥çœ‹ç»“æœ12345import pandas as pdsamples_df = pd.DataFrame.from_dict(samples)samples_df[&quot;scores&quot;] = scoressamples_df.sort_values(&quot;scores&quot;, ascending=False, inplace=True)



1234567for _, row in samples_df.iterrows():    print(f&quot;COMMENT: &#123;row.comments&#125;&quot;)    print(f&quot;SCORE: &#123;row.scores&#125;&quot;)    print(f&quot;TITLE: &#123;row.title&#125;&quot;)    print(f&quot;URL: &#123;row.html_url&#125;&quot;)    print(&quot;=&quot; * 50)    print()



å¯ä»¥æŸ¥çœ‹æœ€åŒ¹é…çš„è¯„è®º
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;&quot;&quot;COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how `transformers` loads models offline fine.@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?SCORE: 25.505046844482422TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)You can now use them offline\`\`\`pythondatasets = load_dataset(&quot;text&quot;, data_files=data_files)\`\`\`We&#x27;ll do a new release soonSCORE: 24.555509567260742TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.----------&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\`\`\`pythonload_dataset(&quot;./my_dataset&quot;)\`\`\`and the dataset script will generate your dataset once and for all.----------About I&#x27;m looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.cf #1724SCORE: 24.14896583557129TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine&gt;&gt; 1. (online machine)&gt;

import datasets
data &#x3D; datasets.load_dataset(â€¦)
data.save_to_disk(&#x2F;YOUR&#x2F;DATASET&#x2F;DIR)
123452. copy the dir from online to the offline machine3. (offline machine)

import datasets
data &#x3D; datasets.load_from_disk(&#x2F;SAVED&#x2F;DATA&#x2F;DIR)
12345678910111213141516171819202122232425262728293031HTH.SCORE: 22.893993377685547TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: here is my way to load a dataset offline, but it **requires** an online machine1. (online machine)\`\`\`import datasetsdata = datasets.load_dataset(...)data.save_to_disk(/YOUR/DATASET/DIR)\`\`\`2. copy the dir from online to the offline machine3. (offline machine)\`\`\`import datasetsdata = datasets.load_from_disk(/SAVED/DATA/DIR)\`\`\`HTH.SCORE: 22.406635284423828TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================&quot;&quot;&quot;

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/4498.html" title="HF Course 04 Dataset"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 04 Dataset"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/4498.html" title="HF Course 04 Dataset">HF Course 04 Dataset</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T07:18:25.982Z" title="å‘è¡¨äº 2022-12-11 15:18:25">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:27:25.372Z" title="æ›´æ–°äº 2022-12-12 22:27:25">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">åŠ è½½æœ¬åœ°æ•°æ®


Data format
Loading script
Example



CSV &amp; TSV
csv
load_dataset(&quot;csv&quot;, data_files=&quot;my_file.csv&quot;)


Text files
text
load_dataset(&quot;text&quot;, data_files=&quot;my_file.txt&quot;)


JSON &amp; JSON Lines
json
load_dataset(&quot;json&quot;, data_files=&quot;my_file.jsonl&quot;)


Pickled DataFrames
pandas
load_dataset(&quot;pandas&quot;, data_files=&quot;my_dataframe.pkl&quot;)


åˆ†åˆ«éœ€è¦åšï¼ŒæŒ‡æ˜æ•°æ®ç±»å‹ï¼ŒæŒ‡æ˜æ–‡ä»¶è·¯å¾„
data_fileså‚æ•°The data_files argument of the load_dataset() function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting data_files=&quot;*.json&quot;). See the ğŸ¤— Datasets documentation for more details.

å¯ä»¥åšæ–‡ä»¶è·¯å¾„

å¯ä»¥åšsplitå°†æ•°æ®æ˜ å°„æˆæƒ³è¦çš„å­—å…¸æ ¼å¼

&#96;&#96;&#96;pythondata_files &#x3D; {â€œtrainâ€: â€œSQuAD_it-train.jsonâ€, â€œtestâ€: â€œSQuAD_it-test.jsonâ€}squad_it_dataset &#x3D; load_dataset(â€œjsonâ€, data_files&#x3D;data_files, field&#x3D;â€dataâ€)squad_it_dataset
â€˜â€™â€™DatasetDict({train: Dataset({    features: [â€˜titleâ€™, â€˜paragraphsâ€™],    num_rows: 442})test: Dataset({    features: [â€˜titleâ€™, â€˜paragraphsâ€™],    num_rows: 48})})â€™â€™â€™
123456789101112    ## åŠ è½½æœåŠ¡å™¨æ•°æ®```pythonurl = &quot;https://github.com/crux82/squad-it/raw/master/&quot;data_files = &#123;    &quot;train&quot;: url + &quot;SQuAD_it-train.json.gz&quot;,    &quot;test&quot;: url + &quot;SQuAD_it-test.json.gz&quot;,&#125;squad_it_dataset = load_dataset(&quot;json&quot;, data_files=data_files, field=&quot;data&quot;)



æ•°æ®å¤„ç†åˆ†éš”ç¬¦å¦‚æœä½ çš„æ•°æ®ä¸æ˜¯ä¼ ç»Ÿçš„CSVæ ¼å¼(ä»¥é€—å·åˆ†å‰²)ï¼Œä½ å¯ä»¥æŒ‡å®šåˆ†éš”ç¬¦
12345from datasets import load_datasetdata_files = &#123;&quot;train&quot;: &quot;drugsComTrain_raw.tsv&quot;, &quot;test&quot;: &quot;drugsComTest_raw.tsv&quot;&#125;# \t is the tab character in Pythondrug_dataset = load_dataset(&quot;csv&quot;, data_files=data_files, delimiter=&quot;\t&quot;)



éšæœºé€‰å–æ ·æœ¬1234567891011121314drug_sample = drug_dataset[&quot;train&quot;].shuffle(seed=42).select(range(1000))# Peek at the first few examplesdrug_sample[:3]&#x27;&#x27;&#x27;&#123;&#x27;Unnamed: 0&#x27;: [87571, 178045, 80482], &#x27;drugName&#x27;: [&#x27;Naproxen&#x27;, &#x27;Duloxetine&#x27;, &#x27;Mobic&#x27;], &#x27;condition&#x27;: [&#x27;Gout, Acute&#x27;, &#x27;ibromyalgia&#x27;, &#x27;Inflammatory Conditions&#x27;], &#x27;review&#x27;: [&#x27;&quot;like the previous person mention, I&amp;#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!&quot;&#x27;,  &#x27;&quot;I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.&quot;&#x27;,  &#x27;&quot;I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.&quot;&#x27;], &#x27;rating&#x27;: [9.0, 3.0, 10.0], &#x27;date&#x27;: [&#x27;September 2, 2015&#x27;, &#x27;November 7, 2011&#x27;, &#x27;June 5, 2013&#x27;], &#x27;usefulCount&#x27;: [36, 13, 128]&#125;&#x27;&#x27;&#x27;



é‡å‘½å123456789101112131415drug_dataset = drug_dataset.rename_column(    original_column_name=&quot;Unnamed: 0&quot;, new_column_name=&quot;patient_id&quot;)drug_dataset&#x27;&#x27;&#x27;DatasetDict(&#123;    train: Dataset(&#123;        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;],        num_rows: 161297    &#125;)    test: Dataset(&#123;        features: [&#x27;patient_id&#x27;, &#x27;drugName&#x27;, &#x27;condition&#x27;, &#x27;review&#x27;, &#x27;rating&#x27;, &#x27;date&#x27;, &#x27;usefulCount&#x27;],        num_rows: 53766    &#125;)&#125;)&#x27;&#x27;&#x27;



è¡¥å……ä¸€ä¸ªåŒ¿åè¡¨è¾¾å¼çš„ç»†èŠ‚

(lambda base, height: 0.5 * base * height)(4, 8)
16 

è½¬æ¢å¤§å°å†™123456def lowercase_condition(example):    return &#123;&quot;condition&quot;: example[&quot;condition&quot;].lower()&#125;drug_dataset.map(lowercase_condition)&#x27;&#x27;&#x27;AttributeError: &#x27;NoneType&#x27; object has no attribute &#x27;lower&#x27;&#x27;&#x27;&#x27;

è¿™é‡ŒæŠ¥é”™äº†
filterdataset.filter
12345678drug_dataset = drug_dataset.filter(lambda x: x[&quot;condition&quot;] is not None)drug_dataset = drug_dataset.map(lowercase_condition)# Check that lowercasing workeddrug_dataset[&quot;train&quot;][&quot;condition&quot;][:3]&#x27;&#x27;&#x27;[&#x27;left ventricular dysfunction&#x27;, &#x27;adhd&#x27;, &#x27;birth control&#x27;]&#x27;&#x27;&#x27;

è¿‡æ»¤ç­›é€‰åˆæ ¼çš„æ•°æ®æ ·æœ¬
å¢åŠ åˆ—123456789101112131415def compute_review_length(example):    return &#123;&quot;review_length&quot;: len(example[&quot;review&quot;].split())&#125;    drug_dataset = drug_dataset.map(compute_review_length)# Inspect the first training exampledrug_dataset[&quot;train&quot;][0]&#x27;&#x27;&#x27;&#123;&#x27;patient_id&#x27;: 206461, &#x27;drugName&#x27;: &#x27;Valsartan&#x27;, &#x27;condition&#x27;: &#x27;left ventricular dysfunction&#x27;, &#x27;review&#x27;: &#x27;&quot;It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil&quot;&#x27;, &#x27;rating&#x27;: 9.0, &#x27;date&#x27;: &#x27;May 20, 2012&#x27;, &#x27;usefulCount&#x27;: 27, &#x27;review_length&#x27;: 17&#125;&#x27;&#x27;&#x27;



è¡¥å……ä¸€ä¸ªsort

12345678910&gt;drug_dataset[&quot;train&quot;].sort(&quot;review_length&quot;)[:3]&gt;&#x27;&#x27;&#x27;&gt;&#123;&#x27;patient_id&#x27;: [103488, 23627, 20558],&#x27;drugName&#x27;: [&#x27;Loestrin 21 1 / 20&#x27;, &#x27;Chlorzoxazone&#x27;, &#x27;Nucynta&#x27;],&#x27;condition&#x27;: [&#x27;birth control&#x27;, &#x27;muscle spasm&#x27;, &#x27;pain&#x27;],&#x27;review&#x27;: [&#x27;&quot;Excellent.&quot;&#x27;, &#x27;&quot;useless&quot;&#x27;, &#x27;&quot;ok&quot;&#x27;],&#x27;rating&#x27;: [10.0, 1.0, 6.0],&#x27;date&#x27;: [&#x27;November 4, 2008&#x27;, &#x27;March 24, 2017&#x27;, &#x27;August 20, 2016&#x27;],&#x27;usefulCount&#x27;: [5, 2, 10],&#x27;review_length&#x27;: [1, 1, 1]&#125;&#x27;&#x27;&#x27;

sortåº”è¯¥ä¹Ÿæœ‰reverseé€‰é¡¹ï¼Œå¦‚æœçœŸè¦åšEDAè¿˜æ˜¯ç”¨Pandaså¥½äº†, æŸ¥çœ‹å¯é…ç½®å‚æ•°

åœ¨è¡¥å……ä¸€ä¸ªDataset.add_column()

An alternative way to add new columns to a dataset is with the Dataset.add_column() function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where Dataset.map() is not well suited for your analysis.

è§£æhtmlå­—ç¬¦12345678import htmltext = &quot;I&amp;#039;m a transformer called BERT&quot;html.unescape(text)&#x27;&#x27;&#x27;&quot;I&#x27;m a transformer called BERT&quot;&#x27;&#x27;&#x27;



1drug_dataset = drug_dataset.map(lambda x: &#123;&quot;review&quot;: html.unescape(x[&quot;review&quot;])&#125;)



map æ–¹æ³•batchWhen you specify batched=True the function receives a dictionary with the fields of the dataset, but each value is now a list of values, and not just a single value. The return value of Dataset.map() should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. 
123new_drug_dataset = drug_dataset.map(    lambda x: &#123;&quot;review&quot;: [html.unescape(o) for o in x[&quot;review&quot;]]&#125;, batched=True)

æ‰¹é‡å¤„ç†ä¸ºTrueçš„è¯ï¼Œæ¯æ¬¡ä¼ è¿›æ¥å°±æ˜¯ä¸€ä¸ªå­—å…¸æ‰¹æ¬¡ã€‚ä¸€èˆ¬æˆ‘ä»¬åšçš„å°±æ˜¯æ›´æ–°è¿™ä¸ªæ•°æ®é›†
If youâ€™re running this code in a notebook, youâ€™ll see that this command executes way faster than the previous one. And itâ€™s not because our reviews have already been HTML-unescaped â€” if you re-execute the instruction from the previous section (without batched=True), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a for loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.
ä¹‹å‰å•ä¸ªå¤„ç†çš„ç”¨çš„æ˜¯forå¾ªç¯ï¼Œè¿™é‡Œæ‰¹é‡å¤„ç†å°±å¯ä»¥ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼Œè¦å¿«çš„å¤š
é…åˆtokenizerä½¿ç”¨1234567891011def tokenize_and_split(examples):    return tokenizer(        examples[&quot;review&quot;],        truncation=True,        max_length=128,        return_overflowing_tokens=True,    )    result = tokenize_and_split(drug_dataset[&quot;train&quot;][0])[len(inp) for inp in result[&quot;input_ids&quot;]]# [128, 49]

ä½¿ç”¨return_overflowing_tokenså‚æ•°æ¥æ¥å—æˆªæ–­çš„éƒ¨åˆ†ï¼Œè¿™é‡Œæˆ‘ä»¬177çš„é•¿åº¦å˜æˆäº†128å’Œ49ä¸¤ä»½
1tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)



æ•°æ®ç±»å‹è½¬æ¢PandasTo enable the conversion between various third-party libraries, ğŸ¤— Datasets provides a Dataset.set_format() function. This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format, which is Apache Arrow. The formatting is done in place. To demonstrate, letâ€™s convert our dataset to Pandas:
drug_dataset.set_format(&quot;pandas&quot;)
ä¸€èˆ¬ä½¿ç”¨train_df = drug_dataset[&quot;train&quot;][:] è·å¾—æ•´ä½“çš„åˆ‡ç‰‡ä½œä¸ºæ–°çš„Dataframe å¯ä»¥è‡ªå·±å°è¯•æ˜¯å¦è¿”å›å¯¹è±¡ä¸ºhfçš„dataset
123456789from datasets import Datasetfreq_dataset = Dataset.from_pandas(frequencies)freq_dataset&#x27;&#x27;&#x27;Dataset(&#123;    features: [&#x27;condition&#x27;, &# ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/58033.html" title="NLP Baseline 01 ç¿»è¯‘"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka26.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP Baseline 01 ç¿»è¯‘"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/58033.html" title="NLP Baseline 01 ç¿»è¯‘">NLP Baseline 01 ç¿»è¯‘</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-04T16:10:48.719Z" title="å‘è¡¨äº 2022-12-05 00:10:48">2022-12-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-10T02:36:53.904Z" title="æ›´æ–°äº 2022-12-10 10:36:53">2022-12-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">
ä»å¤´è®­ç»ƒï¼Œä¸å¦‚fine-tuneï¼Œå¦‚æœä½ æ¯”Google &amp; Mate æœ‰é’±å½“æˆ‘æ²¡è¯´

å¾…å®Œæˆ
Accelarator
get_scheduler
custom_wandb

Translationè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨zh-ençš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œè¿›è¡Œç¿»è¯‘ä»»åŠ¡
è¿™é‡Œéœ€è¦æ³¨å†Œä¸€ä¸ªwandbçš„è´¦å·ï¼Œè®°å¾—å•Šã€‚
ç¤ºä¾‹æŸ¥çœ‹123456789101112131415161718192021from transformers import AutoModelForSeq2SeqLM, AutoTokenizerfrom datasets import load_datasetprx = &#123;&#x27;https&#x27;: &#x27;http://127.0.0.1:7890&#x27;&#125;model_name = &quot;Helsinki-NLP/opus-mt-zh-en&quot;save_path = r&#x27;D:\00mydataset\huggingface model&#x27;data_path = r&#x27;D:\00mydataset\huggingface dataset&#x27;tokenizer = AutoTokenizer.from_pretrained(model_name, proxies=prx, cache_dir=save_path)model = AutoModelForSeq2SeqLM.from_pretrained(model_name, proxies=prx, cache_dir=save_path)dataset = load_dataset(&#x27;news_commentary&#x27;,&#x27;en-fr&#x27;,cache_dir=data_path)dataset&#x27;&#x27;&#x27;DatasetDict(&#123;    train: Dataset(&#123;        features: [&#x27;id&#x27;, &#x27;translation&#x27;],        num_rows: 69206    &#125;)&#125;)&#x27;&#x27;&#x27;

è¿™ä¸ªæŒ‚ä¸ªä»£ç†åŠ é€Ÿä¸‹
1234567891011tokenizer&#x27;&#x27;&#x27;PreTrainedTokenizer(name_or_path=&#x27;Helsinki-NLP/opus-mt-zh-en&#x27;, vocab_size=65001, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;eos_token&#x27;: &#x27;&lt;/s&gt;&#x27;, &#x27;unk_token&#x27;: &#x27;&lt;unk&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;pad&gt;&#x27;&#125;)&#x27;&#x27;&#x27;&#x27;dataset[&#x27;train&#x27;][1][&#x27;translation&#x27;]&#x27;&#x27;&#x27;&#123;&#x27;id&#x27;: &#x27;1&#x27;, &#x27;translation&#x27;: &#123;&#x27;en&#x27;: &#x27;PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening. At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.&#x27;,  &#x27;zh&#x27;: &#x27;å·´é»-éšç€ç»æµå±æœºä¸æ–­åŠ æ·±å’Œè”“å»¶ï¼Œæ•´ä¸ªä¸–ç•Œä¸€ç›´åœ¨å¯»æ‰¾å†å²ä¸Šçš„ç±»ä¼¼äº‹ä»¶å¸Œæœ›æœ‰åŠ©äºæˆ‘ä»¬äº†è§£ç›®å‰æ­£åœ¨å‘ç”Ÿçš„æƒ…å†µã€‚ä¸€å¼€å§‹ï¼Œå¾ˆå¤šäººæŠŠè¿™æ¬¡å±æœºæ¯”ä½œ1982å¹´æˆ–1973å¹´æ‰€å‘ç”Ÿçš„æƒ…å†µï¼Œè¿™æ ·å¾—ç±»æ¯”æ˜¯ä»¤äººå®½å¿ƒçš„ï¼Œå› ä¸ºè¿™ä¸¤æ®µæ—¶æœŸæ„å‘³ç€å…¸å‹çš„å‘¨æœŸæ€§è¡°é€€ã€‚&#x27;&#125;&#125;  &#x27;&#x27;&#x27;  

æŸ¥çœ‹ä¸‹æ•°æ®, å¯ä»¥çœ‹åˆ°è¿”å›çš„æ˜¯å­—å…¸å½¢å¼ï¼Œæˆ‘ä»¬ä¸»è¦ç”¨åˆ°translationä¸‹çš„enã€zh
123456789101112s1 = &#x27;å¤©ä¸‹ç¬¬ä¸€ç¾å°‘å¥³, ç½¢äº†&#x27;inputs = tokenizer(s1, return_tensors=&#x27;pt&#x27;,)inputs&#x27;&#x27;&#x27;(&#123;&#x27;input_ids&#x27;: tensor([[ 9705,   359,  3615,  2797, 14889,     2,     7, 40798,     0]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])&#125;,)&#x27;&#x27;&#x27;outputs = model.generate(**inputs)tokenizer.batch_decode(outputs, skip_special_tokens=True)&#x27;&#x27;&#x27;[&quot;The most beautiful girl in the world, that&#x27;s all.&quot;]&#x27;&#x27;&#x27;

çœ‹ä¸‹è¾“å‡ºï¼Œè¿˜å¯ä»¥

æ³¨æ„ï¼ŒAutoModelForSeq2SeqLMä¸åŒäºAutoModelçš„å°±æ˜¯åŠ å…¥äº†model.generateè¿™ä¸ªç‰¹æ€§ã€‚
ä¸ç„¶model(**inputs)æ˜¯è¦ä½ è¡¥å……ç›®æ ‡è¯­è¨€çš„ã€‚


If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting tokenizer.src_lang and tokenizer.tgt_lang to the right values.

â€‹	å¦‚æœä½ ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹ï¼Œä½ å¾—æŒ‡å®šä½ çš„æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„å‚æ•°



Preprocessing1234567891011121314151617split_datasets = raw_datasets[&quot;train&quot;].train_test_split(train_size=0.9, seed=20)split_datasets&#x27;&#x27;&#x27;DatasetDict(&#123;    train: Dataset(&#123;        features: [&#x27;id&#x27;, &#x27;translation&#x27;],        num_rows: 189155    &#125;)    test: Dataset(&#123;        features: [&#x27;id&#x27;, &#x27;translation&#x27;],        num_rows: 21018    &#125;)&#125;)&#x27;&#x27;&#x27;split_datasets[&quot;validation&quot;] = split_datasets.pop(&quot;test&quot;)


HFçš„datasetå¯ä»¥ç›´æ¥è°ƒç”¨.train_test_split(train_size=0.9, seed=20)

HFçš„datasetå¯ä»¥ç›´æ¥è½¬DataFrameï¼Œè¿™æ ·ä½ ä¹Ÿå¯ä»¥ç›´æ¥é…åˆSklearnä½¿ç”¨


ç»™testé‡å‘½åä¸ºvalidation


DataCollatorForSeq2Seq12345678910111213141516171819202122max_length = 128def preprocess_function(examples):    inputs = [ex[&quot;en&quot;] for ex in examples[&quot;translation&quot;]]    targets = [ex[&quot;fr&quot;] for ex in examples[&quot;translation&quot;]]    model_inputs = tokenizer(        inputs, text_target=targets, max_length=max_length, truncation=True    )    return model_inputs    tokenized_datasets = split_datasets.map(    preprocess_function,    batched=True,    remove_columns=split_datasets[&quot;train&quot;].column_names,)&#x27;&#x27;&#x27;è¿™é‡Œæœ¬æ¥è¿˜æœ‰[&#x27;id&#x27;, &#x27;translation&#x27;],é€šè¿‡ä¸‹é¢çš„è®¾ç½®å°±åˆ é™¤äº†ã€‚remove_columns:    Remove a selection of columns while doing the mapping.    Columns will be removed before updating the examples with the output of `function`,i.e.     if `function` is adding columns with names in `remove_columns`, these columns will be kept.&#x27;&#x27;&#x27;


We donâ€™t pay attention to the attention mask of the targets, as the model wonâ€™t expect it. Instead, the labels corresponding to a padding token should be set to -100 so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to -100.
è¿™é‡Œæˆ‘ä»¬ä¸ä¼šåŠ å…¥paddingï¼Œmaskã€‚ä¹‹åæˆ‘ä»¬çš„maskä¼šè®¾æˆ-100 ä½¿å…¶ä¸ä¼šè®¡ç®—æŸå¤±ã€‚è¿™äº›éƒ½æ˜¯ä¸‹ä¸€æ­¥çš„æ“ä½œ

ps: ä»Šå¤©çœ‹åˆ°ä¸ªbugï¼Œåº”è¯¥æ˜¯æ²¡æœ‰æ›´æ–°åˆ°æœ€æ–°ç‰ˆç‰ˆï¼Œå…·ä½“æ¥è¯´å°±æ˜¯tokenizerä¹‹åæ²¡æœ‰labelï¼Œå¦‚æœbugäº†ï¼Œå¯ä»¥è¿›è¡Œä»¥ä¸‹æ›¿æ¢
12345678910111213141516def preprocess_function(examples):    inputs = [ex[&quot;zh&quot;] for ex in examples[&quot;translation&quot;]]    targets = [ex[&quot;en&quot;] for ex in examples[&quot;translation&quot;]]    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)    # Set up the tokenizer for targets    with tokenizer.as_target_tokenizer():        labels = tokenizer(targets, max_length=max_target_length, truncation=True)    model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]    return model_inputstokenized_datasets = split_datasets.map(    preprocess_function,    batched=True,    remove_columns=split_datasets[&quot;train&quot;].column_names,)



12345678910111213141516171819202122232425262728293031from transformers import DataCollatorForSeq2Seqdata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)batch = data_collator([tokenized_datasets[&quot;train&quot;][i] for i in range(1, 3)])batch.keys()# dict_keys([&#x27;attention_mask&#x27;, &#x27;input_ids&#x27;, &#x27;labels&#x27;, &#x27;decoder_input_ids&#x27;])batch[&quot;labels&quot;]&#x27;&#x27;&#x27; tensor([[57483,     7,  3241,   403,     3,   289,  1817, 25787,    22,     6,          38697,    22,     2,     3,   426,    64,    72, 27734,    14,  9054,          56467,  6667,     8,   721,   512,  2498,   209,    64,    72, 11468,              5,   393,     3,  2597,     4,     3,  1817,     2,   469,   235,            238, 24898,    39,     8, 13579,    50, 17528,     2,    60,    42,          56548,     2,   695,   443, 10119,  5543,     8, 53617,     7, 38261,          40490,    22,     5,     0],         [   24, 22026,    30,  2329, 10349, 22901,    20, 52813,    17,    50,             12, 29940,     4,     3,  2121,    20,  1843,    45,    67,   243,           1945,    30,   368, 36681,    10,     3,  1796,     4, 14961,  2203,              6, 28291,     3, 22986,     2, 11355,     3,  3368,    64,  8700,             18,   469, 38575,    10,   278,    54,     8,  4291,    57, 22301,           1718,     8,   959, 30229,  1294,  6855,  4298,     5,     0,  -100,           -100,  -100,  -100,  -100]])&#x27;&#x27;&#x27;# çœ‹ä¸‹åŸæ¥çš„tokenfor i in range(1, 3):    print(tokenized_datasets[&quot;train&quot;][i][&quot;labels&quot;])&#x27;&#x27;&#x27;[57483, 7, 3241, 403, 3, 289, 1817, 25787, 22, 6, 38697, 22, 2, 3, 426, 64, 72, 27734, 14, 9054, 56467, 6667, 8, 721, 512, 2498, 209, 64, 72, 11468, 5, 393, 3, 2597, 4, 3, 1817, 2, 469, 235, 238, 24898, 39, 8, 13579, 50, 17528, 2, 60, 42, 56548, 2, 695, 443, 10119, 5543, 8, 53617, 7, 38261, 40490, 22, 5, 0][24, 22026, 30, 2329, 10349, 22901, 20, 52813, 17, 50, 12, 29940, 4, 3, 2121, 20, 1843, 45, 67, 243, 1945, 30, 368, 36681, 10, 3, 1796, 4, 14961, 2203, 6, 28291, 3, 22986, 2, 11355, 3, 3368, 64, 8700, 18, 469, 38575, 10, 278, 54, 8, 4291, 57, 22301, 1718, 8, 959, 30229, 1294, 6855, 4298, 5, 0]&#x27;&#x27;&#x27;


å¯ä»¥çœ‹åˆ°paddingçš„ä½ç½®éƒ½å˜æˆ-100äº†ï¼Œpytorchä¸­ä¹Ÿæœ‰è¿™ä¸ªè®¾å®šå¯è§æˆ‘ä¹‹å‰è®²Transformerçš„å†…å®¹


 This is all done by a DataCollatorForSeq2Seq. Like the DataCollatorWithPadding, it takes the tokenizer used to preprocess the inputs, but it also takes the model. This is because this data collator will also be responsible for preparing the decoder input IDs, which are shifted versions of the labelsï¼ˆç§»åŠ¨ç‰ˆçš„æ ‡ç­¾ï¼‰ with a special token at the beginning. Since this shift is done slightly differently for different architectures, the DataCollatorForSeq2Seq needs to know the model objectæå¾—è¿˜æŒºå¤æ‚


Metrics
One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers. So instead, the most commonly used metric for benchmarking translation models today is SacreBLEU, which addresses this weakness (and others) by standardizing the tokenization step


è¿™é‡Œæˆ‘ä»¬åŠ å…¥SacreBLEUä½œä¸ºè¯„åˆ†æ ‡å‡†

12345!pip install sacrebleu	import evaluatemetric = evaluate.load(&quot;sacrebleu&quot;)



ç¤ºä¾‹1
12345678910111213141516171819predictions = [    &quot;This plugin lets you translate web pages between several languages automatically.&quot;]references = [    [        &quot;This plugin allows you to automatically translate web pages between several languages.&quot;    ]]metric.compute(predictions=predictions, references=references)&#x27;&#x27;&#x27;&#123;&#x27;score&#x27;: 46.750469682990165, &#x27;counts&#x27;: [11, 6, 4, 3], &#x27;totals&#x27;: [12, 11, 1 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/36291.html" title="Huggingface Tokenizerè¯¦è§£"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface Tokenizerè¯¦è§£"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/36291.html" title="Huggingface Tokenizerè¯¦è§£">Huggingface Tokenizerè¯¦è§£</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-04T03:19:20.285Z" title="å‘è¡¨äº 2022-12-04 11:19:20">2022-12-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-11T13:27:25.762Z" title="æ›´æ–°äº 2022-12-11 21:27:25">2022-12-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/Preprocessing/">Preprocessing</a></span></div><div class="content">Main Types of Tokenizers
Byte-Pair Encoding (BPE)

WordPiece

SentencePiece



spaCy and Moses are two popular rule-based tokenizers.


Subword
word-level å¤ªå¤§ï¼Œcharacter-levelåŒ…å«ä¸äº†å¾ˆå¤šè¯­ä¹‰
So to get the best of both worlds, transformers models use a hybrid between word-level and character-level tokenization called subword tokenization.


Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.
å°†ä½é¢‘è¯(ä¸“æœ‰åè¯é‚£äº›) æ‹†è§£ã€åˆ†è¯



ä¾‹å¦‚
123456from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)tokenizer.tokenize(&quot;I have a new GPU!&quot;)# [&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;, &quot;gp&quot;, &quot;##u&quot;, &quot;!&quot;]



æ›´å°çš„è¯æ±‡è¡¨æ„å‘³ç€å¤±å»æ›´å¤šçš„è¯ä¹‰ï¼Œåœ¨è¯ä¹‰å’Œè¿ç®—èµ„æºçš„å¹³è¡¡å¼•å‡ºäº†ä¸‹é¢çš„å‡ ä¸ªç®—æ³•ã€‚


Byte-Pair Encoding (BPE)GPT-2, Roberta,  XLM,  FlauBERT ,GPT 
BPEåœ¨Neural Machine Translation of Rare Words with Subword Unitsä¸­è¢«æå‡º

After pre-tokenization, a set of unique words has been created and the frequency of each word it occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.
é¦–å…ˆç»Ÿè®¡è¯é¢‘ â€”&gt; å†æ ¹æ®è¯é¢‘åˆ›å»ºè¯æ±‡è¡¨ , åŒæ—¶åŠ å…¥èåˆè§„åˆ™ â€”&gt; ä¾è¯é¢‘mergeå¾—åˆ°æ–°çš„è¯æ±‡è¡¨ â€”&gt; åœ¨æ–°çš„è¯æ±‡è¡¨åŸºç¡€ä¸Šåœ¨æ­¤merge â€”&gt; åšç§ä½¿å¾—è¯æ±‡è¡¨çš„å¤§å°åœ¨ desired vocabulary size. è¿™ä¸ªå¤§å°æ˜¯å¯è°ƒçš„è¶…å‚æ•°



ä¾‹å¦‚, æ•°å­—è¡¨ç¤ºé¢‘ç‡
(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)

é¦–å…ˆæˆ‘ä»¬å¾—åˆ° base vocabulary is [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;].

ugçš„ç»„åˆæœ‰20çš„é¢‘ç‡ç¬¬ä¸€ä¸ªè¢«åŠ è¿›å»

(&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)


æ¬¡é«˜un â€”&gt; hug

(&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)




å¦å¤– For instance, the word &quot;bug&quot; would be tokenized to [&quot;b&quot;, &quot;ug&quot;] but &quot;mug&quot; would be tokenized as [&quot;&lt;unk&gt;&quot;, &quot;ug&quot;] since the symbol &quot;m&quot; is not in the base vocabulary.


the vocabulary size, i.e. the base vocabulary size + the number of merges
For instance GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.
 GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.


UnigramUnigram introduced in Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018). 
itâ€™s used in conjunction with SentencePiece. è·ŸSentencePieceé…åˆä½¿ç”¨

In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary.

At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, i.e. those symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.

å–å¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œå°†æ‰€æœ‰å­—æ¯åµŒå…¥åŸºç¡€è¯æ±‡è¡¨ï¼Œå¼€å§‹mergeï¼Œè®¡ç®—ç§»é™¤æŸäº›è¯æ‰€é™ä½æˆ–æå‡çš„æŸå¤±ï¼Œé‡å¤ä¸æ–­ç§»é™¤ï¼Œç›´åˆ°ç¬¦åˆç†æƒ³çš„è¯æ±‡è¡¨å¤§å°ã€‚


Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of tokenizing new text after training



WordPieceBERT, DistilBERT, and Electra. æ»¡æ»¡çš„å«é‡‘é‡
WordPiece was outlined in Japanese and Korean Voice Search (Schuster et al., 2012)

WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.
WordPieceä½¿ç”¨é¢‘ç‡è®°å½•characteråšåŸºç¡€è¯æ±‡è¡¨ï¼Œç„¶åä½¿ç”¨æœ€å¤§ä¼¼ç„¶åšè¯„åˆ¤æ ‡å‡† mergeè¯æ±‡ã€‚æ˜¯BPEå’ŒUnigramçš„ç»“åˆä½¿ç”¨æ¡ˆä¾‹ã€‚




SentencePieceXLM,  ALBERT, XLNet, Marian, and T5.
SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018) treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.

All transformers models in the library that use SentencePiece use it in combination with unigram
å°±æ˜¯ä½¿ç”¨Unicodeç ç¼–ç æ‰€æœ‰å­—ç¬¦




Customizing Tokenizerå¦‚åŒä¸Šé¢çœ‹åˆ°çš„ä¸åŒæ¨¡å‹é€‰æ‹©äº†ä¸åŒç®—æ³•çš„åˆ†è¯æ–¹å¼ï¼Œæ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©ä¸åŒçš„åˆ†è¯å™¨ã€‚
1234from tokenizers import models, Tokenizer# å½“ç„¶è¿™é‡Œå¯ä»¥ç›´æ¥ ä½¿ç”¨æ¨¡å‹åå­—å¾—åˆ°ä¸€ä¸ªå¯¹åº”æ¨¡å‹çš„åˆ†è¯å™¨ï¼Œé‚£å°±æ˜¯é€šå¸¸ä½¿ç”¨çš„æ–¹æ³•tokenizer = Tokenizer(models.WordPiece()) # models.BPE() models.Unigram()

ä»¥ä¸Šå³å¯å¼•å‡ºä½ çš„åˆ†è¯å™¨è¿›è¡Œä¸‹é¢çš„å®¢åˆ¶åŒ–ã€‚

Normalization: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.
å¯¹ä½ çš„æ–‡æœ¬è¿›è¡Œä¿®ç¼®ï¼Œè½¬å°å†™å­—æ¯ï¼Œåˆ é™¤æŸäº›ç¬¦å·ç­‰


Pre-tokenization: In charge of splitting the initial input string. Thatâ€™s the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.
å®šåˆ¶ä¸€äº›åˆ†è¯è§„åˆ™


Model: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.
å¾ªç¯å¤„ç†


Post-Processing: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.



é¦–å…ˆè®¾å®šä¸€ä¸‹æ•°æ®é›†å’Œæ•°æ®å‘ç”Ÿå™¨

12345678from datasets import load_datasetdataset = load_dataset(&quot;wikitext&quot;, name=&quot;wikitext-2-raw-v1&quot;, split=&quot;train&quot;)batch_size = 1000def batch_iterator():    for i in range(0, len(dataset), batch_size): # 0åˆ°æ•°æ®é›†çš„é•¿åº¦, æ­¥é•¿ä¸ºbatch_size        yield dataset[i : i + batch_size][&quot;text&quot;]




WordPiece

pre-processing
1234567891011121314151617from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizertokenizer = Tokenizer(models.WordPiece(unl_token=&quot;[UNK]&quot;))tokenizer.normalizer = normalizers.Sequence(    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])# è¿™é‡Œè®¾å®šbertçš„å‰å¤„ç†åˆ†è¯å™¨tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;This is an example!&quot;)&#x27;&#x27;&#x27;[(&#x27;This&#x27;, (0, 4)), (&#x27;is&#x27;, (5, 7)), (&#x27;an&#x27;, (8, 10)), (&#x27;example&#x27;, (11, 18)), (&#x27;!&#x27;, (18, 19))]&#x27;&#x27;&#x27;

Note that the pre-tokenizer not only split the text into words but keeps the offsets
that is the beginning and start of each of those words inside the original text.  è¿™é‡Œæ˜¯ä¸ºQAåšçš„åç§»é‡ç‰¹æ€§



processing
123456special_tokens = [&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]# å¼•å…¥ trainerä¸æ˜¯ç¼–è¾‘å¥½äº†ï¼Œè€Œæ˜¯å€ŸåŠ©å·²ç»å®Œæˆçš„trainerç»“æ„åŠ å…¥special_tokenstrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)# è¿™é‡Œæ˜¯å¼€å§‹è®­ç»ƒäº†tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)


è¿™é‡Œæˆ‘ä»¬å°±å¾—åˆ°äº†åŸºæœ¬å¤„ç†çš„æ•°æ®ï¼Œä¸‹é¢è¿›è¡Œåå¤„ç†ã€‚

post-processing
12345678910111213141516171819202122232425262728293031323334tokenizer.post_processor = processors.TemplateProcessing(    single=f&quot;[CLS]:0 $A:0 [SEP]:0&quot;,    pair=f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;,    special_tokens=[        (&quot;[CLS]&quot;, cls_token_id),        (&quot;[SEP]&quot;, sep_token_id),    ],)#æŸ¥çœ‹æ•°æ®encoder = tokenizer.encode(&quot;This is one sentence.&quot;, &quot;With this one we have a pair.&quot;)encoding.tokens&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;one&#x27;, &#x27;sentence&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;, &#x27;with&#x27;, &#x27;this&#x27;, &#x27;one&#x27;, &#x27;we&#x27;, &#x27;have&#x27;, &#x27;a&#x27;, &#x27;pair&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;] &#x27;&#x27;&#x27;  encoding.type_ids # [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]


We have to indicate in the template how to organize the special tokens with one sentence ($A) or two sentences ($A and $B). The : followed by a number indicates the token type ID to give to each part.
processé˜¶æ®µå¤„ç†å¥½çš„å¥å­è¿›è¡ŒåŒ…è£… å•ä¸ªå¥å­singleå°±æ˜¯00ï¼Œpairå°±æ˜¯01



wrap your tokenizer to transformer object
1234from transformers import PreTrainedTokenizerFastnew_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)# åŸç¤ºä¾‹æ˜¯ new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)





BPE

12345678910111213141516171819tokenizer = Tokenizer(models.BPE())tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)tokenizer.pre_tokenizer.pre_tokenize_str(&quot;This is an example!&quot;)trainer = trainers.BpeTrainer(vocab ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/13310.html" title="Transformer From Scratch"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer From Scratch"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/13310.html" title="Transformer From Scratch">Transformer From Scratch</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-01T15:53:44.718Z" title="å‘è¡¨äº 2022-12-01 23:53:44">2022-12-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-11T07:17:35.820Z" title="æ›´æ–°äº 2022-12-11 15:17:35">2022-12-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">å¾…å®Œæˆ
å¢åŠ decoder inferenceæ¨¡å—
å‰è¨€

æ¶æ„åˆ†å››ä¸ªå¤§å—

encoder 
encoder-layer


decoder
decoder-layer



ç»†èŠ‚ä¸‰ç§mask

encoder-mask
decoder-mask
cross-mask

Embeddingå¥å­è¡¨ç¤ºä¸º [token1ï¼Œ token2, â€¦tokens]
å¥å­1 &#x3D; [token_1ï¼Œ token_2, â€¦token_x]
å¥å­2 &#x3D; [token_1ï¼Œ token_2, â€¦token_y]   x ä¸ä¸€å®šç­‰äº y
token æ„é€ 123456789101112131415161718192021222324252627282930313233343536373839404142import torch.nn.functional as Fimport torchsrc_vocab_size = 16tgt_vocab_size = 16batch_size = 4max_len = 6src_len = torch.randint(2,7, (batch_size,))tgt_len = torch.randint(2,7, (batch_size,))&#x27;&#x27;&#x27;ç»“æœå¦‚ä¸‹tensor([8, 6, 5, 9]) æ­¤batchçš„ç¬¬ä¸€ä¸ªå¥å­é•¿8ï¼Œç¬¬äºŒä¸ª6tensor([6, 5, 9, 5])&#x27;&#x27;&#x27;# æ¥ä¸‹æ¥æˆ‘ä»¬æŠŠä¸å®šé•¿çš„å¥å­padding&#x27;&#x27;&#x27;randint ç”Ÿæˆ(i,)å½¢çŠ¶çš„æ•°æ®padding 0æ¬¡æˆ–è€… max_len - len(i) çš„æ¬¡æ•°unsqueeze å¢åŠ ä¸€ä¸ªç»´åº¦&#x27;&#x27;&#x27;src_seq = [F.pad(torch.randint(1, src_vocab_size, (i,)), (0, max_len-i)).unsqueeze(0) for i in src_len]tgt_seq = [F.pad(torch.randint(1, tgt_vocab_size, (i,)), (0, max_len-i)).unsqueeze(0) for i in tgt_len]# å°†æ•´ä¸ªbatchçš„å¥å­æ•´åˆsrc_seq = torch.cat(src_seq)tgt_seq = torch.cat(tgt_seq)&#x27;&#x27;&#x27;å¦‚ä¸‹tensor([[12, 15, 10,  5,  3, 14],        [ 5,  7,  9,  3, 12,  1],        [ 3,  1,  1,  9,  3,  4],        [ 9,  6,  0,  0,  0,  0]])        tensor([[11, 12, 11,  3,  5, 15],        [ 7,  9, 11,  0,  0,  0],        [12,  6, 13, 11,  0,  0],        [13,  3,  0,  0,  0,  0]])&#x27;&#x27;&#x27;



è¯å‘é‡ç©ºé—´ æ˜ å°„123456789101112131415161718192021222324252627282930import torch.nn as nnd_model = 8src_embedding = nn.Embedding(src_vocab_size+1, d_model)tgt_embedding = nn.Embedding(tgt_vocab_size+1, d_model)src_embedding.weight # shapeä¸º(17, 8)&#x27;&#x27;&#x27;ç¬¬é›¶è¡Œä¸ºpadçš„æ•°æ®Parameter containing:tensor([[ 2.4606,  1.7139, -0.2859, -0.5058,  0.6229, -0.0470,  2.1517,  0.2996],        [ 0.0077, -0.4292, -0.2397,  1.2366, -0.3061,  0.9196, -1.4222, -1.6431],        [-0.6378, -0.7809, -0.4206,  0.5759, -1.4899,  1.2241,  0.9220, -0.6333],        [ 0.0303, -1.4113,  0.9164, -0.1200,  1.7224, -0.4996, -1.6708, -1.8563],        [ 0.0235,  0.0155, -0.1292, -0.9274, -1.1351, -0.9155,  0.4391, -0.0437],        [ 0.8498,  0.4709, -0.9168, -2.1307,  0.1840,  0.3554, -0.3986,  1.2806],        [ 0.7256,  1.2303, -0.8280, -0.2173,  0.8939,  2.4122,  0.4820, -1.9615],        [-0.8607,  2.4886, -0.8877, -0.8852,  0.3905,  0.9511, -0.3732,  0.4872],        [ 0.4882, -0.4518, -0.1945,  0.2857, -0.6832, -0.4870, -1.7165, -2.0987],        [-0.0512,  0.2692, -1.0003,  0.7896,  0.5004,  0.3594, -1.5923, -1.5618],        [ 0.4012,  0.1614,  1.8939,  0.3862, -0.6733, -1.2442, -0.6540, -1.6772],        [ 1.4784,  2.7430,  0.0159,  0.5944, -1.0025,  1.0843,  0.4580, -0.6515],        [ 0.3905,  0.6118, -0.1256, -0.6725,  1.2366,  0.8272,  0.0838, -1.5124],        [-0.1470,  0.2149, -1.4561,  1.8008,  0.7764, -0.8517, -0.3204, -0.2550],        [-1.1534, -0.6837, -1.7165, -1.7905, -1.5423,  1.8812, -0.1794, -0.2357],        [ 1.3046,  1.5021,  1.4846,  1.0622,  1.4066,  0.7299,  0.7929, -1.0107],        [-0.3920,  0.7482,  1.5976,  1.7429, -0.4683,  0.2286,  0.1320, -0.5826]],       requires_grad=True)&#x27;&#x27;&#x27;scr_embedding(scr_seq[0]) # å°†å–å‡ºå¯¹åº”çš„[12, 15, 10,  5,  3, 14]è¡Œ



Key_maskç”±äºåœ¨encoder_layer padçš„ä½ç½®ç»è¿‡softmax ä¹Ÿä¼šåˆ†å¾—æˆ–å¤šæˆ–å°‘æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿™äº›padä¸æ˜¯æˆ‘ä»¬å¸Œæœ›æ¨¡å‹ä»æ•°æ®ä¸­å­¦åˆ°çš„ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å¼•å…¥Key_mask å¸®åŠ©encoderæ›´å¥½çš„å…³æ³¨åœ¨éœ€è¦å…³æ³¨çš„ä½ç½®ä¸Šã€‚ ä¹Ÿæ˜¯ç‰¹åˆ«é‡è¦çš„ä¸€ä¸ªç»†èŠ‚ã€‚

å‰ç½®

12345678910111213max_len = 6embed_dim = 8vacab_max = 5token_ids = [torch.randint(1,6, (len,)) for len in torch.randint(1,7, (max_len,))]token_ids&#x27;&#x27;&#x27;[tensor([2, 3, 5, 2, 4]), tensor([3, 3, 4, 4, 5, 4]), tensor([4, 5, 3]), tensor([5, 1, 4]), tensor([2, 1, 5, 3]), tensor([1, 3, 3, 1])]&#x27;&#x27;&#x27;




pad

1234567891011token_pad_ids = [F.pad(x, (0, max_len-x.shape[0])).unsqueeze(0) for x in token_ids]token_pad_ids = torch.cat(token_pad_ids)token_pad_ids&#x27;&#x27;&#x27;tensor([[2, 3, 5, 2, 4, 0],        [3, 3, 4, 4, 5, 4],        [4, 5, 3, 0, 0, 0],        [5, 1, 4, 0, 0, 0],        [2, 1, 5, 3, 0, 0],        [1, 3, 3, 1, 0, 0]])&#x27;&#x27;&#x27;




å–å¾—embedding

123456789101112131415161718192021src_embedding = nn.Embedding(vacab_max+1, embed_dim)tgt_embedding = nn.Embedding(vacab_max+1, embed_dim)src_embedding.weight, tgt_embedding.weight&#x27;&#x27;&#x27;(Parameter containing: tensor([[-1.4019, -0.3245,  0.8569, -1.6555,  1.3478,  0.0979, -1.7458,  1.3138],         [-0.9099, -0.6957,  0.4430,  0.6305,  0.1099,  0.3213,  0.0841,  0.0786],         [-0.1215, -1.4141,  0.8802, -0.3444,  0.3444, -1.4063, -0.5057,  0.1506],         [ 0.9491,  1.7888,  0.3075, -0.6642,  0.3368,  0.3388, -1.2543, -0.8096],         [ 0.7723, -1.2258, -0.4963,  1.4007, -0.8048, -0.1338,  0.0199,  0.4295],         [ 1.3789, -0.9537,  0.3421,  0.0658, -0.7578, -0.7217, -1.3124,  1.6017]],        requires_grad=True), Parameter containing: tensor([[ 2.0609,  0.7302,  0.9811,  0.7390,  0.7475,  0.2903,  0.0735,  0.3407],         [ 1.5477, -0.5033,  1.3758, -1.5225,  0.8236,  0.6329, -0.2301,  1.2352],         [-0.2906, -1.8842, -0.9998,  1.6752,  0.7286, -0.4089, -0.0515,  0.5763],         [ 0.2128,  0.7354, -0.4248,  0.7142,  0.4635,  1.1675,  0.7193,  1.3474],         [ 0.3543,  1.2881, -0.8270,  0.6220, -1.6282,  0.1802, -0.9306, -0.2407],         [-1.3339, -0.4192, -0.0800,  0.1614,  0.7026, -0.6851,  0.2386, -0.4954]],        requires_grad=True))&#x27;&#x27;&#x27;




æŸ¥çœ‹True False ï¼Œè¿™é‡Œæˆ‘ä»¬å»src_ids çš„ç¬¬å››ä¸ªï¼Œå› ä¸ºé›¶æ¯”è¾ƒå¤šï¼Œ(embeddingçš„é›¶è¡Œæ˜¯padçš„è¯å‘é‡)

1234567891011pad = src_embedding.weight[0]src_embedding(token_pad_ids[3]) == pad, token_pad_ids[3]&#x27;&#x27;&#x27;(tensor([[False, False, False, False, False, False, False, False],         [False, False, False, False, False, False, False, False],         [False, False, False, False, False, False, False, False],         [ True,  True,  True,  True,  True,  True,  True,  True],         [ True,  True,  True,  True,  True,  True,  True,  True],         [ True,  True,  True,  True,  True,  True,  True,  True]]), tensor([5, 1, 4, 0, 0, 0])) &#x27;&#x27;&#x27;




ç”±äºencoderçš„è¾“å…¥éƒ½æ˜¯src æ‰€ä»¥Q*K.T çš„ç»´åº¦ä¸ºï¼ˆbs, src_len, src_len),  maskå°±ç›´æ¥å¯ä»¥å†™äº†

12345678910111213a = token_pad_ids[3].unsqueeze(-1)b = token_pad_ids[3].unsqueeze(0)torch.matmul(a,b), a.shape, b.shape&#x27;&#x27;&#x27;(tensor([[25,  5, 20,  0,  0,  0],         [ 5,  1,  4,  0,  0,  0],         [20,  4, 16,  0,  0,  0],         [ 0,  0,  0,  0,  0,  0],         [ 0,  0,  0,  0,  0,  0],         [ 0,  0,  0,  0,  0,  0]]), torch.Size([6, 1]), torch.Size([1, 6])) &#x27;&#x27;&#x27;




ä¸€æ‰¹é‡æŸ¥çœ‹mask

12345678910111213141516171819202122232425mask = torch.matmul(token_pad_ids.unsqueeze(-1),token_pad_ids.unsqueeze(1)) ==0mask&#x27;&#x27;&#x27;åªçœ‹å‰ä¸‰ä¸ªtensor([[[False, False, False, False, False,  True],         [False, False, False, False, False,  True],         [False, False, False, False, False,  True],         [False, False, False, False, False,  True],         [False, False, False, False, False,  True],         [ True,  True,  True,  True,  True,  True]],        [[False, False, False, False, False, False],         [False, False, False, False, False, False],         [False, False, False, False, False, False],         [False, False, False, False, False, False],         [False, False, False, False, False, False],         [False, False, False, False, False, False]],        [[False, False, False,  True,  True,  True],         [False, False, False,  True,  True,  True],         [False, False, False,  True,  True,  True],         [ True,  True,  True,  True,  True,  True],         [ True,  True,  True,  True,  True,  True],         [ True,  True,  True,  True,  True,  True]], ã€‚ã€‚ã€‚&#x27;&#x27;&#x27;




ä¸Šåˆºåˆ€

123456789101112131415161718192021222324252627scores = torch.randn(6, 6, 6)mask = torch.matmul(token_pad_ids.unsqueeze(-1),token_pad_ids.unsqueeze(1))scores= scores.masked_fill(mask==0, -1e9)scores.softmax(-1)&#x27;&#x27;&#x27;ä¾æ—§åªçœ‹å‰ä¸‰ä¸ªtensor([[[0.0356, 0.0985, 0.6987, 0.0902, 0.0770, 0.0000],         [0.4661, 0.0397, 0.3546, 0.0931, 0.0464, 0.0000],         [0.1917, 0.0149, 0.1564, 0.4113, 0.2259, 0.0000],         [0.4269, 0.0352, 0.1605, 0.1334, 0.2441, 0.0000],         [0.0515, 0.4421, 0.0705, 0.2934, 0.1426, 0.0000],         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],        [[0.0803, 0.0330, 0.3310, 0.0243, 0.3612, 0.1701],         [0.2160, 0.1483, 0.0312, 0.1804, 0.3861, 0.0380],         [0.2151, 0.0807, 0.1072, 0.4335, 0.1200, 0.0435],         [0.0285, 0.2684, 0.1558, 0.2210, 0.1880, 0.1383],         [0.0889, 0.4485, 0.1067, 0.1028, 0.1901, 0.0630],         [0.2885, 0.1682, 0.0935, 0.0179, 0.0289, 0.4031]],        [[0.2862, 0.3934, 0.3204, 0.0000, 0.0000, 0.0000],         [0.2426, 0.2206, 0.5369, 0.0000, 0.0000, 0.0000],         [0.1487, 0.2483, 0.6030, 0.0000, 0.0000, 0.0000],         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],ã€‚ã€‚ã€‚&#x27;&#x27;&#x27;

åŒæ—¶æˆ‘ä»¬å¯ä»¥çœ‹è§ å…¨éƒ½æ˜¯padè‡ªèº«çš„æ³¨æ„åŠ›åˆ†æ•°æ˜¯å¹³å‡çš„ï¼Œä¹Ÿå°±æ˜¯è·Ÿä¹±çŒœä¸€æ ·ï¼Œæ²¡æœ‰æ„ä¹‰ã€‚
Position EmbeddingæŒ‰å…¬å¼å†™å°±è¡Œ
123456789101112131415pe = torch.zeros(max_len, d_model)pos = torch.arange(0, max_len).unsqueeze(1) # å½¢çŠ¶ä¸º(max_len, 1)idx = torch.pow(10000, torch.arange(0, 8, 2).unsqueeze(0)/ d_model )  # å½¢çŠ¶ä¸º (1, 4)pe[:, 0::2] = torch.sin(pos / idx)	# è§¦å‘å¹¿æ’­ (max_len, 4)pe[:, 1::2] = torch.sin(pos / idx)&#x27;&#x27;&#x27;æ­¤å¤„æ¼”ç¤ºåªåŠ å¥‡æ•°åˆ—çš„æ•ˆæœtensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],        [ 0.8415,  0.0000,  0.0998,  0.0000,  0.0100,  0.0000,  0.0010,  0.0000],        [ 0.9093,  0.0000,  0.1987,  0.0000,  0.0200,  0.0000,  0.0020,  0.0000],        [ 0.1411,  0.0000,  0.2955,  0.0000,  0.0300,  0.0000,  0.003 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/28702.html" title="CV 02 Vit å¶å­å›¾ç‰‡åˆ†ç±»"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka3.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CV 02 Vit å¶å­å›¾ç‰‡åˆ†ç±»"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/28702.html" title="CV 02 Vit å¶å­å›¾ç‰‡åˆ†ç±»">CV 02 Vit å¶å­å›¾ç‰‡åˆ†ç±»</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-11-25T01:33:08.146Z" title="å‘è¡¨äº 2022-11-25 09:33:08">2022-11-25</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-13T14:08:54.657Z" title="æ›´æ–°äº 2022-12-13 22:08:54">2022-12-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/CV/">CV</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/CV/">CV</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/Trick/">Trick</a></span></div><div class="content">å‰è¨€Vitâ€”â€”Vision Transformer
è¿™é‡Œé€šè¿‡kaggleçš„å¶å­åˆ†ç±»ä»»åŠ¡æ¥ä½¿ç”¨é¢„è®­ç»ƒ(Pre-train)æ¨¡å‹Vitæ¥æå‡æˆ‘ä»¬çš„ä»»åŠ¡è¡¨ç¤º
1.è§‚å¯Ÿæ¨¡å‹&amp;å¤„ç†æ•°æ®1.1 æ¨¡å‹æ¢ç´¢æ— è®ºæ˜¯åŸºäºpythonçš„ç‰¹æ€§(é€‚é…å„ä¸ªé¢†åŸŸçš„åŒ…)ï¼Œè¿˜æ˜¯NLPé‡Œå¤§è¡Œå…¶é“çš„Pre-trainèŒƒå¼ï¼Œæ‹¥æœ‰å¿«é€Ÿäº†è§£ä¸€ä¸ªåŒ…çš„ç‰¹æ€§ä»¥é€‚ç”¨äºæˆ‘ä»¬å·¥ä½œçš„èƒ½åŠ›ï¼Œå°†æå¤§çš„æå‡æˆ‘ä»¬å·¥ä½œçš„æ•ˆç‡å’Œç»“æœã€‚æ‰€ä»¥ä¸‹é¢æˆ‘ä»¬æ¥å¿«é€Ÿä½“éªŒä¸€ä¸‹HuggingFaceç»™å‡ºçš„æ¨¡å‹èŒƒä¾‹ï¼Œå¹¶é’ˆå¯¹æˆ‘ä»¬çš„ä»»åŠ¡è¿›è¡Œç›¸åº”çš„æ•°æ®å¤„ç†ã€‚
1234567891011121314151617from transformers import ViTFeatureExtractor, ViTForImageClassificationfrom PIL import Imageimport requestsurl = &#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;image = Image.open(requests.get(url, stream=True).raw)feature_extractor = ViTFeatureExtractor.from_pretrained(&#x27;google/vit-base-patch16-224&#x27;)model = ViTForImageClassification.from_pretrained(&#x27;google/vit-base-patch16-224&#x27;)inputs = feature_extractor(images=image, return_tensors=&quot;pt&quot;)outputs = model(**inputs)logits = outputs.logits# model predicts one of the 1000 ImageNet classespredicted_class_idx = logits.argmax(-1).item()print(&quot;Predicted class:&quot;, model.config.id2label[predicted_class_idx])

ä¸Šé¢çš„ä»£ç å¯ä»¥è‡ªè¡Œè¿è¡Œ
1.1.1 ç¤ºä¾‹è§£è¯»
ä¸Šåè¡Œä»£ç : é¦–å…ˆé€šè¿‡requestsåº“æ‹¿åˆ°ä¸€å¼ å›¾ç‰‡å¹¶ç”¨imageç”Ÿæˆå›¾ç‰‡å½¢å¼ï¼Œä¸‹é¢ä¸¤è¡ŒåŠ è½½äº†Vit16çš„ç‰¹å¾æå–å™¨å’ŒHFç‰¹ä¾›çš„å›¾ç‰‡åˆ†ç±»é€‚é…æ¨¡å‹

ä¸‹é¢æˆ‘ä»¬çœ‹çœ‹ ç‰¹å¾æå–åçš„è¾“å…¥(inputs)
1234567891011121314151617181920212223242526272829# inputs è¾“å‡ºå¦‚ä¸‹&#x27;&#x27;&#x27;&#123;&#x27;pixel_values&#x27;: tensor([[[[ 0.1137,  0.1686,  0.1843,  ..., -0.1922, -0.1843, -0.1843],          [ 0.1373,  0.1686,  0.1843,  ..., -0.1922, -0.1922, -0.2078],          [ 0.1137,  0.1529,  0.1608,  ..., -0.2314, -0.2235, -0.2157],          ...,          [ 0.8353,  0.7882,  0.7333,  ...,  0.7020,  0.6471,  0.6157],          [ 0.8275,  0.7961,  0.7725,  ...,  0.5843,  0.4667,  0.3961],          [ 0.8196,  0.7569,  0.7569,  ...,  0.0745, -0.0510, -0.1922]],         [[-0.8039, -0.8118, -0.8118,  ..., -0.8902, -0.8902, -0.8980],          [-0.7882, -0.7882, -0.7882,  ..., -0.8745, -0.8745, -0.8824],          [-0.8118, -0.8039, -0.7882,  ..., -0.8902, -0.8902, -0.8902],          ...,          [-0.2706, -0.3176, -0.3647,  ..., -0.4275, -0.4588, -0.4824],          [-0.2706, -0.2941, -0.3412,  ..., -0.4824, -0.5451, -0.5765],          [-0.2784, -0.3412, -0.3490,  ..., -0.7333, -0.7804, -0.8353]],         [[-0.5451, -0.4667, -0.4824,  ..., -0.7412, -0.6941, -0.7176],          [-0.5529, -0.5137, -0.4902,  ..., -0.7412, -0.7098, -0.7412],          [-0.5216, -0.4824, -0.4667,  ..., -0.7490, -0.7490, -0.7647],          ...,          [ 0.5686,  0.5529,  0.4510,  ...,  0.4431,  0.3882,  0.3255],          [ 0.5451,  0.4902,  0.5137,  ...,  0.3020,  0.2078,  0.1294],          [ 0.5686,  0.5608,  0.5137,  ..., -0.2000, -0.4275, -0.5294]]]])&#125;          &#x27;&#x27;&#x27;inputs[&#x27;pixel_values&#x27;].size()# torch.Size([1, 3, 224, 224])

å¯ä»¥çœ‹åˆ°æ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹çš„tensoræ•°æ®ï¼Œå…¶ç»´åº¦ä¸º(b, C, W, H)
å› æ­¤æˆ‘ä»¬å–‚ç»™æ¨¡å‹çš„æ•°æ®ä¹Ÿå¾—æ˜¯å››ç»´çš„ç»“æ„

æ¥ä¸‹æ¥çœ‹çœ‹æ¨¡å‹åå‡ºæ¥çš„ç»“æœ
1234567891011121314151617181920212223242526# outputs è¾“å…¥å¦‚ä¸‹&#x27;&#x27;&#x27;MaskedLMOutput(loss=tensor(0.4776, grad_fn=&lt;DivBackward0&gt;), logits=tensor([[[[-0.0630, -0.0475, -0.1557,  ...,  0.0950,  0.0216, -0.0084],          [-0.1219, -0.0329, -0.0849,  ..., -0.0152, -0.0143, -0.0663],          [-0.1063, -0.0925, -0.0350,  ...,  0.0238, -0.0206, -0.2159],          ...,          [ 0.2204,  0.0593, -0.2771,  ...,  0.0819,  0.0535, -0.1783],          [-0.0302, -0.1537, -0.1370,  ..., -0.1245, -0.1181, -0.0070],          [ 0.0875,  0.0626, -0.0693,  ...,  0.1331,  0.1088, -0.0835]],         [[ 0.1977, -0.2163,  0.0469,  ...,  0.0802, -0.0414,  0.0552],          [ 0.1125, -0.0369,  0.0175,  ...,  0.0598, -0.0843,  0.0774],          [ 0.1559, -0.0994, -0.0055,  ..., -0.0215,  0.2452, -0.0603],          ...,          [ 0.0603,  0.1887,  0.2060,  ...,  0.0415, -0.0383,  0.0990],          [ 0.2106,  0.0992, -0.1562,  ..., -0.1254, -0.0603,  0.0685],          [ 0.0256,  0.1578,  0.0304,  ..., -0.0894,  0.0659,  0.1493]],         [[-0.0348, -0.0362, -0.1617,  ...,  0.0527,  0.1927,  0.1431],          [-0.0447,  0.0137, -0.0798,  ...,  0.1057, -0.0299, -0.0742],          [-0.0725,  0.1473, -0.0118,  ..., -0.1284,  0.0010, -0.0773],          ...,          [-0.0315,  0.1065, -0.1130,  ...,  0.0091, -0.0650,  0.0688],          [ 0.0314,  0.1034, -0.0964,  ...,  0.0144,  0.0532, -0.0415],          [-0.0205,  0.0046, -0.0987,  ...,  0.1317, -0.0065, -0.1617]]]],       grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None) &#x27;&#x27;&#x27;

å¯ä»¥çœ‹åˆ°æœ‰lossã€logitsã€hidden_statesã€attentionsï¼Œè€Œæˆ‘ä»¬çš„èŒƒä¾‹åªå–äº†logitsä½œä¸ºç»“æœè¾“å‡ºã€‚è¿™é‡Œå¹¶ä¸æ˜¯è¯´å…¶ä»–çš„éƒ¨åˆ†æ²¡ç”¨ï¼Œæ˜¯åªå–é€‚é…ä¸‹æ¸¸ä»»åŠ¡çš„è¾“å‡ºå³å¯ã€‚è¯¦æƒ…å¯ç ”ç©¶Vitçš„è®ºæ–‡

æœ€åé€šè¿‡argmaxå‡½æ•°å’Œmodel.config.id2labelå¾—å‡ºæ ‡ç­¾ç›¸å¯¹åº”çš„æ–‡å­—
argmaxå°±æ˜¯è¿”å›æœ€å¤§å€¼çš„ä½ç½®ä¸‹æ ‡ã€model.config.id2labelé…ç½®äº†å¯¹åº”æ ‡ç­¾çš„åç§°ï¼Œä¹ŸçŸ¥é“äº†æœ€åçš„classifierå±‚æ˜¯1000ç»´çš„


1.1.2 å°ç»“é€šè¿‡ä»¥ä¸Šæ¢ç´¢ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼š

è¾“å…¥çš„ç»´åº¦ä¸º(batch_size, 3, 224, 224)
æœ€åçš„classifieréœ€ç”±1000æ”¹æˆæˆ‘ä»¬å¶å­çš„ç±»åˆ«æ•°

1.2 æ•°æ®å¤„ç†æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¢ç´¢æ•°æ®çš„ç‰¹æ€§ï¼Œå¹¶ä¿®æ”¹ä»¥é€‚åº”æˆ‘ä»¬çš„æ¨¡å‹
1.2.1 EDAå³Exploratory Data Analysis
é¦–å…ˆå¯¼å…¥æ‰€éœ€åŒ…
12345678910111213141516171819202122232425# å¯¼å…¥å„ç§åŒ…import torchimport torch.nn as nnfrom torch.nn import functional as Fimport randomimport copyfrom fastprogress.fastprogress import master_bar, progress_barfrom torch.cuda.amp import autocastimport pandas as pdimport numpy as npfrom torch.utils.data import Dataset, DataLoader, TensorDatasetfrom torchvision import transformsfrom sklearn.model_selection import KFoldfrom PIL import Imageimport osimport matplotlib.pyplot as pltimport torchvision.models as modelsfrom torch.optim.lr_scheduler import CosineAnnealingLRfrom transformers import (AdamW, get_scheduler)from transformers import ViTFeatureExtractor, ViTForImageClassification

æŸ¥çœ‹åˆå§‹æ•°æ®
train_df = pd.read_csv(&#39;/kaggle/input/classify-leaves/train.csv&#39;)


ä½¿ç”¨ä¸‹é¢ä»£ç ç»™åˆ†ç±»é…ä¸Šåºå·
123456789101112def num_map(file_path):    data_df = pd.read_csv(&#x27;/kaggle/input/classify-leaves/train.csv&#x27;)        categories = data_df.label.unique().tolist()    categories_zip = list(zip( range(len(categories)) , categories))    categories_dict = &#123;v:k for k, v in categories_zip&#125;        data_df[&#x27;num_label&#x27;] = data_df.label.map(categories_dict)        return data_dfshow_df = num_map(&#x27;/kaggle/input/classify-leaves/train.csv&#x27;)show_df.to_csv(&#x27;train_valid_dataset.csv&#x27;)





1.2.2 å›¾ç‰‡æ•°æ®æŸ¥çœ‹12345678path = &#x27;/kaggle/input/classify-leaves/&#x27;img = Image.open(path+train_df.image[1])# plt.figure(&quot;Image&quot;) # å›¾åƒçª—å£åç§°plt.imshow(img)plt.axis(&#x27;off&#x27;) # å…³æ‰åæ ‡è½´ä¸º offplt.title(&#x27;image&#x27;) # å›¾åƒé¢˜ç›®plt.show()



è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸‹ç»´åº¦è½¬æ¢ å³ [0, 1, 2]  æ¢æˆ [2, 1, 0], å¹¶åªå–æŸä¸€ä¸ªé€šé“ çœ‹çœ‹
12345# np.asarray(img).shape# å¯ä»¥çœ‹åˆ°å›¾ç‰‡åäº†ï¼Œæ­£ç¡®çš„é¡ºåºæ˜¯.transpose([2, 0, 1])img_trans = np.asarray(img).transpose([2,1,0])plt.imshow(img_trans[0])plt.show()





2.Preprocessingæ¥ä¸‹æ¥æˆ‘ä»¬åˆ†åˆ«è¦åš æ•°æ®å¢å¼ºã€æ•°æ®ç±»å®šä¹‰ã€æ•°æ®åŠ è½½å™¨æµ‹è¯•
2.1.1 å…ˆæ¥ç®—ä¸ªå¹³å‡å€¼æ ‡å‡†å·®è¿™é‡Œç®—çš„meanè·Ÿstdæ˜¯ä¸ºäº†Normalizeæˆ‘ä»¬çš„æ•°æ®ä½¿è®­ç»ƒæ›´ç¨³å®š
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import osimport cv2import numpy as npimport mathdef get_image_list(img_dir, isclasses=False):    &quot;&quot;&quot;å°†å›¾åƒçš„åç§°åˆ—è¡¨    args: img_dir:å­˜æ”¾å›¾ç‰‡çš„ç›®å½•          isclasses:å›¾ç‰‡æ˜¯å¦æŒ‰ç±»åˆ«å­˜æ”¾æ ‡å¿—    return: å›¾ç‰‡æ–‡ä»¶åç§°åˆ—è¡¨    &quot;&quot;&quot;    img_list = []    # è·¯å¾„ä¸‹å›¾åƒæ˜¯å¦æŒ‰ç±»åˆ«åˆ†ç±»å­˜æ”¾    if isclasses:        img_file = os.listdir(img_dir)        for class_name in img_file:            if not os.path.isfile(os.path.join(img_dir, class_name)):                     class_img_list = os.listdir(os.path.join(img_dir, class_name))                img_list.extend(class_img_list)             else:        img_list = os.listdir(img_dir)    print(img_list)    print(&#x27;image numbers: &#123;&#125;&#x27;.format(len(img_list)))    return img_listdef get_image_pixel_mean(img_dir, img_list, img_size):    &quot;&quot;&quot;æ±‚æ•°æ®é›†å›¾åƒçš„Rã€Gã€Bå‡å€¼    args: img_dir:          img_list:          img_size:    &quot;&quot;&quot;    R_sum = 0    G_sum = 0    B_sum = 0    count = 0    # å¾ªç¯è¯»å–æ‰€æœ‰å›¾ç‰‡    for img_name in img_list:        img_path = os.path.join(img_dir, img_name)        if not os.path.isdir(img_path):            image = cv2.imread(img_path)            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)            image = cv2.resize(image, (img_size, img_size))      # &lt;class &#x27;numpy.ndarray&#x27;&gt;            R_sum += image[:, :, 0].mean()            G_sum += image[:, :, 1].mean()            B_sum += image[:, :, 2].mean()            count += 1    R_mean = R_sum / count    G_mean = G_sum / count    B_mean = B_sum / count    print(&#x27;R_mean:&#123;&#125;, G_mean:&#123;&#125;, B_mean:&#123;&#125;&#x27;.format(R_mean,G_mean,B_mean))    RGB_mean = [R_mean, G_mean, B_mean]    return RGB_meandef get_image_pixel_std(img_dir, img_mean, img_list, img_size):    R_squared_mean = 0    G_squared_mean = 0    B_squared_mean = 0    count = 0    image_mean = np.array(img_mean)    # å¾ªç¯è¯»å–æ‰€æœ‰å›¾ç‰‡    for img_name in img_list:        img_path = os.path.join(img_dir, img_name)        if not os.path.isdir(img_path):            image = cv2.imread(img_path)    # è¯»å–å›¾ç‰‡            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)            image = cv2.resize(image, (img_size, img_size))      # &lt;class &#x27;numpy.ndarray&#x27;&gt;            image = image - image_mean    # é›¶å‡å€¼            # æ±‚å•å¼ å›¾ç‰‡çš„æ–¹å·®            R_squared_mean += np.mean(np.square(image[:, :, 0]).flatten())            G_squared_mean += np.mean(np.square(image[:, :, 1]).flatten())            B_squared_mean += np.mean(np.square(image[:, :, 2]).flatten())            count += 1    R_std = math.sqrt(R_squared_mean / count)    G_std = math.sqrt(G_squared_mean / count)    B_std = math.sqrt(B_squared_mean / count)    print(&#x27;R_std:&#123;&#125;, G_std:&#123;&#125;, B_std:&#123;&#125;&#x27;.format(R_std, G_std, B_std))    RGB_std = [R_std, G_std, B_std]    return R ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/#content-inner">5</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>ğŸ›´å‰å¾€github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>ç½‘ç«™èµ„è®¯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">æ–‡ç« æ•°ç›® :</div><div class="item-count">27</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»å­—æ•° :</div><div class="item-count">50k</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™è®¿å®¢æ•° :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»è®¿é—®é‡ :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ€åæ›´æ–°æ—¶é—´ :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-16T11:17:55.868Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è®¡ç®—æœºè§†è§‰</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è‡ªç„¶è¯­è¨€å¤„ç†</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">è®ºæ–‡ç²¾è¯»</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">æµç•…çš„Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">13</span><span class="categoryBar-list-descr">æ‹¥æœ‰ä¸€åˆ‡ å´å˜æˆå¤ªç©º</span></li></ul></div></div>';
      console.log('å·²æŒ‚è½½butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>