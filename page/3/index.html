<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/3/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-11-26 10:02:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://npm.elemecdn.com/poyone1222/eris/Eris51.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/11701.html" title="02 Bert Toxic 评论"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris21.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="02 Bert Toxic 评论"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/11701.html" title="02 Bert Toxic 评论">02 Bert Toxic 评论</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T02:16:14.994Z" title="发表于 2022-11-21 10:16:14">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-23T06:22:50.366Z" title="更新于 2022-11-23 14:22:50">2022-11-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/">情感分析</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E5%8F%A5%E5%AD%90%E5%88%86%E7%B1%BB/">句子分类</a></span></div><div class="content">packagetqdm
1from tqdm.notebook import tqdm

Kfold 废了 没用的东西 nlp中还是去练prompt好了
kfold引用的是下标，所以在dataloader中加载时，将下标赋值给sampler进行采样
1234from sklearn.model_selection import KFoldk_folds = 5kfold = KFold(n_splits=k_folds, shuffle=True)



Datasetdataset的返回有大概两种

返回数据，可以直接处理的那种

返回路径，即列表是存储 指向数据的路径的字符串，

在dataloader( cutmix( dataset ) )进行二次处理，
dataloader( tokenizer( dataset ) ) 相同，&#x3D;&#x3D;大概吧，还没试过，tokenizer返回的是打包的元组&#x3D;&#x3D;  不行，换kfold了


与kfold 或者 特类notebook中的tqdm 使用要注意细节


Bert 模型选择
bert的选择很关键，注意要看到文档说明的返回数据的形状
损失函数的选择也很重要，BCE中要使用view( -1, num_class) 进行转换。

Pipeline导包123456789101112import reimport pandas as pdimport numpy as npfrom tqdm.auto import tqdmimport torchimport torch.nn as nnfrom torch.utils.data import DataLoader, Datasetfrom transformers import BertTokenizer, BertForSequenceClassificationfrom sklearn.utils import shuffle as resetfrom sklearn.metrics import f1_score, accuracy_score

预处理&#x3D;&#x3D;train、valid 划分&#x3D;&#x3D;12345678910111213141516171819202122232425262728293031def one_hot_key(file_path):    data_df = pd.read_csv(file_path, error_bad_lines=False,engine=&#x27;python&#x27;)    colunms_name = data_df.columns.to_list()    onehot_labels = train_df[colunms_name].values.tolist()    #onehot_labels = [data_df.iloc[i, -6:].to_list() for i in tqdm(range(len(data_df)))]    data_df[&#x27;label&#x27;] = onehot_labels    data_for_bert = data_df[[&#x27;comment_text&#x27;, &#x27;label&#x27;]]        return data_for_bert# 对数据集index打乱后进行 82分def train_valid_split(data_df, test_size, shuffle=True, random_state=None):    if shuffle:        data_df = reset(data_df, random_state=random_state)    train = data_df[int(len(data_df)*test_size):].reset_index(drop = True)    valid  = data_df[:int(len(data_df)*test_size)].reset_index(drop = True)    return train, validdef train_epoch(num, file_path, batch_num):    data_df = one_hot_key(file_path)    # 进行 fold次划分返回 input_ids, token_type, mask    for i in range(num):        train, valid = train_valid_split(data_df, test_size=(1/fold))        train_set = mydataset(train)        valid_set = mydataset(valid)                yield train_loader, valid_loader # 做yield一次一次返回

补充一个对含有字符串的处理
123456def df_to_content(file_path):     colunms_name = data_df.columns.to_list()    onehot_labels = list(data_df[colunms_name].applymap(str).values.tolist())    data_df[&#x27;content&#x27;] = onehot_labels    data_df[&#x27;content&#x27;] = data_df[&#x27;content&#x27;].apply(lambda x: &#x27; &#x27;.join(x))     return train_df



Dataset12345678910111213141516171819202122232425class mydataset(Dataset):    def __init__(self, data, with_label=True):        self.data = data        self.tokenizer = tokenizer        self.with_label = with_label            def __getitem__(self, idx):        self.feature = self.data.comment_text[idx]        self.label = self.data.label[idx]                inputs = tokenizer(self.feature, return_tensors=&quot;pt&quot;,                            padding=&#x27;max_length&#x27;, max_length=160, truncation=True)                input_ids= inputs.input_ids.squeeze(0)        token_type = inputs.token_type_ids.squeeze(0)        mask = inputs.attention_mask.squeeze(0)        label = torch.Tensor(self.label)                if self.with_label:            return input_ids, token_type, mask, label        else:            return input_ids, token_type, mask            def __len__(self):        return self.data.shape[0]

训练Config123456789101112131415device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, num_labels=6) tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)loss_func = nn.BCEWithLogitsLoss()optimizer = torch.optim.Adam(model.parameters(), lr=0.001)path = &#x27;/content/train.csv&#x27;epochs = 5batch_num = 64best_score = 0!mkdir model!nvidia-smi

Train1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162for train_set, valid_set in train_epoch(epochs, path, batch_num):        train_loader = DataLoader(train_set, batch_size=batch_num)    valid_loader = DataLoader(valid_set, batch_size=batch_num)    train_loss, valid_loss = [], []    # train    model.to(device)    model.train()    for batch in tqdm(train_loader):        input_ids, token_type, mask, label = batch        input_ids, token_type, mask, label = input_ids.to(device), token_type.to(device),         									 mask.to(device), label.to(device)                optimizer.zero_grad()                outputs = model(input_ids, token_type, mask)        logits = outputs[0]        loss = loss_func(logits.view(-1,6),label.view(-1,6))        train_loss.append(loss.item())                loss.backward()        optimizer.step()            train_ave_loss = ( sum(train_loss)/ len(train_loss))        # valid    model.eval()    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]    for batch in tqdm(valid_loader):        input_ids, token_type, mask, label = tuple(t.to(device) for t in batch)                with torch.no_grad():            outputs = model(input_ids, token_type, mask)            b_logit_pred = outputs[0]            pred_label = torch.sigmoid(b_logit_pred)                        b_logit_pred = b_logit_pred.detach().cpu().numpy()            pred_label = pred_label.to(&#x27;cpu&#x27;).numpy()            label = label.to(&#x27;cpu&#x27;).numpy()            #tokenized_texts.append(b_input_ids) #这是啥            #logit_preds.append(b_logit_pred)            true_labels.append(label)            pred_labels.append(pred_label)                pred_labels = [item for sublist in pred_labels for item in sublist]    true_labels = [item for sublist in true_labels for item in sublist]    threshold = 0.50    pred_bools = [pl&gt;threshold for pl in pred_labels] #大于0.5的会被设置为True    true_bools = [tl==1 for tl in true_labels]    val_f1_accuracy = f1_score(true_bools,pred_bools,average=&#x27;micro&#x27;)*100    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100    print(&#x27;F1 Validation Accuracy: &#x27;, val_f1_accuracy)    print(&#x27;Flat Validation Accuracy: &#x27;, val_flat_accuracy)            	global best_score             if best_score &lt; val_f1_accuracy + val_flat_accuracy:            best_score = val_f1_accuracy + val_flat_accuracy            torch.save(model.state_dict(), f&#x27;/model/Score:&#123;best_score&#125;.pth&#x27;)            #model.config.to_json_file()

不太明白为什么要阀值输出，做了以下优化
12345678910sample = np.random.random((3,4)).tolist()label = np.array([[1,1,0,0],[0,1,1,1],[0,0,0,1]]).tolist()df = pd.DataFrame(sample)result = []for i in range(df.shape[0]):    length = sum(label[i])    result.append(df.iloc[i,:].sort_values(ascending=False).index.to_list()[:length])result

12345678pred_df = pd.DataFrame(pred_labels.tolist())result = []for i in range(df.shape[0]):    length = sum(label[i])    result.append(pred_df[i,:].sort_values(ascending=False).index.to_list()[:length])# result即为结果

是因为预测的时候，没有标签数量啦，笨蛋
测试测试的时候就不能shuffle了
123456#preprocesing 之后直接丢给 fine_tune好的 bertmodel = BertForSequenceClassification.from_pretrained(&#x27;model_path.pth&#x27;).to(device)pred_label_ids = torch.argmax(outputs[0])name_class = tokenizer.convert_ids_to_tokens(pred_label_ids)

F1：阈值搜索123456789101112131415161718192021222324252627282930313233macro_thresholds = np.array(range(1,10))/10f1_results, flat_acc_results = [], []for th in macro_thresholds:    pred_bools = [pl&gt;th for pl in pred_labels]    test_f1_accuracy = f1_score(true_bools,pred_bools,average=&#x27;micro&#x27;)    test_flat_accuracy = accuracy_score(true_bools, pred_bools)    f1_results.append(test_f1_accuracy)    flat_acc_results.append(test_flat_accuracy)best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold valuemicro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold valuesf1_results, flat_acc_results = [], []for th in micro_thresholds:    pred_bools = [pl&gt;th for pl in pred_labels]    test_f1_accuracy = f1_score(true_bools,pred_bools,average=&#x27;micro&#x27;)    test_flat_accuracy = accuracy_score(true_bools, pred_bools)    f1_results.append(test_f1_accuracy)    flat_acc_results.append(test_flat_accuracy)best_f1_idx = np.argmax(f1_results) #best threshold value# Printing and saving classification reportprint(&#x27;Best Threshold: &#x27;, micro_thresholds[best_f1_idx])print(&#x27;Test F1 Accuracy: &#x27;, f1_results[best_f1_idx])print(&#x27;Test Flat Accuracy: &#x27;, flat_acc_results[best_f1_idx], &#x27;\n&#x27;)best_pred_bools = [pl&gt;micro_thresholds[best_f1_idx] for pl in pred_labels]clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)pickle.dump(clf_report_optimized, open(&#x27;classification_report_optimized.txt&#x27;,&#x27;wb&#x27;))print(clf_report_optimized)

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/14457.html" title="01 叶子分类Tricks"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris49.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="01 叶子分类Tricks"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/14457.html" title="01 叶子分类Tricks">01 叶子分类Tricks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T02:16:14.970Z" title="发表于 2022-11-21 10:16:14">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-23T06:22:50.384Z" title="更新于 2022-11-23 14:22:50">2022-11-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/CV/">CV</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/">图片识别</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/CNN/">CNN</a></span></div><div class="content">LibrarySklearn
预处理函数preprocessing

12345678910from sklearn import preprocessing# preprocessing有很多个子函数，比如正态化数据，做one-hot-key等sex = pd.Series([&quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;])encoder = preprocessing.LabelEncoder()       #获取一个LabelEncoder					#可用df[&#x27;label&#x27;].unique().to_list()获得标签类别model = encoder.fit([&quot;male&quot;, &quot;female&quot;])      #训练LabelEncoder, 把male编码为0，female编码为1sex = model.transform(sex)                   #使用训练好的LabelEncoder对原数据进行编码# 得到[1 0 0 1 1]

CutMix1234567891011121314151617181920212223242526from cutmix.cutmix import CutMixfrom cutmix.utils import CutMixCrossEntropyLosstrain_loss_function = CutMixCrossEntropyLoss(True)#其内部实现如下:	  # Compute the accuracy for current batch.      # acc = (logits.argmax(dim=-1) == labels).float().mean()      # Record the loss and accuracy    trainloader = torch.utils.data.DataLoader(                      CutMix(TrainValidData(train_val_path, img_path, transform = train_transform), 	                              num_class=176, beta=1.0, prob=0.5, num_mix=2),                       batch_size=128, sampler=train_subsampler, num_workers=0)#train_transform、train_subsampler变量如下:train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)train_transform = transforms.Compose([    # 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。    # 然后，缩放图像以创建224 x 224的新图像    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),    transforms.RandomHorizontalFlip(),    # 随机更改亮度，对比度和饱和度    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),    # 添加随机噪声    transforms.ToTensor(),    # 标准化图像的每个通道    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])



TrickLr_scheduler1234from torch.optim.lr_scheduler import CosineAnnealingLRscheduler = CosineAnnealingLR(optimizer,T_max=10)scheduler.step()



数据增强图片增强: 随机剪裁、翻转、对比度和颜色调整、随机噪声
1234567891011121314151617train_transform = transforms.Compose([    # 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。    # 然后，缩放图像以创建224 x 224的新图像    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),    transforms.RandomHorizontalFlip(),    # 随机更改亮度，对比度和饱和度    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),    # 添加随机噪声    transforms.ToTensor(),    # 标准化图像的每个通道    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])val_test_transform = transforms.Compose([    transforms.Resize(256),    # 从图像中心裁切224x224大小的图片    transforms.CenterCrop(224),    transforms.ToTensor(),    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])



Ensemble将数据通过多个不同的模型，以其最终在验证集上的正确率做权重分配，投票得出最终的submission
或者直接平均，要注意每个模型对数据的要求和敏感度。
K折交叉验证不光训练的时候K折验证，提交submission也要K折，以获得最平滑平均的正确率
示例:
123456789101112131415&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn.model_selection import KFold&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])&gt;&gt;&gt; y = np.array([1, 2, 3, 4])&gt;&gt;&gt; kf = KFold(n_splits=2)&gt;&gt;&gt; kf.get_n_splits(X)2&gt;&gt;&gt; print(kf)KFold(n_splits=2, random_state=None, shuffle=False)&gt;&gt;&gt; for train_index, test_index in kf.split(X):...     print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)...     X_train, X_test = X[train_index], X[test_index]...     y_train, y_test = y[train_index], y[test_index]TRAIN: [2 3] TEST: [0 1]TRAIN: [0 1] TEST: [2 3]

kfold.split()将返回一个训练索引集合，一个验证索引集合
1234567from sklearn.model_selection import KFoldk_folds = 5kfold = KFold(n_splits=k_folds, shuffle=True)#trainin：for fold, (train_ids,valid_ids) in enumerate(kfold.split(train_val_dataset)):



Code Trick
固定随机种子seed

12345678910111213141516def seed_everything(seed):    random.seed(seed)    np.random.seed(seed)    torch.manual_seed(seed)    torch.cuda.manual_seed(seed)    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = True    # LHY版本myseed = 6666  # set a random seed for reproducibilitytorch.backends.cudnn.deterministic = Truetorch.backends.cudnn.benchmark = Falsenp.random.seed(myseed)      torch.manual_seed(myseed)if torch.cuda.is_available():    torch.cuda.manual_seed_all(myseed)


查看模型信息summary

12from torchinfo import summarysummary(model)


模型的参数冷冻

123456789101112131415# 是否要冻住模型的前面一些层def set_parameter_requires_grad(model, feature_extracting):    if feature_extracting:        model = model        for param in model.parameters():            param.requires_grad = False   		# 因为是全局变量，设置为false就会冻住# ResNeSt模型def resnest_model(num_classes, feature_extract = False):    model_ft = resnest50(pretrained=True)    set_parameter_requires_grad(model_ft, feature_extract)    num_ftrs = model_ft.fc.in_features    model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, num_classes))    return model_ft



Summary查看博主的论文inference

EDA
Train trick

具体代码training epoch可以换成LHY的不进步多少个循环再停止
12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667print(&#x27;--------------------------------------&#x27;)#使用sklearn分割数据集 									#注意此处使用的特殊的数据集，kfold本质是对index处理for fold, (train_ids,valid_ids) in enumerate(kfold.split(train_val_dataset)):  print(f&#x27;FOLD &#123;fold&#125;&#x27;)  print(&#x27;--------------------------------------&#x27;)  #采样加入噪音  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)  valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)  #因为分割后返回的只是数据集的索引编号，所以需腰引入dataloader  trainloader = torch.utils.data.DataLoader(                      CutMix(TrainValidData(train_val_path, img_path, transform = train_transform), num_class=176, beta=1.0, prob=0.5, num_mix=2),                       batch_size=32, sampler=train_subsampler, num_workers=0)  validloader = torch.utils.data.DataLoader(                      TrainValidData(train_val_path, img_path, transform = val_test_transform),                      batch_size=32, sampler=valid_subsampler, num_workers=0)    #放到gpu  model = resnest_model(176)  model = model.to(device)  model.device = device    #优化器和优化方案  optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay= weight_decay)  scheduler = CosineAnnealingLR(optimizer,T_max=10)  #训练循环  for epoch in range(0,num_epochs):        model.train()    print(f&#x27;Starting epoch &#123;epoch+1&#125;&#x27;)    train_losses = []    train_accs = []    for batch in tqdm(trainloader):	  #GPU      imgs, labels = batch      imgs = imgs.to(device)      labels = labels.to(device)      logits = model(imgs)      loss = train_loss_function(logits,labels)      optimizer.zero_grad()      loss.backward()      optimizer.step()          # Compute the accuracy for current batch. 注意这里是每个batch的loss      # acc = (logits.argmax(dim=-1) == labels).float().mean()      # Record the loss and accuracy.              train_losses.append(loss.item())      # train_accs.append(acc)    print(&quot;第%d个epoch的学习率：%f&quot; % (epoch+1,optimizer.param_groups[0][&#x27;lr&#x27;]))        scheduler.step()	# 总loss / batch数    train_loss = np.sum(train_losses) / len(train_losses)    # train_acc = np.sum(train_accs) / len(train_accs)    print(f&quot;[ Train | &#123;epoch + 1:03d&#125;/&#123;num_epochs:03d&#125; ] loss = &#123;train_loss:.5f&#125;&quot;)  # Train process (all epochs) is complete  print(&#x27;Training process has finished. Saving trained model.&#x27;)  print(&#x27;Starting validation&#x27;)

validation123456789101112131415161718192021222324252627282930313233343536#这一段代码跟上面是连着的  # 按K折存储模型  print(&#x27;saving model with loss &#123;:.3f&#125;&#x27;.format(train_loss))  save_path = f&#x27;./model-fold-&#123;fold&#125;.pth&#x27;  torch.save(model.state_dict(),save_path)  #开始验证  model.eval()  valid_losses = []  valid_accs = []  with torch.no_grad():    for batch in tqdm(validloader):      imgs, labels = batch      logits = model(imgs.to(device))      #这里的loss-function用的是nn.CrossEntropyLoss()      loss = valid_loss_function(logits,labels.to(device))       acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()      valid_losses.append(loss.item())              valid_accs.append(acc)    valid_loss = np.sum(valid_losses)/len(valid_losses)    valid_acc = np.sum(valid_accs)/len(valid_accs)    print(f&quot;[ Valid | &#123;epoch + 1:03d&#125;/&#123;num_epochs:03d&#125; ] loss = &#123;valid_loss:.5f&#125;,           acc =&#123;valid_acc:.5f&#125;&quot;)    print(&#x27;Accuracy for fold %d: %d&#x27; % (fold, valid_acc))    print(&#x27;--------------------------------------&#x27;)    results[fold] = valid_acc          print(f&#x27;K-FOLD CROSS VALIDATION RESULTS FOR &#123;k_folds&#125; FOLDS&#x27;)print(&#x27;--------------------------------&#x27;)total_summation = 0.0for key, value in results.items():  print(f&#x27;Fold &#123;key&#125;: &#123;value&#125; &#x27;)  total_summation += valueprint(f&#x27;Average: &#123;total_summation/len(results.items())&#125; &#x27;)



test submission1234567891011121314151617181920212223242526272829303132333435363738394041424344#加入TTA数据增强包 不知道增强了什么import ttach as tta#载入测试数据集testloader = torch.utils.data.DataLoader(                      TestData(test_path, img_path, transform = val_test_transform),                      batch_size=32, num_workers=0)##模型预设model = resnest_model(176)model = model.to(device)#按K折载入模型for test_fold in range(k_folds):  model_path = f&#x27;./model-fold-&#123;test_fold&#125;.pth&#x27;  saveFileName = f&#x27;./submission-fold-&#123;test_fold&#125;.csv&#x27;  model.load_state_dict(torch.load(model_path))  # Some modules like Dropout o ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/2/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/#content-inner">2</a><span class="page-number current">3</span></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">14</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">28.2k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-11-26T02:02:42.843Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" href="categories/CV/">CV</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" href="categories/NLP/">NLP</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" href="categories/Trick/">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">小技巧</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" href="categories/Dive-Into-Paper/">Dive Into Paper</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" href="categories/Python/">Python</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" href="categories/Package/">Package</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">import torch as tf</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>