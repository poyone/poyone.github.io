<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/3/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-06 20:02:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/61418.html" title="22-12-4 drill spoiler alert"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/made in Abyss/nanachi02.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="22-12-4 drill spoiler alert"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/61418.html" title="22-12-4 drill spoiler alert">22-12-4 drill spoiler alert</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-04T16:08:37.450Z" title="å‘è¡¨äº 2022-12-05 00:08:37">2022-12-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-04T16:38:43.706Z" title="æ›´æ–°äº 2022-12-05 00:38:43">2022-12-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E8%8B%B1%E8%AF%AD/">è‹±è¯­</a></span></div><div class="content">
It will be interesting to see if our fine-tuned model picks up on those particularities of the dataset (spoiler alert: it will).

ç‰¹æ®Š n.
å‰§é€è­¦å‘Š (ä¿šè¯­get)


You know the drill

drillåŒæ—¶ä¹Ÿæœ‰ç»è¿‡ä¸¥æ ¼è®­ç»ƒçš„æ„æ€


æ˜¯ä¸€ä¸ªéå¸¸åœ°é“çš„ç¾è¯­å£è¯­è¡¨è¾¾ï¼ŒYou know the drillçš„æ„æ€æ˜¯â€ä½ çŸ¥é“è¯¥æ€ä¹ˆåšâ€ï¼Œå¥—ç”¨ä¸­æ–‡é‡Œæµè¡Œçš„ä¸€å¥è¯´æ³•â€ä½ æ‡‚çš„â€ï¼ŒåŒ—äº¬è¯é‡Œæœ‰ä¸€ä¸ªéå¸¸è´´åˆ‡çš„è¯´æ³•â€é—¨å„¿æ¸…â€ã€‚è¿™ä¸ªçŸ­è¯­é€šå¸¸æŒ‡:ç”±äºæŸäººå¯¹ç‰¹å®šæƒ…å†µä¸‹çš„åšäº‹æµç¨‹éå¸¸ç†Ÿæ‚‰ï¼Œæ‰€ä»¥ä¸éœ€è¦å¤šè§£é‡Šï¼Œå°±çŸ¥é“è¯¥å¦‚ä½•åº”å¯¹ã€å¤„ç†ã€‚You know the drillçš„è‹±æ–‡è§£é‡Šä¸º: to understand what usually happens in a givenä½ çŸ¥é“é’»æ˜¯ä¸€ä¸ªéå¸¸åœ°é“çš„ç¾è¯­å£è¯­è¡¨è¾¾ï¼Œä½ çŸ¥é“é’»çš„æ„æ€æ˜¯â€œä½ çŸ¥é“è¯¥æ€ä¹ˆåšâ€ï¼Œå¥—ç”¨ä¸­æ–‡é‡Œæµè¡Œçš„ä¸€å¥è¯´æ³•â€œä½ æ‡‚çš„â€ï¼ŒåŒ—äº¬è¯é‡Œæœ‰ä¸€ä¸ªéå¸¸è´´åˆ‡çš„è¯´æ³•â€œé—¨å„¿æ¸…â€ã€‚è¿™ä¸ªçŸ­è¯­é€šå¸¸æŒ‡:ç”±äºæŸäººå¯¹ç‰¹å®šæƒ…å†µä¸‹çš„åšäº‹æµç¨‹éå¸¸ç†Ÿæ‚‰ï¼Œæ‰€ä»¥ä¸éœ€è¦å¤šè§£é‡Šï¼Œå°±çŸ¥é“è¯¥å¦‚ä½•åº”å¯¹ã€å¤„ç†.æ‚¨çŸ¥é“çš„è‹±æ–‡è§£é‡Šä¸ºï¼šè¦äº†è§£åœ¨ç»™å®šçš„situation; an expression meaning â€œyou know what to do, no questions requiredâ€.; to know whatneeds to be done or what usually happens in a situation.æƒ…å†µï¼›æ„æ€æ˜¯â€œä½ çŸ¥é“è¯¥åšä»€ä¹ˆï¼Œä¸éœ€è¦é—®é—®é¢˜â€ï¼›çŸ¥é“éœ€è¦åšä»€ä¹ˆï¼Œæˆ–è€…åœ¨ä¸€ä¸ªæƒ…å†µä¸‹é€šå¸¸ä¼šå‘ç”Ÿä»€ä¹ˆã€‚



</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/35234.html" title="huggingface proxy"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris26.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="huggingface proxy"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/35234.html" title="huggingface proxy">huggingface proxy</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-04T06:10:13.101Z" title="å‘è¡¨äº 2022-12-04 14:10:13">2022-12-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-04T08:20:44.937Z" title="æ›´æ–°äº 2022-12-04 16:20:44">2022-12-04</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/Proxy/">Proxy</a></span></div><div class="content">huggingfaceåœ¨æœ‰ä»£ç†çš„æƒ…å†µä¸‹
1234567from transformers import AutoModelForSeq2SeqLM, AutoTokenizerprx = &#123;&#x27;https&#x27;: &#x27;http://127.0.0.1:7890&#x27;&#125;model_name = &quot;Helsinki-NLP/opus-mt-zh-en&quot;save_path = r&#x27;D:\00mydataset\huggingface model&#x27;tokenizer = AutoTokenizer.from_pretrained(model_name, proxies=prx, cache_dir=save_path)

ç›´æ¥è®¾ç½®ä»£ç†å°±å¯ä»¥æ¥åˆ°huggingfaceäº†
vscodeåœ¨settingé‡Œé¢çš„proxyçš„ç¬¬ä¸€æ è®¾ç½®http://127.0.0.1:7890 åé¢çš„7890å°±æ˜¯ä½ çš„ç«¯å£å·ï¼Œåœ¨ä½ çš„ä»£ç†å¤„å¯ä»¥æŸ¥çœ‹
pippip æˆ‘æ ¹æ®è¿™ç¯‡æ–‡ç« åœ¨ç»ˆç«¯ç›´æ¥è®¾ç½® set http_proxy=&#39;http://127.0.0.1:7890&#39; è¿™å°±å¥½äº†
123456789# åœ¨pipç›®å½•åˆ›å»ºå¹¶ç¼–è¾‘pip.iniï¼ˆé…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼‰cd C:\Users\(ä½ çš„ç”¨æˆ·å)   mkdir pip                # åˆ›å»ºpipæ–‡ä»¶å¤¹cd pip                     # è¿›å…¥pipè·¯å¾„ç›®å½•ä¸‹cd.&gt;pip.ini              # åˆ›å»ºpip.iniæ–‡ä»¶# ç„¶åæ‰“å¼€C:\Users(ç”¨æˆ·å)\pip\pip.iniï¼Œæ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š[global]proxy=http://10.20.217.2:8080

</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/10656.html" title="å¥æ„ç›¸ä¼¼åº¦ PipeLineæ€»ç»“"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris41.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="å¥æ„ç›¸ä¼¼åº¦ PipeLineæ€»ç»“"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/10656.html" title="å¥æ„ç›¸ä¼¼åº¦ PipeLineæ€»ç»“">å¥æ„ç›¸ä¼¼åº¦ PipeLineæ€»ç»“</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-11-27T02:47:31.323Z" title="å‘è¡¨äº 2022-11-27 10:47:31">2022-11-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-04T16:48:36.782Z" title="æ›´æ–°äº 2022-12-05 00:48:36">2022-12-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%8F%A5%E6%84%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6/">å¥æ„ç›¸ä¼¼åº¦</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/Pipeline/">Pipeline</a></span></div><div class="content">ä¸»è¦è¿›è¡Œè®­ç»ƒæ¡†æ¶ä¼˜åŒ–

ç«¯åˆ°ç«¯ ML å®æ–½ï¼ˆè®­ç»ƒã€éªŒè¯ã€é¢„æµ‹ã€è¯„ä¼°ï¼‰
è½»æ¾é€‚åº”æ‚¨è‡ªå·±çš„æ•°æ®é›†
ä¿ƒè¿›å…¶ä»–åŸºäº BERT çš„æ¨¡å‹ï¼ˆBERTã€ALBERTã€â€¦ï¼‰çš„å¿«é€Ÿå®éªŒ
ä½¿ç”¨æœ‰é™çš„è®¡ç®—èµ„æºè¿›è¡Œå¿«é€Ÿè®­ç»ƒï¼ˆæ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯â€¦â€¦ï¼‰
å¤š GPU æ‰§è¡Œ
åˆ†ç±»å†³ç­–çš„é˜ˆå€¼é€‰æ‹©ï¼ˆä¸ä¸€å®šæ˜¯ 0.5ï¼‰
å†»ç»“ BERT å±‚ï¼Œåªæ›´æ–°åˆ†ç±»å±‚æƒé‡æˆ–æ›´æ–°æ‰€æœ‰æƒé‡
ç§å­è®¾ç½®ï¼Œå¯å¤ç°ç»“æœ

PipeLineå¯¼åŒ…12345678910111213import torchimport torch.nn as nnimport osimport copyimport torch.optim as optimimport randomimport numpy as npimport pandas as pdfrom torch.utils.data import DataLoader, Datasetfrom torch.cuda.amp import autocast, GradScalerfrom tqdm.auto import tqdmfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmupfrom datasets import load_dataset, load_metric

Dataset123456789101112131415161718192021222324252627282930313233343536class CustomDataset(Dataset):    def __init__(self, data, maxlen, with_labels=True, bert_model=&#x27;albert-base-v2&#x27;):        self.data = data  # pandas dataframe        #Initialize the tokenizer        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)          self.maxlen = maxlen        self.with_labels = with_labels     def __len__(self):        return len(self.data)    def __getitem__(self, index):        #æ ¹æ®ç´¢å¼•ç´¢å–DataFrameä¸­å¥å­1ä½™å¥å­2        sent1 = str(self.data.loc[index, &#x27;sentence1&#x27;])        sent2 = str(self.data.loc[index, &#x27;sentence2&#x27;])        # å¯¹å¥å­å¯¹åˆ†è¯ï¼Œå¾—åˆ°input_idsã€attention_maskå’Œtoken_type_ids        encoded_pair = self.tokenizer(sent1, sent2,                                       padding=&#x27;max_length&#x27;,  # å¡«å……åˆ°æœ€å¤§é•¿åº¦                                      truncation=True,  # æ ¹æ®æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­                                      max_length=self.maxlen,                                        return_tensors=&#x27;pt&#x27;)  # è¿”å›torch.Tensorå¼ é‡                token_ids = encoded_pair[&#x27;input_ids&#x27;].squeeze(0)  # tensor token ids        attn_masks = encoded_pair[&#x27;attention_mask&#x27;].squeeze(0)  # padded valueså¯¹åº”ä¸º &quot;0&quot; ï¼Œå…¶ä»–tokenä¸º1        token_type_ids = encoded_pair[&#x27;token_type_ids&#x27;].squeeze(0)  #ç¬¬ä¸€ä¸ªå¥å­çš„å€¼ä¸º0ï¼Œç¬¬äºŒä¸ªå¥å­çš„å€¼ä¸º1 # åªæœ‰ä¸€å¥å…¨ä¸º0        if self.with_labels:  # True if the dataset has labels            label = self.data.loc[index, &#x27;label&#x27;]            return token_ids, attn_masks, token_type_ids, label          else:            return token_ids, attn_masks, token_type_ids

å»ºè®®ï¼Œè¿›è¡Œæµ‹è¯•
12sample = next(iter(DataLoader(tr_dataset, batch_size=2)))sample

12tr_model = SentencePairClassifier(freeze_bert=True)tr_model(sample[0], sample[1], sample[2])

å°±æ˜¯æ–¹ä¾¿æœ€åçš„ç»´åº¦è½¬æ¢ï¼Œsqueezeã€flattenã€viewï¼›ç”šè‡³å¯ä»¥ç”¨reshapeæ–¹æ³•
æ¨¡å‹å®šä¹‰12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SentencePairClassifier(nn.Module):    def __init__(self, bert_model=&quot;albert-base-v2&quot;, freeze_bert=False):        super(SentencePairClassifier, self).__init__()        #  åˆå§‹åŒ–é¢„è®­ç»ƒæ¨¡å‹Bert xxx        self.bert_layer = AutoModel.from_pretrained(bert_model)        #  encoder éšè—å±‚å¤§å°        if bert_model == &quot;albert-base-v2&quot;:  # 12M å‚æ•°            hidden_size = 768        elif bert_model == &quot;albert-large-v2&quot;:  # 18M å‚æ•°            hidden_size = 1024        elif bert_model == &quot;albert-xlarge-v2&quot;:  # 60M å‚æ•°            hidden_size = 2048        elif bert_model == &quot;albert-xxlarge-v2&quot;:  # 235M å‚æ•°            hidden_size = 4096        elif bert_model == &quot;bert-base-uncased&quot;: # 110M å‚æ•°            hidden_size = 768        elif bert_model == &quot;roberta-base&quot;: #             hidden_size = 768        # å›ºå®šBertå±‚ æ›´æ–°åˆ†ç±»è¾“å‡ºå±‚        if freeze_bert:            for p in self.bert_layer.parameters():                p.requires_grad = False                        self.dropout = nn.Dropout(p=0.1)        # åˆ†ç±»è¾“å‡º        self.cls_layer = nn.Linear(hidden_size, 1)    @autocast()  # æ··åˆç²¾åº¦è®­ç»ƒ    def forward(self, input_ids, attn_masks, token_type_ids):        &#x27;&#x27;&#x27;        Inputs:            -input_ids : Tensor  containing token ids            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2        &#x27;&#x27;&#x27;        # è¾“å…¥ç»™Bertï¼Œè·å–ä¸Šä¸‹æ–‡è¡¨ç¤º        # cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)        # last_hidden_state,pooler_output,all_hidden_states 12å±‚        # å°†last layer hidden-state of the [CLS] è¾“å…¥åˆ° classifier layer        # - last_hidden_state çš„å‘é‡å¹³å‡        # - å–all_hidden_statesæœ€åå››å±‚ï¼Œç„¶ååšå¹³å‡ weighted å¹³å‡        # - last_hidden_state+lstm        # è·å–è¾“å‡º        logits = self.cls_layer(self.dropout(outputs[&#x27;pooler_output&#x27;]))        return logits

å›ºå®šéšæœºç§å­12345678910def set_seed(seed):    &quot;&quot;&quot; å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¤ç°    &quot;&quot;&quot;    torch.manual_seed(seed)    torch.cuda.manual_seed_all(seed)    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    np.random.seed(seed)    random.seed(seed)    os.environ[&#x27;PYTHONHASHSEED&#x27;] = str(seed)

è®­ç»ƒå’Œè¯„ä¼°12!mkdir models 	#å¯ä»¥åœ¨ä¹‹å‰è¡¥å……ç»å¯¹è·¯å¾„!mkdir results



123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):    best_loss = np.Inf    best_ep = 1    nb_iterations = len(train_loader)    print_every = nb_iterations // 5  # æ‰“å°é¢‘ç‡    iters = []    train_losses = []    val_losses = []    scaler = GradScaler()    for ep in range(epochs):        net.train()        running_loss = 0.0        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):            # è½¬ä¸ºcudaå¼ é‡            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)                # æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒ            with autocast():                # Obtaining the logits from the model                logits = net(seq, attn_masks, token_type_ids)                # Computing loss                loss = criterion(logits.squeeze(-1), labels.float())                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged            # Backpropagating the gradients            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.            scaler.scale(loss).backward()            if (it + 1) % iters_to_accumulate == 0:                # Optimization step                # scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.                # If these gradients do not contain infs or NaNs, opti.step() is then called,                # otherwise, opti.step() is skipped.                scaler.step(opti)                # Updates the scale for next iteration.                scaler.update()                # æ ¹æ®è¿­ä»£æ¬¡æ•°è°ƒæ•´å­¦ä¹ ç‡ã€‚                lr_scheduler.step()                # æ¢¯åº¦æ¸…é›¶                opti.zero_grad()            running_loss += loss.item()            if (it + 1) % print_every == 0:  # Print training loss information                print()                print(f&quot;Iteration &#123;it+1&#125;/&#123;nb_iterations&#125; of epoch &#123;ep+1&#125; complete. \                Loss : &#123;running_loss / print_every&#125; &quot;)                running_loss = 0.0        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss        print()        print(f&quot;Epoch &#123;ep+1&#125; complete! Validation Loss : &#123;val_loss&#125;&quot;)        if val_loss &lt; best_loss:            print(&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;.format(best_loss, val_loss))            print()            net_copy = copy.deepcopy(net)  # # ä¿å­˜æœ€ä¼˜æ¨¡å‹            best_loss = val_loss            best_ep = ep + 1    # ä¿å­˜æ¨¡å‹    path_to_model=f&#x27;models/&#123;bert_model&#125;_lr_&#123;lr&#125;_val_loss_&#123;round(best_loss, 5)&#125;_ep_&#123;best_ep&#125;.pt&#x27;    torch.save(net_copy.state_dict(), path_to_model)    print(&quot;The model has been saved in &#123;&#125;&quot;.format(path_to_model))    del loss    torch.cuda.empty_cache() # æ¸…ç©ºæ˜¾å­˜    def evaluate_loss(net, device, criterion, dataloader):    &quot;&quot;&quot;    è¯„ä¼°è¾“å‡º    &quot;&quot;&quot;    net.eval()    mean_loss = 0    count = 0    with torch.no_grad():        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)            logits = net(seq, attn_masks, token_type_ids)            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()            count += 1    return mean_loss / count


æ³¨æ„autocastå’Œç´¯è®¡æ¢¯åº¦ è¿™ä¸¤ç§åŠ é€Ÿè®¡ç®—çš„æ–¹æ³•

evaluateçš„æ—¶å€™è¦æ³¨æ„æ•°æ®çš„ç»´åº¦ï¼Œæ ‡ç­¾çš„ç±»å‹


è¶…å‚æ•° &amp; å¼€å§‹è®­ç»ƒ1234567bert_model = &quot;albert-base-v2&quot;  # &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;freeze_bert = False  # æ˜¯å¦å†»ç»“Bertmaxlen = 128  # æœ€å¤§é•¿åº¦bs = 16  # batch sizeiters_to_accumulate = 2  # æ¢¯åº¦ç´¯åŠ lr = 2e-5  # learning rateepochs = 2  # è®­ç»ƒè½®æ•°



123456789101112131415161718192021222324252627282930#  å›ºå®šéšæœºç§å­ ä¾¿äºå¤ç°set_seed(1) # 2022 # åˆ›å»ºè®­ç»ƒé›†ä¸éªŒè¯é›†print(&quot;Reading training data...&quot;)train_set = CustomDataset(df_train, maxlen, bert_model)print(&quot;Reading validation data...&quot;)val_set = CustomDataset(df_val, maxlen, bert_model)# å¸¸è§è®­ç»ƒé›†ä¸éªŒè¯é›†DataLoadertrain_loader = DataLoader(train_set, batch_size=bs, num_workers=0)val_loader = DataLoader(val_set, ba ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/26087.html" title="Weight &amp; Bias"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Weight &amp; Bias"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/26087.html" title="Weight &amp; Bias">Weight &amp; Bias</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-11-26T09:17:27.733Z" title="å‘è¡¨äº 2022-11-26 17:17:27">2022-11-26</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-06T11:02:19.379Z" title="æ›´æ–°äº 2022-12-06 19:02:19">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Trick/">Trick</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/wandb/">wandb</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/pytorch/">pytorch</a></span></div><div class="content">å¾…å®Œæˆ
æºç ç»†èŠ‚æ•´ç†

torch.inference_mode()with no_gradientçš„ä¸€ç§åŠ é€Ÿ  å‚è€ƒæ–‡æ¡£
 nn.MarginRankingLoss()æ–‡æ¡£ margin &#x3D; 0  x1å¤§äºx2 åˆ™å»-yï¼Œviceversa å– y
*loss(x1,x2,y)&#x3D;max(0,âˆ’yâˆ—(x1âˆ’x2)+margin)*
è¿™é‡Œæœ€åçš„lossæ˜¯å¹³å‡åçš„
1234567891011121314151617181920212223loss = nn.MarginRankingLoss()input1 = torch.randn(3, requires_grad=True)input2 = torch.randn(3, requires_grad=True)target = torch.randn(3).sign()output = loss(input1, input2, target)output.backward()```input1, input2, target, output(tensor([ 0.0277, -0.3806,  1.0405], requires_grad=True), tensor([-0.9075,  0.3271,  0.1156], requires_grad=True), tensor([ 1., -1., -1.]), tensor(0.3083, grad_fn=&lt;MeanBackward0&gt;)) input1 - input2 , (input1 - input2) * (-target)(tensor([ 0.9352, -0.7077,  0.9249], grad_fn=&lt;SubBackward0&gt;), tensor([-0.9352, -0.7077,  0.9249], grad_fn=&lt;MulBackward0&gt;), loss = 0.9249/3 ```



gc.collect()æ¸…é™¤å†…å­˜
defaultdictè·å¾—åˆ›å»ºkeyä¸ç»™valueä¹Ÿä¸æŠ¥é”™çš„dict
12345from collections import defaultdicthistory = defaultdict(list)history[&#x27;Train Loss&#x27;].append(1.1)



StratifiedKFold()12345678from sklearn.model_selection import StratifiedKFold, KFoldskf = StratifiedKFold(n_splits=CONFIG[&#x27;n_fold&#x27;], shuffle=True, random_state=CONFIG[&#x27;seed&#x27;])for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):    df.loc[val_ , &quot;kfold&quot;] = int(fold)    df[&quot;kfold&quot;] = df[&quot;kfold&quot;].astype(int)

ç¬¬äº”è¡Œ å°†Xåˆ†kæŠ˜ï¼Œyæ ‡ç­¾ä¸ºæ ·æœ¬å¯¹åº”indexï¼Œfold åœ¨ 0~5
å¾—åˆ°df[â€œkfoldâ€] åˆ—åŒ…å« å±äºç¬¬å‡ æŠ˜çš„ validæ•°æ®
é€šè¿‡ä¸‹é¢çš„å‡½æ•°ç›´æ¥é€‰æ‹©éæœ¬æŠ˜çš„æ•°æ®ä½œä¸ºtrainï¼Œå…¶ä»–çš„å°±æ˜¯valid
df_train = df[df.kfold != fold].reset_index(drop=True) df_valid = df[df.kfold == fold].reset_index(drop=True)
12345678910111213def prepare_loaders(fold):    df_train = df[df.kfold != fold].reset_index(drop=True)    df_valid = df[df.kfold == fold].reset_index(drop=True)        train_dataset = JigsawDataset(df_train, tokenizer=CONFIG[&#x27;tokenizer&#x27;], max_length=CONFIG[&#x27;max_length&#x27;])    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG[&#x27;tokenizer&#x27;], max_length=CONFIG[&#x27;max_length&#x27;])    train_loader = DataLoader(train_dataset, batch_size=CONFIG[&#x27;train_batch_size&#x27;],                               num_workers=2, shuffle=True, pin_memory=True, drop_last=True)    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[&#x27;valid_batch_size&#x27;],                               num_workers=2, shuffle=False, pin_memory=True)        return train_loader, valid_loade



tqdm1bar = tqdm(enumerate(dataloader), total=len(dataloader))

å•ä¸ªepochä¸‹é¢å¯¹baråšå¦‚ä¸‹è®¾ç½®
12bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,                        LR=optimizer.param_groups[0][&#x27;lr&#x27;])  



Weights &amp; Biases (W&amp;B) 
hash ä¸€ä¸ªé¡¹ç›®id

train valid å®šä¹‰ä¸€ä¸ª 1ä¸ªepoch çš„å‡½æ•° è¿”å› åˆ†åˆ«å…¶ä¸­çš„loss

wandb.log({â€œTrain Lossâ€: train_epoch_loss}) ä½¿ç”¨ log æ–¹å¼è®°å½• æŸå¤±å‡½æ•°

run = wandb.init(project=&#39;Jigsaw&#39;, 
                     config=CONFIG,
                     job_type=&#39;Train&#39;,
                     group=CONFIG[&#39;group&#39;],
                     tags=[&#39;roberta-base&#39;, f&#39;&#123;HASH_NAME&#125;&#39;, &#39;margin-loss&#39;],
                     name=f&#39;&#123;HASH_NAME&#125;-fold-&#123;fold&#125;&#39;,
                     anonymous=&#39;must&#39;)
1TRAIN PART

run.finish()

1234æ˜¾ç¤ºå¦‚ä¸‹			&#x27;hash--------name&#x27;Syncing run k5nu8k69390a-fold-0 to Weights &amp; Biases (docs).



æµç¨‹è®­ç»ƒæç‚¼
for fold in range(0, CONFIG[â€˜n_foldâ€™])
wandb.init
prepare_loadersã€fetch_scheduler
run_training
train_one_epochã€valid_one_epoch â€”-&gt; to got model, loss for wandb



ä¸­é—´æºæ‚ W&amp;B çš„æ•°æ®å®æ—¶è½½å…¥åˆ†æå³å¯
df[â€˜yâ€™].value_counts(normalize&#x3D;True) to got the percentage of each values
åŸæ–‡é“¾æ¥
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/45348.html" title="HF 02"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris12.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF 02"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45348.html" title="HF 02">HF 02</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.815Z" title="å‘è¡¨äº 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-06T10:59:33.572Z" title="æ›´æ–°äº 2022-12-06 18:59:33">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a></span></div><div class="content">å¾…å®Œæˆ
ç¤ºä¾‹è¯¦è§£

Transformeråˆ†ä¸¤å—BERT&amp;GPTéƒ½å¾ˆèƒ½æ‰“
BERTç”¨çš„æ˜¯transformerçš„encoder

BERTæ˜¯ç”¨äº†Transformerçš„encoderä¾§çš„ç½‘ç»œï¼Œencoderä¸­çš„Self-attentionæœºåˆ¶åœ¨ç¼–ç ä¸€ä¸ªtokençš„æ—¶å€™åŒæ—¶åˆ©ç”¨äº†å…¶ä¸Šä¸‹æ–‡çš„tokenï¼Œå…¶ä¸­â€˜åŒæ—¶åˆ©ç”¨ä¸Šä¸‹æ–‡â€™å³ä¸ºåŒå‘çš„ä½“ç°ï¼Œè€Œå¹¶éæƒ³Bi-LSTMé‚£æ ·æŠŠå¥å­å€’åºè¾“å…¥ä¸€éã€‚


GPTç”¨çš„æ˜¯transformerçš„decoder

åœ¨å®ƒä¹‹å‰æ˜¯GPTï¼ŒGPTä½¿ç”¨çš„æ˜¯Transformerçš„decoderä¾§çš„ç½‘ç»œï¼ŒGPTæ˜¯ä¸€ä¸ªå•å‘è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œæ›´é€‚ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œé€šè¿‡å‰æ–‡å»é¢„æµ‹å½“å‰çš„å­—ã€‚



Bertçš„embeddingEmbeddingç”±ä¸‰ç§Embeddingæ±‚å’Œè€Œæˆï¼š

Token Embeddingsæ˜¯è¯å‘é‡ï¼Œç¬¬ä¸€ä¸ªå•è¯æ˜¯CLSæ ‡å¿—ï¼Œå¯ä»¥ç”¨äºä¹‹åçš„åˆ†ç±»ä»»åŠ¡

BERTåœ¨ç¬¬ä¸€å¥å‰ä¼šåŠ ä¸€ä¸ª[CLS]æ ‡å¿—ï¼Œæœ€åä¸€å±‚è¯¥ä½å¯¹åº”å‘é‡å¯ä»¥ä½œä¸ºæ•´å¥è¯çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œç”¨äºä¸‹æ¸¸çš„åˆ†ç±»ä»»åŠ¡ç­‰ã€‚å› ä¸ºä¸æ–‡æœ¬ä¸­å·²æœ‰çš„å…¶å®ƒè¯ç›¸æ¯”ï¼Œè¿™ä¸ªæ— æ˜æ˜¾è¯­ä¹‰ä¿¡æ¯çš„ç¬¦å·ä¼šæ›´â€œå…¬å¹³â€åœ°èåˆæ–‡æœ¬ä¸­å„ä¸ªè¯çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½çš„è¡¨ç¤ºæ•´å¥è¯çš„è¯­ä¹‰ã€‚ å…·ä½“æ¥è¯´ï¼Œself-attentionæ˜¯ç”¨æ–‡æœ¬ä¸­çš„å…¶å®ƒè¯æ¥å¢å¼ºç›®æ ‡è¯çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä½†æ˜¯ç›®æ ‡è¯æœ¬èº«çš„è¯­ä¹‰è¿˜æ˜¯ä¼šå ä¸»è¦éƒ¨åˆ†çš„ï¼Œå› æ­¤ï¼Œç»è¿‡BERTçš„12å±‚ï¼ˆBERT-baseä¸ºä¾‹ï¼‰ï¼Œæ¯æ¬¡è¯çš„embeddingèåˆäº†æ‰€æœ‰è¯çš„ä¿¡æ¯ï¼Œå¯ä»¥å»æ›´å¥½çš„è¡¨ç¤ºè‡ªå·±çš„è¯­ä¹‰ã€‚è€Œ[CLS]ä½æœ¬èº«æ²¡æœ‰è¯­ä¹‰ï¼Œç»è¿‡12å±‚ï¼Œå¥å­çº§åˆ«çš„å‘é‡ï¼Œç›¸æ¯”å…¶ä»–æ­£å¸¸è¯ï¼Œå¯ä»¥æ›´å¥½çš„è¡¨å¾å¥å­è¯­ä¹‰ã€‚


Segment Embeddingsç”¨æ¥åŒºåˆ«ä¸¤ç§å¥å­ï¼Œå› ä¸ºé¢„è®­ç»ƒä¸å…‰åšLMè¿˜è¦åšä»¥ä¸¤ä¸ªå¥å­ä¸ºè¾“å…¥çš„åˆ†ç±»ä»»åŠ¡

Position Embeddingså’Œä¹‹å‰æ–‡ç« ä¸­çš„Transformerä¸ä¸€æ ·ï¼Œä¸æ˜¯ä¸‰è§’å‡½æ•°è€Œæ˜¯å­¦ä¹ å‡ºæ¥çš„


APItokenizer12345678910111213141516171819202122232425262728293031323334from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging# configæ¨¡å—MODEL_NAME=&quot;bert-base-chinese&quot;config = AutoConfig.from_pretrained(MODEL_NAME) #c onfigå¯ä»¥é…ç½®æ¨¡å‹ä¿¡æ¯# tokenizeræ¨¡å—tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)tokenizer.all_special_ids # æŸ¥çœ‹ç‰¹æ®Šç¬¦å·çš„id [100, 102, 0, 101, 103]tokenizer.all_special_tokens # æŸ¥çœ‹token  [&#x27;[UNK]&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;]tokenizer.vocab_size # è¯æ±‡è¡¨å¤§å°tokenizer.vocab # è¯æ±‡å¯¹åº”çš„dictå½¢å¼## tokeningtext=&quot;æˆ‘åœ¨åŒ—äº¬å·¥ä½œ&quot;token_ids=tokenizer.encode(text)token_ids # [101, 2769, 1762, 1266, 776, 2339, 868, 102]tokenizer.convert_ids_to_tokens(token_ids) # [&#x27;[CLS]&#x27;, &#x27;æˆ‘&#x27;, &#x27;åœ¨&#x27;, &#x27;åŒ—&#x27;, &#x27;äº¬&#x27;, &#x27;å·¥&#x27;, &#x27;ä½œ&#x27;, &#x27;[SEP]&#x27;]		  # convert_tokens_to_ids(tokens) ä¸ºå¯¹åº”æ–¹æ³•    ## padding åšå‘é‡å¡«å……token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)## encode_plustoken_ids=tokenizer.encode_plus(    text,padding=&quot;max_length&quot;,    max_length=30,    add_special_tokens=True,    return_tensors=&#x27;pt&#x27;,    return_token_type_ids=True,    return_attention_mask=True)



ä½¿ç”¨pre_trainæ¨¡å‹è½½å…¥æ•°æ®12model=AutoModel.from_pretrained(MODEL_NAME)outputs=model(token_ids[&#x27;input_ids&#x27;],token_ids[&#x27;attention_mask&#x27;])



æ•°æ®é›†datasetå®šä¹‰12345678910111213141516171819202122232425262728293031323334class EnterpriseDataset(Dataset):    def __init__(self,texts,labels,tokenizer,max_len):        self.texts=texts        self.labels=labels        self.tokenizer=tokenizer        self.max_len=max_len    def __len__(self):        return len(self.texts)        def __getitem__(self,item):        &quot;&quot;&quot;        item ä¸ºæ•°æ®ç´¢å¼•ï¼Œè¿­ä»£å–ç¬¬itemæ¡æ•°æ®        &quot;&quot;&quot;        text=str(self.texts[item])        label=self.labels[item]                encoding=self.tokenizer.encode_plus(            text,            add_special_tokens=True,            max_length=self.max_len,            return_token_type_ids=True,            pad_to_max_length=True,            return_attention_mask=True,            return_tensors=&#x27;pt&#x27;,  #è½¬ä¸ºtensor        )        #print(encoding[&#x27;input_ids&#x27;])        return &#123;            &#x27;texts&#x27;:text,            &#x27;input_ids&#x27;:encoding[&#x27;input_ids&#x27;].flatten(),            &#x27;attention_mask&#x27;:encoding[&#x27;attention_mask&#x27;].flatten(),            # toeken_type_ids:0            &#x27;labels&#x27;:torch.tensor(label,dtype=torch.long)        &#125;

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/45347.html" title="HF 01"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris12.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF 01"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45347.html" title="HF 01">HF 01</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.804Z" title="å‘è¡¨äº 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-06T10:59:15.183Z" title="æ›´æ–°äº 2022-12-06 18:59:15">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">â€¢</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a></span></div><div class="content">å¾…å®Œæˆ
ç¤ºä¾‹è¯¦è§£


Attention åŸæ–‡
Why
å…¨é¢æ‹¥æŠ±Transformerï¼šNLPä¸‰å¤§ç‰¹å¾æŠ½å–å™¨(CNN&#x2F;RNN&#x2F;TF)ä¸­ï¼Œè¿‘ä¸¤å¹´æ–°æ¬¢Transformeræ˜æ˜¾ä¼šå¾ˆå¿«æˆä¸ºNLPé‡Œæ‹…å½“å¤§ä»»çš„æœ€ä¸»æµçš„ç‰¹å¾æŠ½å–å™¨ã€‚

åƒWordvecå‡ºç°ä¹‹åä¸€æ ·ï¼Œåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸç§å„ç§ç›®æ ‡çš†å¯å‘é‡åŒ–ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬ç»å¸¸å¬åˆ°çš„â€œä¸‡ç‰©çš†å¯Embeddingâ€ã€‚è€ŒTransformeræ¨¡å‹å’ŒBertæ¨¡å‹çš„å‡ºç°ï¼Œæ›´æ˜¯NLPé¢†åŸŸåˆ’æ—¶ä»£çš„äº§ç‰©ï¼šå°†transformerå’ŒåŒå‘è¯­è¨€æ¨¡å‹è¿›è¡Œèåˆï¼Œä¾¿å¾—åˆ°NLPåˆ’æ—¶ä»£çš„ï¼Œä¹Ÿæ˜¯å½“ä¸‹åœ¨å„è‡ªNLPä¸‹æµä»»åŠ¡ä¸­è·å¾—state-of-the-artçš„æ¨¡å‹-BERT

BERTèµ·æºäºé¢„è®­ç»ƒçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼Œä¸ä¹‹å‰çš„æ¨¡å‹ä¸åŒï¼ŒBERTæ˜¯ä¸€ç§æ·±åº¦åŒå‘çš„ã€æ— ç›‘ç£çš„è¯­è¨€è¡¨ç¤ºï¼Œä¸”ä»…ä½¿ç”¨çº¯æ–‡æœ¬è¯­æ–™åº“è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚ä¸Šä¸‹æ–‡æ— å…³æ¨¡å‹ï¼ˆå¦‚word2vecæˆ–GloVeï¼‰ä¸ºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå•è¯ç”Ÿæˆä¸€ä¸ªè¯å‘é‡è¡¨ç¤ºï¼Œå› æ­¤å®¹æ˜“å‡ºç°å•è¯çš„æ­§ä¹‰é—®é¢˜ã€‚BERTè€ƒè™‘åˆ°å•è¯å‡ºç°æ—¶çš„ä¸Šä¸‹æ–‡ã€‚ä¾‹å¦‚ï¼Œè¯â€œæ°´åˆ†â€çš„word2vecè¯å‘é‡åœ¨â€œæ¤ç‰©éœ€è¦å¸æ”¶æ°´åˆ†â€å’Œâ€œè´¢åŠ¡æŠ¥è¡¨é‡Œæœ‰æ°´åˆ†â€æ˜¯ç›¸åŒçš„ï¼Œä½†BERTæ ¹æ®ä¸Šä¸‹æ–‡çš„ä¸åŒæä¾›ä¸åŒçš„è¯å‘é‡ï¼Œè¯å‘é‡ä¸å¥å­è¡¨è¾¾çš„å¥æ„æœ‰å…³ã€‚


Embeddingï¼š

é¦–å…ˆç±»ä¼¼ word2vec çš„ tokenåŒ–ï¼Œå†è¿›è¡Œç‰‡æ®µæ ‡è®°( segment )ï¼Œæœ€å ids çš„ä½ç½®ç¼–ç (  position )
ç¼–ç åä¸€ä¸ª â€™è¯â€˜ æœ‰ä¸‰ä¸ªä¿¡æ¯ï¼Œtokenã€æ®µè½ä½ç½®ä¿¡æ¯ã€ç»å¯¹ä½ç½®ä¿¡æ¯( id: 1ã€2ã€3â€¦)

Embeddingè§£å†³çš„é—®é¢˜:
é¦–å…ˆæ˜¯ä¹‹å‰ç”¨çš„ One-Hot Keyï¼Œé«˜ç»´åº¦ï¼Œç¦»æ•£çš„ï¼Œä½ä¿¡æ¯å¯†åº¦çš„å‚¨å­˜å½¢å¼
å…¶æ¬¡æ˜¯æ›´å¥½çš„ Contextual Similarityï¼Œä¸Šä¸‹æ–‡ç›¸å…³ç›¸ä¼¼æ€§ã€‚

Preview Apiå‰ç½®æŸ¥çœ‹ï¼š123456from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained(&quot;bert-base-chinese&quot;) # è·å–ç›¸åº”æ¨¡å‹çš„tokenizerfrom transformers import AutoTokenizer, AutoModelForMaskedLMmodel = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-chinese&quot;) #æŸ¥çœ‹æ¨¡å‹çš„åˆ†å±‚



å‡½æ•°è°ƒç”¨ï¼šå­—å…¸å¤§å°ï¼ŒtokenåŒ–ï¼ŒidsåŒ–12345678910vocab = tokenizer.vocabprint(&quot;å­—å…¸å¤§å°ï¼š&quot;, len(vocab)) 	# æŸ¥çœ‹å­—å…¸å¤§å°text = &quot;[CLS] ç­‰åˆ°æ½®æ°´ [MASK] äº†ï¼Œå°±çŸ¥é“è°æ²’ç©¿è£¤å­ã€‚&quot;tokens = tokenizer.tokenize(text)				# å°†æ–‡å­—åˆ†è¯ids = tokenizer.convert_tokens_to_ids(tokens)	# å°†æ–‡å­—è½¬åŒ–ä¸ºæ•°å­—ï¼Œè¿›è¡Œç¼–ç &#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;ç­‰&#x27;, &#x27;åˆ°&#x27;, &#x27;æ½®&#x27;, &#x27;æ°´&#x27;, &#x27;[MASK]&#x27;, &#x27;äº†&#x27;, &#x27;ï¼Œ&#x27;, &#x27;å°±&#x27;, &#x27;çŸ¥&#x27;] ...[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ... &#x27;&#x27;&#x27;

Maskæ¨¡å‹çš„ä½¿ç”¨1234567891011121314151617181920212223242526from transformers import BertForMaskedLM# é™¤äº† tokens ä»¥å¤–æˆ‘å€‘é‚„éœ€è¦è¾¨åˆ¥å¥å­çš„ segment idstokens_tensor = torch.tensor([ids])  # (1, seq_len)segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)# ä½¿ç”¨ masked LM ä¼°è¨ˆ [MASK] ä½ç½®æ‰€ä»£è¡¨çš„å¯¦éš› token maskedLM_model.eval()with torch.no_grad():    outputs = maskedLM_model(tokens_tensor, segments_tensors)    predictions = outputs[0]    # (1, seq_len, num_hidden_units)del maskedLM_model# å°‡ [MASK] ä½ç½®çš„æ©Ÿç‡åˆ†ä½ˆå– top k æœ€æœ‰å¯èƒ½çš„ tokens å‡ºä¾†masked_index = 5k = 3probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())# é¡¯ç¤º top k å¯èƒ½çš„å­—ã€‚ä¸€èˆ¬æˆ‘å€‘å°±æ˜¯å– top 1 å½“åšé¢„æµ‹å€¼print(&quot;è¼¸å…¥ tokens ï¼š&quot;, tokens[:10], &#x27;...&#x27;)print(&#x27;-&#x27; * 50)for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):    tokens[masked_index] = t    print(&quot;Top &#123;&#125; (&#123;:2&#125;%)ï¼š&#123;&#125;&quot;.format(i, int(p.item() * 100), tokens[:10]), &#x27;...&#x27;)

â€‹	è¼¸å…¥ tokens ï¼š [â€˜[CLS]â€™, â€˜ç­‰â€™, â€˜åˆ°â€™, â€˜æ½®â€™, â€˜æ°´â€™, â€˜[MASK]â€™, â€˜äº†â€™, â€˜ï¼Œâ€™, â€˜å°±â€™, â€˜çŸ¥â€™] â€¦â€‹	Top 1 (65%)ï¼š[â€˜[CLS]â€™, â€˜ç­‰â€™, â€˜åˆ°â€™, â€˜æ½®â€™, â€˜æ°´â€™, â€˜æ¥â€™, â€˜äº†â€™, â€˜ï¼Œâ€™, â€˜å°±â€™, â€˜çŸ¥â€™] â€¦â€‹	Top 2 ( 4%)ï¼š[â€˜[CLS]â€™, â€˜ç­‰â€™, â€˜åˆ°â€™, â€˜æ½®â€™, â€˜æ°´â€™, â€˜è¿‡â€™, â€˜äº†â€™, â€˜ï¼Œâ€™, â€˜å°±â€™, â€˜çŸ¥â€™] â€¦â€‹	Top 3 ( 4%)ï¼š[â€˜[CLS]â€™, â€˜ç­‰â€™, â€˜åˆ°â€™, â€˜æ½®â€™, â€˜æ°´â€™, â€˜å¹²â€™, â€˜äº†â€™, â€˜ï¼Œâ€™, â€˜å°±â€™, â€˜çŸ¥â€™] â€¦
å¯è§†åŒ–æ¨¡å‹: bertvizPandasé¢„å¤„ç†æ–‡æœ¬
å¤šä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
nltkåº“çš„stopwords
textblobåº“çš„æ‹¼å†™æ£€æŸ¥ã€è¯å¹²æŠ½å–ã€è¯æ€§è¿˜åŸç­‰

æ–‡æœ¬æ•°æ®çš„åŸºæœ¬ä½“å¾æå–
è¯æ±‡æ•°é‡

å­—ç¬¦æ•°é‡

å¹³å‡å­—é•¿

åœç”¨è¯æ•°é‡

ç‰¹æ®Šå­—ç¬¦æ•°é‡

æ•°å­—æ•°é‡

å¤§å†™å­—æ¯æ•°é‡


æ–‡æœ¬æ•°æ®çš„åŸºæœ¬é¢„å¤„ç†
å°å†™è½¬æ¢
å»é™¤æ ‡ç‚¹ç¬¦å·
å»é™¤åœç”¨è¯
å»é™¤é¢‘ç°è¯
å»é™¤ç¨€ç–è¯
æ‹¼å†™æ ¡æ­£
åˆ†è¯(tokenization)
è¯å¹²æå–(stemming)
è¯å½¢è¿˜åŸ(lemmatization)

</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/2/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/#content-inner">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/4/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>ğŸ›´å‰å¾€github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>ç½‘ç«™èµ„è®¯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">æ–‡ç« æ•°ç›® :</div><div class="item-count">20</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»å­—æ•° :</div><div class="item-count">32.1k</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™è®¿å®¢æ•° :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»è®¿é—®é‡ :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ€åæ›´æ–°æ—¶é—´ :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-06T12:02:22.496Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è®¡ç®—æœºè§†è§‰</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è‡ªç„¶è¯­è¨€å¤„ç†</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">è®ºæ–‡ç²¾è¯»</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">æµç•…çš„Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">8</span><span class="categoryBar-list-descr">æ‹¥æœ‰ä¸€åˆ‡ å´å˜æˆå¤ªç©º</span></li></ul></div></div>';
      console.log('å·²æŒ‚è½½butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>