<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/page/3/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/page/3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-06 20:02:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/61418.html" title="22-12-4 drill spoiler alert"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/made in Abyss/nanachi02.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="22-12-4 drill spoiler alert"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/61418.html" title="22-12-4 drill spoiler alert">22-12-4 drill spoiler alert</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-04T16:08:37.450Z" title="发表于 2022-12-05 00:08:37">2022-12-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-04T16:38:43.706Z" title="更新于 2022-12-05 00:38:43">2022-12-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E8%8B%B1%E8%AF%AD/">英语</a></span></div><div class="content">
It will be interesting to see if our fine-tuned model picks up on those particularities of the dataset (spoiler alert: it will).

特殊 n.
剧透警告 (俚语get)


You know the drill

drill同时也有经过严格训练的意思


是一个非常地道的美语口语表达，You know the drill的意思是”你知道该怎么做”，套用中文里流行的一句说法”你懂的”，北京话里有一个非常贴切的说法”门儿清”。这个短语通常指:由于某人对特定情况下的做事流程非常熟悉，所以不需要多解释，就知道该如何应对、处理。You know the drill的英文解释为: to understand what usually happens in a given你知道钻是一个非常地道的美语口语表达，你知道钻的意思是“你知道该怎么做”，套用中文里流行的一句说法“你懂的”，北京话里有一个非常贴切的说法“门儿清”。这个短语通常指:由于某人对特定情况下的做事流程非常熟悉，所以不需要多解释，就知道该如何应对、处理.您知道的英文解释为：要了解在给定的situation; an expression meaning “you know what to do, no questions required”.; to know whatneeds to be done or what usually happens in a situation.情况；意思是“你知道该做什么，不需要问问题”；知道需要做什么，或者在一个情况下通常会发生什么。



</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/35234.html" title="huggingface proxy"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris26.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="huggingface proxy"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/35234.html" title="huggingface proxy">huggingface proxy</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-04T06:10:13.101Z" title="发表于 2022-12-04 14:10:13">2022-12-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-04T08:20:44.937Z" title="更新于 2022-12-04 16:20:44">2022-12-04</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Proxy/">Proxy</a></span></div><div class="content">huggingface在有代理的情况下
1234567from transformers import AutoModelForSeq2SeqLM, AutoTokenizerprx = &#123;&#x27;https&#x27;: &#x27;http://127.0.0.1:7890&#x27;&#125;model_name = &quot;Helsinki-NLP/opus-mt-zh-en&quot;save_path = r&#x27;D:\00mydataset\huggingface model&#x27;tokenizer = AutoTokenizer.from_pretrained(model_name, proxies=prx, cache_dir=save_path)

直接设置代理就可以接到huggingface了
vscode在setting里面的proxy的第一栏设置http://127.0.0.1:7890 后面的7890就是你的端口号，在你的代理处可以查看
pippip 我根据这篇文章在终端直接设置 set http_proxy=&#39;http://127.0.0.1:7890&#39; 这就好了
123456789# 在pip目录创建并编辑pip.ini（配置文件不存在时）cd C:\Users\(你的用户名)   mkdir pip                # 创建pip文件夹cd pip                     # 进入pip路径目录下cd.&gt;pip.ini              # 创建pip.ini文件# 然后打开C:\Users(用户名)\pip\pip.ini，添加如下内容：[global]proxy=http://10.20.217.2:8080

</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/10656.html" title="句意相似度 PipeLine总结"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris41.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="句意相似度 PipeLine总结"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/10656.html" title="句意相似度 PipeLine总结">句意相似度 PipeLine总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-27T02:47:31.323Z" title="发表于 2022-11-27 10:47:31">2022-11-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-04T16:48:36.782Z" title="更新于 2022-12-05 00:48:36">2022-12-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%8F%A5%E6%84%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6/">句意相似度</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Pipeline/">Pipeline</a></span></div><div class="content">主要进行训练框架优化

端到端 ML 实施（训练、验证、预测、评估）
轻松适应您自己的数据集
促进其他基于 BERT 的模型（BERT、ALBERT、…）的快速实验
使用有限的计算资源进行快速训练（混合精度、梯度累积……）
多 GPU 执行
分类决策的阈值选择（不一定是 0.5）
冻结 BERT 层，只更新分类层权重或更新所有权重
种子设置，可复现结果

PipeLine导包12345678910111213import torchimport torch.nn as nnimport osimport copyimport torch.optim as optimimport randomimport numpy as npimport pandas as pdfrom torch.utils.data import DataLoader, Datasetfrom torch.cuda.amp import autocast, GradScalerfrom tqdm.auto import tqdmfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmupfrom datasets import load_dataset, load_metric

Dataset123456789101112131415161718192021222324252627282930313233343536class CustomDataset(Dataset):    def __init__(self, data, maxlen, with_labels=True, bert_model=&#x27;albert-base-v2&#x27;):        self.data = data  # pandas dataframe        #Initialize the tokenizer        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)          self.maxlen = maxlen        self.with_labels = with_labels     def __len__(self):        return len(self.data)    def __getitem__(self, index):        #根据索引索取DataFrame中句子1余句子2        sent1 = str(self.data.loc[index, &#x27;sentence1&#x27;])        sent2 = str(self.data.loc[index, &#x27;sentence2&#x27;])        # 对句子对分词，得到input_ids、attention_mask和token_type_ids        encoded_pair = self.tokenizer(sent1, sent2,                                       padding=&#x27;max_length&#x27;,  # 填充到最大长度                                      truncation=True,  # 根据最大长度进行截断                                      max_length=self.maxlen,                                        return_tensors=&#x27;pt&#x27;)  # 返回torch.Tensor张量                token_ids = encoded_pair[&#x27;input_ids&#x27;].squeeze(0)  # tensor token ids        attn_masks = encoded_pair[&#x27;attention_mask&#x27;].squeeze(0)  # padded values对应为 &quot;0&quot; ，其他token为1        token_type_ids = encoded_pair[&#x27;token_type_ids&#x27;].squeeze(0)  #第一个句子的值为0，第二个句子的值为1 # 只有一句全为0        if self.with_labels:  # True if the dataset has labels            label = self.data.loc[index, &#x27;label&#x27;]            return token_ids, attn_masks, token_type_ids, label          else:            return token_ids, attn_masks, token_type_ids

建议，进行测试
12sample = next(iter(DataLoader(tr_dataset, batch_size=2)))sample

12tr_model = SentencePairClassifier(freeze_bert=True)tr_model(sample[0], sample[1], sample[2])

就是方便最后的维度转换，squeeze、flatten、view；甚至可以用reshape方法
模型定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SentencePairClassifier(nn.Module):    def __init__(self, bert_model=&quot;albert-base-v2&quot;, freeze_bert=False):        super(SentencePairClassifier, self).__init__()        #  初始化预训练模型Bert xxx        self.bert_layer = AutoModel.from_pretrained(bert_model)        #  encoder 隐藏层大小        if bert_model == &quot;albert-base-v2&quot;:  # 12M 参数            hidden_size = 768        elif bert_model == &quot;albert-large-v2&quot;:  # 18M 参数            hidden_size = 1024        elif bert_model == &quot;albert-xlarge-v2&quot;:  # 60M 参数            hidden_size = 2048        elif bert_model == &quot;albert-xxlarge-v2&quot;:  # 235M 参数            hidden_size = 4096        elif bert_model == &quot;bert-base-uncased&quot;: # 110M 参数            hidden_size = 768        elif bert_model == &quot;roberta-base&quot;: #             hidden_size = 768        # 固定Bert层 更新分类输出层        if freeze_bert:            for p in self.bert_layer.parameters():                p.requires_grad = False                        self.dropout = nn.Dropout(p=0.1)        # 分类输出        self.cls_layer = nn.Linear(hidden_size, 1)    @autocast()  # 混合精度训练    def forward(self, input_ids, attn_masks, token_type_ids):        &#x27;&#x27;&#x27;        Inputs:            -input_ids : Tensor  containing token ids            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2        &#x27;&#x27;&#x27;        # 输入给Bert，获取上下文表示        # cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)        # last_hidden_state,pooler_output,all_hidden_states 12层        # 将last layer hidden-state of the [CLS] 输入到 classifier layer        # - last_hidden_state 的向量平均        # - 取all_hidden_states最后四层，然后做平均 weighted 平均        # - last_hidden_state+lstm        # 获取输出        logits = self.cls_layer(self.dropout(outputs[&#x27;pooler_output&#x27;]))        return logits

固定随机种子12345678910def set_seed(seed):    &quot;&quot;&quot; 固定随机种子，保证结果复现    &quot;&quot;&quot;    torch.manual_seed(seed)    torch.cuda.manual_seed_all(seed)    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    np.random.seed(seed)    random.seed(seed)    os.environ[&#x27;PYTHONHASHSEED&#x27;] = str(seed)

训练和评估12!mkdir models 	#可以在之前补充绝对路径!mkdir results



123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):    best_loss = np.Inf    best_ep = 1    nb_iterations = len(train_loader)    print_every = nb_iterations // 5  # 打印频率    iters = []    train_losses = []    val_losses = []    scaler = GradScaler()    for ep in range(epochs):        net.train()        running_loss = 0.0        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):            # 转为cuda张量            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)                # 混合精度加速训练            with autocast():                # Obtaining the logits from the model                logits = net(seq, attn_masks, token_type_ids)                # Computing loss                loss = criterion(logits.squeeze(-1), labels.float())                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged            # Backpropagating the gradients            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.            scaler.scale(loss).backward()            if (it + 1) % iters_to_accumulate == 0:                # Optimization step                # scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.                # If these gradients do not contain infs or NaNs, opti.step() is then called,                # otherwise, opti.step() is skipped.                scaler.step(opti)                # Updates the scale for next iteration.                scaler.update()                # 根据迭代次数调整学习率。                lr_scheduler.step()                # 梯度清零                opti.zero_grad()            running_loss += loss.item()            if (it + 1) % print_every == 0:  # Print training loss information                print()                print(f&quot;Iteration &#123;it+1&#125;/&#123;nb_iterations&#125; of epoch &#123;ep+1&#125; complete. \                Loss : &#123;running_loss / print_every&#125; &quot;)                running_loss = 0.0        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss        print()        print(f&quot;Epoch &#123;ep+1&#125; complete! Validation Loss : &#123;val_loss&#125;&quot;)        if val_loss &lt; best_loss:            print(&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;.format(best_loss, val_loss))            print()            net_copy = copy.deepcopy(net)  # # 保存最优模型            best_loss = val_loss            best_ep = ep + 1    # 保存模型    path_to_model=f&#x27;models/&#123;bert_model&#125;_lr_&#123;lr&#125;_val_loss_&#123;round(best_loss, 5)&#125;_ep_&#123;best_ep&#125;.pt&#x27;    torch.save(net_copy.state_dict(), path_to_model)    print(&quot;The model has been saved in &#123;&#125;&quot;.format(path_to_model))    del loss    torch.cuda.empty_cache() # 清空显存    def evaluate_loss(net, device, criterion, dataloader):    &quot;&quot;&quot;    评估输出    &quot;&quot;&quot;    net.eval()    mean_loss = 0    count = 0    with torch.no_grad():        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)            logits = net(seq, attn_masks, token_type_ids)            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()            count += 1    return mean_loss / count


注意autocast和累计梯度 这两种加速计算的方法

evaluate的时候要注意数据的维度，标签的类型


超参数 &amp; 开始训练1234567bert_model = &quot;albert-base-v2&quot;  # &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;freeze_bert = False  # 是否冻结Bertmaxlen = 128  # 最大长度bs = 16  # batch sizeiters_to_accumulate = 2  # 梯度累加lr = 2e-5  # learning rateepochs = 2  # 训练轮数



123456789101112131415161718192021222324252627282930#  固定随机种子 便于复现set_seed(1) # 2022 # 创建训练集与验证集print(&quot;Reading training data...&quot;)train_set = CustomDataset(df_train, maxlen, bert_model)print(&quot;Reading validation data...&quot;)val_set = CustomDataset(df_val, maxlen, bert_model)# 常见训练集与验证集DataLoadertrain_loader = DataLoader(train_set, batch_size=bs, num_workers=0)val_loader = DataLoader(val_set, ba ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/26087.html" title="Weight &amp; Bias"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Weight &amp; Bias"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/26087.html" title="Weight &amp; Bias">Weight &amp; Bias</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-26T09:17:27.733Z" title="发表于 2022-11-26 17:17:27">2022-11-26</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T11:02:19.379Z" title="更新于 2022-12-06 19:02:19">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Trick/">Trick</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/wandb/">wandb</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/pytorch/">pytorch</a></span></div><div class="content">待完成
源码细节整理

torch.inference_mode()with no_gradient的一种加速  参考文档
 nn.MarginRankingLoss()文档 margin &#x3D; 0  x1大于x2 则去-y，viceversa 取 y
*loss(x1,x2,y)&#x3D;max(0,−y∗(x1−x2)+margin)*
这里最后的loss是平均后的
1234567891011121314151617181920212223loss = nn.MarginRankingLoss()input1 = torch.randn(3, requires_grad=True)input2 = torch.randn(3, requires_grad=True)target = torch.randn(3).sign()output = loss(input1, input2, target)output.backward()```input1, input2, target, output(tensor([ 0.0277, -0.3806,  1.0405], requires_grad=True), tensor([-0.9075,  0.3271,  0.1156], requires_grad=True), tensor([ 1., -1., -1.]), tensor(0.3083, grad_fn=&lt;MeanBackward0&gt;)) input1 - input2 , (input1 - input2) * (-target)(tensor([ 0.9352, -0.7077,  0.9249], grad_fn=&lt;SubBackward0&gt;), tensor([-0.9352, -0.7077,  0.9249], grad_fn=&lt;MulBackward0&gt;), loss = 0.9249/3 ```



gc.collect()清除内存
defaultdict获得创建key不给value也不报错的dict
12345from collections import defaultdicthistory = defaultdict(list)history[&#x27;Train Loss&#x27;].append(1.1)



StratifiedKFold()12345678from sklearn.model_selection import StratifiedKFold, KFoldskf = StratifiedKFold(n_splits=CONFIG[&#x27;n_fold&#x27;], shuffle=True, random_state=CONFIG[&#x27;seed&#x27;])for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):    df.loc[val_ , &quot;kfold&quot;] = int(fold)    df[&quot;kfold&quot;] = df[&quot;kfold&quot;].astype(int)

第五行 将X分k折，y标签为样本对应index，fold 在 0~5
得到df[“kfold”] 列包含 属于第几折的 valid数据
通过下面的函数直接选择非本折的数据作为train，其他的就是valid
df_train = df[df.kfold != fold].reset_index(drop=True) df_valid = df[df.kfold == fold].reset_index(drop=True)
12345678910111213def prepare_loaders(fold):    df_train = df[df.kfold != fold].reset_index(drop=True)    df_valid = df[df.kfold == fold].reset_index(drop=True)        train_dataset = JigsawDataset(df_train, tokenizer=CONFIG[&#x27;tokenizer&#x27;], max_length=CONFIG[&#x27;max_length&#x27;])    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG[&#x27;tokenizer&#x27;], max_length=CONFIG[&#x27;max_length&#x27;])    train_loader = DataLoader(train_dataset, batch_size=CONFIG[&#x27;train_batch_size&#x27;],                               num_workers=2, shuffle=True, pin_memory=True, drop_last=True)    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[&#x27;valid_batch_size&#x27;],                               num_workers=2, shuffle=False, pin_memory=True)        return train_loader, valid_loade



tqdm1bar = tqdm(enumerate(dataloader), total=len(dataloader))

单个epoch下面对bar做如下设置
12bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,                        LR=optimizer.param_groups[0][&#x27;lr&#x27;])  



Weights &amp; Biases (W&amp;B) 
hash 一个项目id

train valid 定义一个 1个epoch 的函数 返回 分别其中的loss

wandb.log({“Train Loss”: train_epoch_loss}) 使用 log 方式记录 损失函数

run = wandb.init(project=&#39;Jigsaw&#39;, 
                     config=CONFIG,
                     job_type=&#39;Train&#39;,
                     group=CONFIG[&#39;group&#39;],
                     tags=[&#39;roberta-base&#39;, f&#39;&#123;HASH_NAME&#125;&#39;, &#39;margin-loss&#39;],
                     name=f&#39;&#123;HASH_NAME&#125;-fold-&#123;fold&#125;&#39;,
                     anonymous=&#39;must&#39;)
1TRAIN PART

run.finish()

1234显示如下			&#x27;hash--------name&#x27;Syncing run k5nu8k69390a-fold-0 to Weights &amp; Biases (docs).



流程训练提炼
for fold in range(0, CONFIG[‘n_fold’])
wandb.init
prepare_loaders、fetch_scheduler
run_training
train_one_epoch、valid_one_epoch —-&gt; to got model, loss for wandb



中间掺杂 W&amp;B 的数据实时载入分析即可
df[‘y’].value_counts(normalize&#x3D;True) to got the percentage of each values
原文链接
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/45348.html" title="HF 02"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris12.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF 02"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45348.html" title="HF 02">HF 02</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.815Z" title="发表于 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T10:59:33.572Z" title="更新于 2022-12-06 18:59:33">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a></span></div><div class="content">待完成
示例详解

Transformer分两块BERT&amp;GPT都很能打
BERT用的是transformer的encoder

BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。


GPT用的是transformer的decoder

在它之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。



Bert的embeddingEmbedding由三种Embedding求和而成：

Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务

BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。


Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务

Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的


APItokenizer12345678910111213141516171819202122232425262728293031323334from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging# config模块MODEL_NAME=&quot;bert-base-chinese&quot;config = AutoConfig.from_pretrained(MODEL_NAME) #c onfig可以配置模型信息# tokenizer模块tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)tokenizer.all_special_ids # 查看特殊符号的id [100, 102, 0, 101, 103]tokenizer.all_special_tokens # 查看token  [&#x27;[UNK]&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[CLS]&#x27;, &#x27;[MASK]&#x27;]tokenizer.vocab_size # 词汇表大小tokenizer.vocab # 词汇对应的dict形式## tokeningtext=&quot;我在北京工作&quot;token_ids=tokenizer.encode(text)token_ids # [101, 2769, 1762, 1266, 776, 2339, 868, 102]tokenizer.convert_ids_to_tokens(token_ids) # [&#x27;[CLS]&#x27;, &#x27;我&#x27;, &#x27;在&#x27;, &#x27;北&#x27;, &#x27;京&#x27;, &#x27;工&#x27;, &#x27;作&#x27;, &#x27;[SEP]&#x27;]		  # convert_tokens_to_ids(tokens) 为对应方法    ## padding 做向量填充token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)## encode_plustoken_ids=tokenizer.encode_plus(    text,padding=&quot;max_length&quot;,    max_length=30,    add_special_tokens=True,    return_tensors=&#x27;pt&#x27;,    return_token_type_ids=True,    return_attention_mask=True)



使用pre_train模型载入数据12model=AutoModel.from_pretrained(MODEL_NAME)outputs=model(token_ids[&#x27;input_ids&#x27;],token_ids[&#x27;attention_mask&#x27;])



数据集dataset定义12345678910111213141516171819202122232425262728293031323334class EnterpriseDataset(Dataset):    def __init__(self,texts,labels,tokenizer,max_len):        self.texts=texts        self.labels=labels        self.tokenizer=tokenizer        self.max_len=max_len    def __len__(self):        return len(self.texts)        def __getitem__(self,item):        &quot;&quot;&quot;        item 为数据索引，迭代取第item条数据        &quot;&quot;&quot;        text=str(self.texts[item])        label=self.labels[item]                encoding=self.tokenizer.encode_plus(            text,            add_special_tokens=True,            max_length=self.max_len,            return_token_type_ids=True,            pad_to_max_length=True,            return_attention_mask=True,            return_tensors=&#x27;pt&#x27;,  #转为tensor        )        #print(encoding[&#x27;input_ids&#x27;])        return &#123;            &#x27;texts&#x27;:text,            &#x27;input_ids&#x27;:encoding[&#x27;input_ids&#x27;].flatten(),            &#x27;attention_mask&#x27;:encoding[&#x27;attention_mask&#x27;].flatten(),            # toeken_type_ids:0            &#x27;labels&#x27;:torch.tensor(label,dtype=torch.long)        &#125;

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/45347.html" title="HF 01"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris12.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF 01"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/45347.html" title="HF 01">HF 01</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T08:59:54.804Z" title="发表于 2022-11-21 16:59:54">2022-11-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T10:59:15.183Z" title="更新于 2022-12-06 18:59:15">2022-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Bert/">Bert</a></span></div><div class="content">待完成
示例详解


Attention 原文
Why
全面拥抱Transformer：NLP三大特征抽取器(CNN&#x2F;RNN&#x2F;TF)中，近两年新欢Transformer明显会很快成为NLP里担当大任的最主流的特征抽取器。

像Wordvec出现之后一样，在人工智能领域种各种目标皆可向量化，也就是我们经常听到的“万物皆可Embedding”。而Transformer模型和Bert模型的出现，更是NLP领域划时代的产物：将transformer和双向语言模型进行融合，便得到NLP划时代的，也是当下在各自NLP下流任务中获得state-of-the-art的模型-BERT

BERT起源于预训练的上下文表示学习，与之前的模型不同，BERT是一种深度双向的、无监督的语言表示，且仅使用纯文本语料库进行预训练的模型。上下文无关模型（如word2vec或GloVe）为词汇表中的每个单词生成一个词向量表示，因此容易出现单词的歧义问题。BERT考虑到单词出现时的上下文。例如，词“水分”的word2vec词向量在“植物需要吸收水分”和“财务报表里有水分”是相同的，但BERT根据上下文的不同提供不同的词向量，词向量与句子表达的句意有关。


Embedding：

首先类似 word2vec 的 token化，再进行片段标记( segment )，最后 ids 的位置编码(  position )
编码后一个 ’词‘ 有三个信息，token、段落位置信息、绝对位置信息( id: 1、2、3…)

Embedding解决的问题:
首先是之前用的 One-Hot Key，高维度，离散的，低信息密度的储存形式
其次是更好的 Contextual Similarity，上下文相关相似性。

Preview Api前置查看：123456from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained(&quot;bert-base-chinese&quot;) # 获取相应模型的tokenizerfrom transformers import AutoTokenizer, AutoModelForMaskedLMmodel = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-chinese&quot;) #查看模型的分层



函数调用：字典大小，token化，ids化12345678910vocab = tokenizer.vocabprint(&quot;字典大小：&quot;, len(vocab)) 	# 查看字典大小text = &quot;[CLS] 等到潮水 [MASK] 了，就知道谁沒穿裤子。&quot;tokens = tokenizer.tokenize(text)				# 将文字分词ids = tokenizer.convert_tokens_to_ids(tokens)	# 将文字转化为数字，进行编码&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;等&#x27;, &#x27;到&#x27;, &#x27;潮&#x27;, &#x27;水&#x27;, &#x27;[MASK]&#x27;, &#x27;了&#x27;, &#x27;，&#x27;, &#x27;就&#x27;, &#x27;知&#x27;] ...[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761] ... &#x27;&#x27;&#x27;

Mask模型的使用1234567891011121314151617181920212223242526from transformers import BertForMaskedLM# 除了 tokens 以外我們還需要辨別句子的 segment idstokens_tensor = torch.tensor([ids])  # (1, seq_len)segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)# 使用 masked LM 估計 [MASK] 位置所代表的實際 token maskedLM_model.eval()with torch.no_grad():    outputs = maskedLM_model(tokens_tensor, segments_tensors)    predictions = outputs[0]    # (1, seq_len, num_hidden_units)del maskedLM_model# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來masked_index = 5k = 3probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())# 顯示 top k 可能的字。一般我們就是取 top 1 当做预测值print(&quot;輸入 tokens ：&quot;, tokens[:10], &#x27;...&#x27;)print(&#x27;-&#x27; * 50)for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):    tokens[masked_index] = t    print(&quot;Top &#123;&#125; (&#123;:2&#125;%)：&#123;&#125;&quot;.format(i, int(p.item() * 100), tokens[:10]), &#x27;...&#x27;)

​	輸入 tokens ： [‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘[MASK]’, ‘了’, ‘，’, ‘就’, ‘知’] …​	Top 1 (65%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘来’, ‘了’, ‘，’, ‘就’, ‘知’] …​	Top 2 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘过’, ‘了’, ‘，’, ‘就’, ‘知’] …​	Top 3 ( 4%)：[‘[CLS]’, ‘等’, ‘到’, ‘潮’, ‘水’, ‘干’, ‘了’, ‘，’, ‘就’, ‘知’] …
可视化模型: bertvizPandas预处理文本
多使用自定义函数
nltk库的stopwords
textblob库的拼写检查、词干抽取、词性还原等

文本数据的基本体征提取
词汇数量

字符数量

平均字长

停用词数量

特殊字符数量

数字数量

大写字母数量


文本数据的基本预处理
小写转换
去除标点符号
去除停用词
去除频现词
去除稀疏词
拼写校正
分词(tokenization)
词干提取(stemming)
词形还原(lemmatization)

</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/2/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/#content-inner">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/4/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客 <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">20</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">32.1k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-06T12:02:22.496Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">8</span><span class="categoryBar-list-descr">拥有一切 却变成太空</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>