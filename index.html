<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-12 17:34:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/7194.html" title="HF Course 09 Custom Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 09 Custom Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/7194.html" title="HF Course 09 Custom Tokenizer">HF Course 09 Custom Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-12T08:15:06.518Z" title="å‘è¡¨äº 2022-12-12 16:15:06">2022-12-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T09:30:53.736Z" title="æ›´æ–°äº 2022-12-12 17:30:53">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">
ä¸ä¹‹å‰ç»§æ‰¿å¼çš„åˆ†è¯å™¨ä¸åŒï¼Œè¿™è¯æˆ‘ä»¬å°†ä»è¯­æ–™åº“ä¸­è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„åˆ†è¯å™¨
é¦–å…ˆæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªWordPieceç±»å‹çš„åˆ†è¯å™¨

åŠ è½½æ–‡æ¡£12345678910111213from datasets import load_datasetdataset = load_dataset(&quot;wikitext&quot;, name=&quot;wikitext-2-raw-v1&quot;, split=&quot;train&quot;)def get_training_corpus():    for i in range(0, len(dataset), 1000):        yield dataset[i : i + 1000][&quot;text&quot;]# ä¹Ÿå¯ä»¥ä»æœ¬åœ°æ‰“å¼€æ–‡æ¡£with open(&quot;wikitext-2.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:    for i in range(len(dataset)):        f.write(dataset[i][&quot;text&quot;] + &quot;\n&quot;)



åŠ è½½æ„ä»¶1234567891011from tokenizers import (    decoders,    models,    normalizers,    pre_tokenizers,    processors,    trainers,    Tokenizer,)tokenizer = Tokenizer(models.WordPiece(unk_token=&quot;[UNK]&quot;))

æˆ‘ä»¬ä»tokenizeråº“ä¸­åŠ è½½ç‰¹æ®Šçš„modelæ„ä»¶ï¼Œæ¥ä½¿ç”¨WordPieceæ–¹æ³•
è®¾å®šé‡åˆ°æ²¡è§è¿‡çš„è¯æ ‡è®°ä¸º[UNK]ï¼Œ åŒæ—¶å¯ä»¥è®¾ç½®max_input_chars_per_wordä½œä¸ºæœ€å¤§è¯é•¿
è®¾ç½®Normalizerè¿™é‡Œæˆ‘ä»¬é€‰æ‹©bertçš„è®¾ç½®ï¼ŒåŒ…æ‹¬: 
æ‰€æœ‰å­—æ¯å°å†™ã€strip_accentsé™¤å»é‡éŸ³ã€åˆ é™¤æ§åˆ¶å­—ç¬¦ã€å°†æ‰€æœ‰å¤šä¸ªç©ºæ ¼è®¾ç½®ä¸ºå•ä¸ªç©ºæ ¼ã€æ±‰å­—å‘¨å›´æ”¾ç½®ç©ºæ ¼ã€‚
123456789tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)# ä½ ä¹Ÿå¯ä»¥è‡ªå®šä¹‰tokenizer.normalizer = normalizers.Sequence(    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])print(tokenizer.normalizer.normalize_str(&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;))# hello how are u?

ä¸Šé¢è‡ªå®šä¹‰ä¸­æˆ‘ä»¬ä½¿ç”¨Sequenceæ–¹æ³•å®šä¹‰æˆ‘ä»¬è‡ªå·±çš„è§„èŒƒåŒ–è§„åˆ™
Pre-tokenizationå’Œä¸Šé¢ä¸€æ ·å¯ä»¥é€šè¿‡tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()å¥—ç”¨bertçš„è®¾ç½®
ä¸‹é¢æ˜¯customç‰ˆæœ¬
12345tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)), (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;



.Whitespace()æ˜¯å¯¹æ ‡ç‚¹ç©ºæ ¼åˆ†éš”ï¼Œä½ å¯ç”¨ä¸‹é¢çš„åˆ†éš”
1234pre_tokenizer = pre_tokenizers.WhitespaceSplit()pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&quot;Let&#x27;s&quot;, (0, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre-tokenizer.&#x27;, (14, 28))]&#x27;&#x27;&#x27;



æ¨èä½¿ç”¨Sequenceæ–¹æ³•ç»„åˆä½ çš„é¢„åˆ†è¯
1234567pre_tokenizer = pre_tokenizers.Sequence(    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()])pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)), (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;



Trainerè®­ç»ƒä¹‹å‰æˆ‘ä»¬éœ€è¦åŠ å…¥ç‰¹æ®Štokenå› ä¸ºä»–ä¸åœ¨ä½ çš„è¯åº“ä¹‹ä¸­
12special_tokens = [&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)


As well as specifying the vocab_size and special_tokens, we can set the min_frequency (the number of times a token must appear to be included in the vocabulary) or change the continuing_subword_prefix (if we want to use something different from ##).
æ”¹æŸä¸ªtokenå¿…é¡»å‡ºç°å¤šå°‘æ¬¡ã€æ”¹è¿æ¥å‰ç¼€##ä¸ºåˆ«çš„



å¼€å§‹è®­ç»ƒ
12345tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)# å¦å¤–çš„ç‰ˆæœ¬tokenizer.model = models.WordPiece(unk_token=&quot;[UNK]&quot;)tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)

ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ç”¨ä¸Šé¢å®šä¹‰çš„ç”Ÿæˆå™¨
ç¬¬äºŒç§ä¼ å…¥â€wikitext-2.txtâ€æ–‡ä»¶
åˆ°æ­¤æˆ‘ä»¬tokenizerå°±å…·æœ‰äº†ä¸€èˆ¬tokenizerçš„æ‰€æœ‰æ–¹æ³•å¦‚encode
1234encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)# [&#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;]



Post-processingæœ€åæˆ‘ä»¬éœ€è¦åŒ…è£¹æˆ‘ä»¬çš„tokenåˆ°ç‰¹æ®Šçš„æ ¼å¼å¦‚: [CLS]â€¦[SEP]â€¦[SEP]
é¦–å…ˆæˆ‘ä»¬è·å–æ‰€éœ€çš„ç‰¹æ®Štokençš„ä¸‹æ ‡
12345cls_token_id = tokenizer.token_to_id(&quot;[CLS]&quot;)sep_token_id = tokenizer.token_to_id(&quot;[SEP]&quot;)print(cls_token_id, sep_token_id)# (2, 3)



æ¥ä¸‹æ¥å¤„ç†æˆ‘ä»¬çš„æ¨¡æ¿
12345tokenizer.post_processor = processors.TemplateProcessing(    single=f&quot;[CLS]:0 $A:0 [SEP]:0&quot;,    pair=f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;,    special_tokens=[(&quot;[CLS]&quot;, cls_token_id), (&quot;[SEP]&quot;, sep_token_id)],)

æ¨¡æ¿æˆ‘ä»¬éœ€è¦è®¾ç½®ä¸¤ç§æ¨¡å¼:

singleâ€“å•ä¸ªå¥å­æƒ…å†µä¸‹

[0,0,0,0]


pair

[0,0,0,1,1,1]


æœ€åæŒ‡å®šç‰¹æ®Štokençš„id


æŸ¥çœ‹
12345678910encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)# [&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer...&quot;, &quot;on a pair of sentences.&quot;)print(encoding.tokens)print(encoding.type_ids)&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;...&#x27;, &#x27;[SEP]&#x27;, &#x27;on&#x27;, &#x27;a&#x27;, &#x27;pair&#x27;, &#x27;of&#x27;, &#x27;sentences&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]&#x27;&#x27;&#x27;



decoderæ¥ä¸‹æ¥å¯¹è§£ç å™¨åšä¸€å®šè®¾ç½®
1234tokenizer.decoder = decoders.WordPiece(prefix=&quot;##&quot;)tokenizer.decode(encoding.ids)# &quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;



ä¿å­˜ &amp; åŠ è½½ Custom Tokenizertokenizer.save(&quot;tokenizer.json&quot;)
new_tokenizer = Tokenizer.from_file(&quot;tokenizer.json&quot;)
è½¬æˆFast TokenizerTo use this tokenizer in ğŸ¤— Transformers, we have to wrap it in a PreTrainedTokenizerFast. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, BertTokenizerFast). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.

å¯ä»¥ç»§æ‰¿ä½ çš„ç‰¹å®šç±»BertTokenizerFastï¼Œä¹Ÿå¯ä»¥ç”¨æ³›ç±»PreTrainedTokenizerFast

1234567891011from transformers import PreTrainedTokenizerFastwrapped_tokenizer = PreTrainedTokenizerFast(    tokenizer_object=tokenizer,    # tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively    unk_token=&quot;[UNK]&quot;,    pad_token=&quot;[PAD]&quot;,    cls_token=&quot;[CLS]&quot;,    sep_token=&quot;[SEP]&quot;,    mask_token=&quot;[MASK]&quot;,)

è¿™é‡Œå¯ä»¥ä»æ–‡ä»¶ä¸­åŠ è½½ä½ çš„tokenizerè®¾ç½®ã€ä¹Ÿå¯ç›´æ¥èµ‹å€¼ã€æ³¨æ„ä½ çš„ç‰¹æ®Šç¬¦å·å¿…é¡»é‡æ–°å®šä¹‰
BPEç±»å‹çš„åˆ†è¯å™¨1234567tokenizer = Tokenizer(models.BPE())tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test pre-tokenization!&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;s&quot;, (3, 5)), (&#x27;Ä test&#x27;, (5, 10)), (&#x27;Ä pre&#x27;, (10, 14)), (&#x27;-&#x27;, (14, 15)), (&#x27;tokenization&#x27;, (15, 27)), (&#x27;!&#x27;, (27, 28))]&#x27;&#x27;&#x27;



GPT2åªéœ€è¦å¼€å§‹å’Œç»“æŸçš„ç‰¹æ®Štoken
12345678910trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[&quot;&lt;|endoftext|&gt;&quot;])tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)tokenizer.model = models.BPE()tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)&#x27;&#x27;&#x27;[&#x27;L&#x27;, &#x27;et&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;Ä test&#x27;, &#x27;Ä this&#x27;, &#x27;Ä to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;





12345678tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)sentence = &quot;Let&#x27;s test this tokenizer.&quot;encoding = tokenizer.encode(sentence)start, end = encoding.offsets[4]sentence[start:end]# &#x27; test&#x27;

The trim_offsets = False option indicates to the post-processor that we should leave the offsets of tokens that begin with â€˜Ä â€™ as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Letâ€™s have a look at the result with the text we just encoded, where &#39;Ä test&#39; is the token at index 4

trim_offsetsè®¾å®šæ˜¯å¦ä¿®æ­£å­—ç¬¦çš„ç©ºæ ¼ä½ç½®è¿›å…¥åç§»é‡

123tokenizer.decoder = decoders.ByteLevel()tokenizer.decode(encoding.ids)# &quot;Let&#x27;s test this tokenizer.&quot;



åŒ…è£…
123456789101112from transformers import PreTrainedTokenizerFastwrapped_tokenizer = PreTrainedTokenizerFast(    tokenizer_object=tokenizer,    bos_token=&quot;&lt;|endoftext|&gt;&quot;,    eos_token=&quot;&lt;|endoftext|&gt;&quot;,)# æˆ–è€…from transformers import GPT2TokenizerFastwrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)



Unigramç±»å‹çš„åˆ†è¯å™¨12345678910111213tokenizer = Tokenizer(models.Unigram())from tokenizers import Regextokenizer.normalizer = normalizers.Sequence(    [        normalizers.Replace(&quot;``&quot;, &#x27;&quot;&#x27;),        normalizers.Replace(&quot;&#x27;&#x27;&quot;, &#x27;&quot;&#x27;),        normalizers.NFKD(),        normalizers.StripAccents(),        normalizers.Replace(Regex(&quot; &#123;2,&#125;&quot;), &quot; &quot;),    ])

ç¬¬ä¸€ã€äºŒä¸ªnormå°†ç¬¦å·æ›¿æ¢ï¼Œæœ€åä¸€ä¸ªå°†å¤šä¸ªç©ºæ ¼æ›¿æ¢æˆä¸€ä¸ª
1234tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test the pre-tokenizer!&quot;)# [(&quot;â–Let&#x27;s&quot;, (0, 5)), (&#x27;â–test&#x27;, (5, 10)), (&#x27;â–the&#x27;, (10, 14)), (&#x27;â–pre-tokenizer!&#x27;, (14, 29))]



123456789special_tokens = [&quot;&lt;cls&gt;&quot;, &quot;&lt;sep&gt;&quot;, &quot;&lt;unk&gt;&quot;, &quot;&lt;pad&gt;&quot;, &quot;&lt;mask&gt;&quot;, &quot;&lt;s&gt;&quot;,  ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/7193.html" title="HF Course 08 Tokenizeråº•å±‚ç®—æ³•"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 08 Tokenizeråº•å±‚ç®—æ³•"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/7193.html" title="HF Course 08 Tokenizeråº•å±‚ç®—æ³•">HF Course 08 Tokenizeråº•å±‚ç®—æ³•</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-12T03:35:18.892Z" title="å‘è¡¨äº 2022-12-12 11:35:18">2022-12-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T08:16:00.524Z" title="æ›´æ–°äº 2022-12-12 16:16:00">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">æµç¨‹ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬çš„tokenizeræœ‰å¦‚ä¸‹æµç¨‹

è§„èŒƒåŒ–è§„èŒƒåŒ–æ˜¯å¯¹å­—ç¬¦åšå¤§å°å†™å¤„ç†ä¹‹ç±»çš„æˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹APIæŸ¥çœ‹åº•å±‚çš„normalizationæ–¹æ³•
123456789from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)print(type(tokenizer.backend_tokenizer))# &lt;class &#x27;tokenizers.Tokenizer&#x27;&gt;print(tokenizer.backend_tokenizer.normalizer.normalize_str(&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;))# &#x27;hello how are u?&#x27;



é¢„åˆ†è¯é€šè¿‡å¦‚ä¸‹apiæŸ¥çœ‹åˆ†è¯å™¨æ˜¯å¦‚ä½•åšpre_tokenizeçš„
123tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)&#x27;&#x27;&#x27;&#x27;[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;how&#x27;, (7, 10)), (&#x27;are&#x27;, (11, 14)), (&#x27;you&#x27;, (16, 19)), (&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;

å¯ä»¥çœ‹åˆ°åé¢çš„åç§»é‡åæ ‡ï¼Œè¿™ä¹Ÿæ˜¯ä¸Šä¸€èŠ‚offset-mappingçš„ç”±æ¥
ä¸åŒçš„é¢„åˆ†è¯gpt
1234tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)&#x27;&#x27;&#x27;[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;Ä how&#x27;, (6, 10)), (&#x27;Ä are&#x27;, (10, 14)), (&#x27;Ä &#x27;, (14, 15)), (&#x27;Ä you&#x27;, (15, 19)),(&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;&#x27;



t5
123tokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)# [(&#x27;â–Hello,&#x27;, (0, 6)), (&#x27;â–how&#x27;, (7, 10)), (&#x27;â–are&#x27;, (11, 14)), (&#x27;â–you?&#x27;, (16, 20))]



ä¸‰ç§åˆ†è¯ç®—æ³•æ€»è§ˆå¦‚ä¸Šï¼Œä¸åŒçš„æ¨¡å‹é€‚ç”¨ä¸åŒçš„åˆ†è¯ç®—æ³•
sentencepieceå®ƒç»å¸¸ä¸unigramç®—æ³•ä¸€èµ·ï¼Œä¸”å¹¶ä¸éœ€è¦é¢„åˆ†è¯ï¼Œæ˜¯ç‰¹æ”»ä¸­æ–‡æ—¥æ–‡ï¼Œè¿™ç§æ— æ³•åˆ†è¯çš„è¯­è¨€çš„
ç®—æ³•é¢„è§ˆ


Model
BPE
WordPiece
Unigram



Training
Starts from a small vocabulary and learns rules to merge tokens
Starts from a small vocabulary and learns rules to merge tokens
Starts from a large vocabulary and learns rules to remove tokens


Training step
Merges the tokens corresponding to the most common pair
Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent
Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus


Learns
Merge rules and a vocabulary
Just a vocabulary
A vocabulary with a score for each token


Encoding
Splits a word into characters and applies the merges learned during training
Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word
Finds the most likely split into tokens, using the scores learned during training


BPEç®€è¿°BPEæ˜¯Byte-Pair Encoding çš„ç®€å†™ï¼Œä»–æœ‰ä¸‰æ­¥

å°†corpusæ‰€æœ‰ç‹¬ä¸€æ— äºŒå­—ç¬¦æ‹†å‡ºæ¥ï¼Œå¦‚è‹±æ–‡ä¸­çš„26ä¸ªå­—æ¯ï¼Œæ ‡ç‚¹å’Œå…¶ä»–ç‰¹æ®Šç¬¦å·
åœ¨æœ‰åŸºç¡€å­—ç¬¦çš„åŸºç¡€ä¸Šï¼Œä»¥é¢‘ç‡ä½œä¸ºé€‰å–æ ‡å‡†ï¼Œå°†ä¸¤ä¸ªå­—ç¬¦åŒ¹é…åœ¨ä¸€èµ·ï¼Œé€‰æ‹©é¢‘ç‡æœ€é«˜çš„è¯è¿›è¡Œå…¥åº“
é‡å¤ç¬¬äºŒæ­¥ç›´åˆ°æ»¡è¶³ä½ è®¾å®šçš„è¯åº“å¤§å°


The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they donâ€™t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.
GPTå’Œrobertaä½¿ç”¨çš„æ˜¯æ¯”ç‰¹çº§åˆ«çš„å­—ç¬¦ï¼Œå°±æ˜¯0100è¿™ç§ï¼Œè¿™å°±æ˜¯ä»–ä»¬çš„åŸºç¡€è¯­æ–™åº“ï¼Œç„¶ååœ¨åŸºç¡€ä¸Šèåˆå‡ºæ¥è¯è¿›è¡Œæ„å»ºè¯åº“

å®ä¾‹ä¸‹é¢è¿›è¡Œå®ä¾‹è§£æè®¾å®šè¯­æ–™åº“å¦‚ä¸‹
è¯­æ–™åº“: &quot;hug&quot;, &quot;pug&quot;, &quot;pun&quot;, &quot;bun&quot;, &quot;hugs&quot;
è¯é¢‘: (&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
1# (&quot;h&quot; &quot;u&quot; &quot;g&quot;, 10), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, 5)



ç¬¬ä¸€è½®
æœ€å¤šçš„æ˜¯ ugçš„ç»„åˆï¼Œ20æ¬¡
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;]Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;

ç¬¬äºŒè½®
æœ€å¤šæ˜¯un
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;]Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;

ç¬¬ä¸‰è½®
æœ€å¤šçš„æ˜¯hug
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;

â€¦å¦‚ä½•å¾ªç¯åˆ°è®¾å®šçš„è¯åº“å¤§å°
ç®€è¦ä»£ç è¯­æ–™åº“
123456corpus = [    &quot;This is the Hugging Face Course.&quot;,    &quot;This chapter is about tokenization.&quot;,    &quot;This section shows several tokenizer algorithms.&quot;,    &quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;,]



ç»Ÿè®¡è¯é¢‘
12345678910111213141516171819from transformers import AutoTokenizerfrom collections import defaultdicttokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)word_freqs = defaultdict(int)for text in corpus:    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)    new_words = [word for word, offset in words_with_offsets]    for word in new_words:        word_freqs[word] += 1print(word_freqs)&#x27;&#x27;&#x27;defaultdict(int, &#123;&#x27;This&#x27;: 3, &#x27;Ä is&#x27;: 2, &#x27;Ä the&#x27;: 1, &#x27;Ä Hugging&#x27;: 1, &#x27;Ä Face&#x27;: 1, &#x27;Ä Course&#x27;: 1, &#x27;.&#x27;: 4, &#x27;Ä chapter&#x27;: 1,    &#x27;Ä about&#x27;: 1, &#x27;Ä tokenization&#x27;: 1, &#x27;Ä section&#x27;: 1, &#x27;Ä shows&#x27;: 1, &#x27;Ä several&#x27;: 1, &#x27;Ä tokenizer&#x27;: 1, &#x27;Ä algorithms&#x27;: 1,    &#x27;Hopefully&#x27;: 1, &#x27;,&#x27;: 1, &#x27;Ä you&#x27;: 1, &#x27;Ä will&#x27;: 1, &#x27;Ä be&#x27;: 1, &#x27;Ä able&#x27;: 1, &#x27;Ä to&#x27;: 1, &#x27;Ä understand&#x27;: 1, &#x27;Ä how&#x27;: 1,    &#x27;Ä they&#x27;: 1, &#x27;Ä are&#x27;: 1, &#x27;Ä trained&#x27;: 1, &#x27;Ä and&#x27;: 1, &#x27;Ä generate&#x27;: 1, &#x27;Ä tokens&#x27;: 1&#125;)&#x27;&#x27;&#x27;


é¦–å…ˆè½½å…¥gptçš„åˆ†è¯å™¨ï¼Œåšé¢„åˆ†è¯
å†è½½å…¥collectionä¸­çš„defaultdictè®¾å®šä¸ºintç±»å‹

åŸºç¡€è¯æ±‡è¡¨
123456789101112alphabet = []for word in word_freqs.keys():    for letter in word:        if letter not in alphabet:            alphabet.append(letter)alphabet.sort()print(alphabet)&#x27;&#x27;&#x27;[ &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;r&#x27;, &#x27;s&#x27;,  &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;Ä &#x27;]&#x27;&#x27;&#x27;

åŠ ä¸ªè¡¨å¤´vocab = [&quot;&lt;|endoftext|&gt;&quot;] + alphabet.copy()
å°†å•è¯æ˜ å°„ä¸º{â€˜wordâ€™: [â€˜wâ€™, â€˜oâ€™, â€˜râ€™, â€˜dâ€™]}çš„å½¢å¼è¿›è¡Œè®­ç»ƒ
1234splits = &#123;word: [c for c in word] for word in word_freqs.keys()&#125;# æˆ‘è§‰å¯ä»¥æ”¹ä¸€ä¸‹`splits = &#123;word: list(word) for word in word_freqs.keys()&#125;`



å­—æ¯å¯¹é¢‘ç‡å‡½æ•°
1234567891011121314151617181920212223242526272829303132333435363738394041def compute_pair_freqs(splits):    pair_freqs = defaultdict(int)    for word, freq in word_freqs.items():        split = splits[word]	# å–å¾—wordå¯¹åº”çš„å€¼å¦‚[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]        if len(split) == 1:            continue        for i in range(len(split) - 1):            pair = (split[i], split[i + 1])            pair_freqs[pair] += freq	# è®°å½•å­—æ¯å¯¹çš„é¢‘ç‡    return pair_freqs# ç¤ºä¾‹pair_freqs = compute_pair_freqs(splits)for i, key in enumerate(pair_freqs.keys()):    print(f&quot;&#123;key&#125;: &#123;pair_freqs[key]&#125;&quot;)    if i &gt;= 5:        break&#x27;&#x27;&#x27;(&#x27;T&#x27;, &#x27;h&#x27;): 3(&#x27;h&#x27;, &#x27;i&#x27;): 3(&#x27;i&#x27;, &#x27;s&#x27;): 5(&#x27;Ä &#x27;, &#x27;i&#x27;): 2(&#x27;Ä &#x27;, &#x27;t&#x27;): 7(&#x27;t&#x27;, &#x27;h&#x27;): 3&#x27;&#x27;&#x27;# å–æœ€å¤§å€¼best_pair = &quot;&quot;max_freq = Nonefor pair, freq in pair_freqs.items():    if max_freq is None or max_freq &lt; freq:        best_pair = pair        max_freq = freqprint(best_pair, max_freq)# (&#x27;Ä &#x27;, &#x27;t&#x27;) 7# åˆå¹¶å…¥åº“merges = &#123;(&quot;Ä &quot;, &quot;t&quot;): &quot;Ä t&quot;&#125;vocab.append(&quot;Ä t&quot;)



å°†å­—ç¬¦å¯¹æ„å»ºè¿›æ–°çš„åŸºç¡€è¯è¡¨split (ä¸æ˜¯vocab)
12345678910111213141516171819def merge_pair(a, b, splits):    for word in word_freqs:        split = splits[word] # å–å¾—wordå¯¹åº”çš„å€¼å¦‚[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]        if len(split) == 1:            continue        i = 0        while i &lt; len(split) - 1:            if split[i] == a and split[i + 1] == b:             	# æ‰¾åˆ°è¯å¯¹çš„ä½ç½®ï¼Œå°†abå­—ç¬¦ä¸²è¿æ¥èµ·æ¥ï¼Œåšä¸ªåˆ—è¡¨å­˜èµ·æ¥                split = split[:i] + [a + b] + split[i + 2 :]            else:                i += 1         splits[word] = split # æ›´æ–° [&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;] -&gt; [&#x27;wo&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]    return splitssplits = merge_pair(&quot;Ä &quot;, &quot;t&quot;, splits)print(splits[&quot;Ä trained&quot;])# [&#x27;Ä t&#x27;, &#x27;r&#x27;, &#x27;a&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]



æ„å»ºå¾ªç¯
1234567891011121314151617181920vocab_size = 50while len(vocab) &lt; v ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/37742.html" title="HF Course 07 NER QA Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 07 NER QA Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/37742.html" title="HF Course 07 NER QA Tokenizer">HF Course 07 NER QA Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T14:53:10.235Z" title="å‘è¡¨äº 2022-12-11 22:53:10">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T04:09:35.967Z" title="æ›´æ–°äº 2022-12-12 12:09:35">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">è®°å¾—æ’ç‰ˆ åˆ†å‰²çº¿å¾…å®Œæˆ
QAéƒ¨åˆ†


æœ¬ç« æˆ‘ä»¬éœ€è¦å¯¹åšç‰¹æ®Šçš„tokenizerä»¥é€‚åº”NERå’ŒQAä»»åŠ¡æ•°æ®çš„ç‰¹æ®Šæ€§

Fast Tokenizer1234567from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)example = &quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;encoding = tokenizer(example)print(type(encoding))# &lt;class &#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;&gt;

åˆ†è¯åè¿”å›çš„ç»“æœç±»å‹ä¸ç®€å•æ˜¯å­—å…¸çš„æ˜ å°„
è¿˜åŒ…å«å¾ˆå¤šæ–¹æ³•
123456789101112tokenizer.is_fast, encoding.is_fast(True,True)encoding.tokens()&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;, &#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]&#x27;&#x27;&#x27; encoding.word_ids()&#x27;&#x27;&#x27;[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]&#x27;&#x27;&#x27;

word_ids()æ–¹æ³•å¯çœ‹åˆ°åˆ†è¯çš„ç»“æœæ¥è‡ªå“ªä¸ªå•è¯
æœ€åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨word_to_chars() or token_to_chars() and char_to_word() or char_to_token() æŸ¥çœ‹å•è¯
123start, end = encoding.word_to_chars(3)example[start:end]# Sylvain



NER
åœ¨NERä¸­æˆ‘ä»¬ä»¥åç§»é‡çš„æ ‡è®°æ¥é”å®šåŸæ–‡çš„å­—ç¬¦

pipelineæ–¹æ³•é¦–å…ˆæŸ¥çœ‹pipelineæ–¹æ³•çš„neræµç¨‹
12345678910111213from transformers import pipelinetoken_classifier = pipeline(&quot;token-classification&quot;)token_classifier(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 35&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;, &#x27;start&#x27;: 35, &#x27;end&#x27;: 40&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;, &#x27;start&#x27;: 41, &#x27;end&#x27;: 45&#125;, &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;



ç®€æ´ç‰ˆ
12345678from transformers import pipelinetoken_classifier = pipeline(&quot;token-classification&quot;, aggregation_strategy=&quot;simple&quot;)token_classifier(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)&#x27;&#x27;&#x27;[&#123;&#x27;entity_group&#x27;: &#x27;PER&#x27;, &#x27;score&#x27;: 0.9981694, &#x27;word&#x27;: &#x27;Sylvain&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity_group&#x27;: &#x27;ORG&#x27;, &#x27;score&#x27;: 0.97960204, &#x27;word&#x27;: &#x27;Hugging Face&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 45&#125;, &#123;&#x27;entity_group&#x27;: &#x27;LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;

aggregation_strategyæœ‰ä¸åŒçš„å‚æ•°ï¼Œsimpleæ˜¯åˆ†è¯åçš„å¹³å‡åˆ†æ•°
å¦‚ä¸Šé¢çš„sylvainåˆ†æ•°æ¥è‡ª æ­£å¸¸ç‰ˆçš„å››é¡¹å¹³å‡&#39;S&#39;, &#39;##yl&#39;, &#39;##va&#39;, &#39;##in&#39;

&quot;first&quot;, where the score of each entity is the score of the first token of that entity (so for â€œSylvainâ€ it would be 0.993828, the score of the token S)
&quot;max&quot;, where the score of each entity is the maximum score of the tokens in that entity (so for â€œHugging Faceâ€ it would be 0.98879766, the score of â€œFaceâ€)
&quot;average&quot;, where the score of each entity is the average of the scores of the words composing that entity (so for â€œSylvainâ€ there would be no difference from the &quot;simple&quot; strategy, but â€œHugging Faceâ€ would have a score of 0.9819, the average of the scores for â€œHuggingâ€, 0.975, and â€œFaceâ€, 0.98879)

logitsè¿™é‡Œé€šè¿‡è¿”å›çš„ç»“æœä½¿ç”¨argmax(-1)å¾—åˆ°æ˜ å°„çš„åˆ†ç±»
123456789from transformers import AutoTokenizer, AutoModelForTokenClassificationmodel_checkpoint = &quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)example = &quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;inputs = tokenizer(example, return_tensors=&quot;pt&quot;)outputs = model(**inputs)



12345print(inputs[&quot;input_ids&quot;].shape)print(outputs.logits.shape)&#x27;&#x27;&#x27;torch.Size([1, 19])torch.Size([1, 19, 9])&#x27;&#x27;&#x27;



123456789101112131415161718import torchprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()predictions = outputs.logits.argmax(dim=-1)[0].tolist()print(predictions)# [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]model.config.id2label&#x27;&#x27;&#x27;&#123;0: &#x27;O&#x27;, 1: &#x27;B-MISC&#x27;, 2: &#x27;I-MISC&#x27;, 3: &#x27;B-PER&#x27;, 4: &#x27;I-PER&#x27;, 5: &#x27;B-ORG&#x27;, 6: &#x27;I-ORG&#x27;, 7: &#x27;B-LOC&#x27;, 8: &#x27;I-LOC&#x27;&#125;&#x27;&#x27;&#x27;



åç§»é‡postprocessingç»„ç»‡ä¸€ä¸‹æ ¼å¼ï¼Œå¤ç°ä¸Šé¢çš„å†…å®¹
123456789101112131415161718192021results = []tokens = inputs.tokens()for idx, pred in enumerate(predictions):    label = model.config.id2label[pred]    if label != &quot;O&quot;:        results.append(            &#123;&quot;entity&quot;: label, &quot;score&quot;: probabilities[idx][pred], &quot;word&quot;: tokens[idx]&#125;        )print(results)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;&#125;]&#x27;&#x27;&#x27;



åç§»é‡ offsets_mapping
123456inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)inputs_with_offsets[&quot;offset_mapping&quot;]&#x27;&#x27;&#x27;[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]&#x27;&#x27;&#x27;

è¿™é‡Œçš„19å¯¹å…ƒç»„å°±æ˜¯å¯¹åº”19ä¸ªåˆ†è¯åtokençš„ä¸‹æ ‡
1[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;,&#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]

æ¯”å¦‚(0,0)æ˜¯ç•™ç»™[CLS]çš„ï¼›	æ¯”å¦‚ç¬¬å…­ä¸ªtokenå¯¹åº”çš„æ˜¯ ##ly  é‚£ä¹ˆä»–åœ¨åŸæ–‡ä¸­çš„æ ‡æ³¨å°±æ˜¯ï¼ˆ12,14ï¼‰ï¼Œå¦‚ä¸‹
12example[12:14]# yl



ç»§ç»­æˆ‘ä»¬çš„å¤ç°pipeline
123456789101112131415161718192021222324252627282930results = []inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)tokens = inputs_with_offsets.tokens()offsets = inputs_with_offsets[&quot;offset_mapping&quot;]for idx, pred in enumerate(predictions):    label = model.config.id2label[pred]    if label != &quot;O&quot;:        start, end = offsets[idx]        results.append(            &#123;                &quot;entity&quot;: label,                &quot;score&quot;: probabilities[idx][pred],                &quot;word&quot;: tokens[idx],                &quot;start&quot;: start,                &quot;end&quot;: end,            &#125;        )print(results)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity&#x27;: &#x ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/55913.html" title="HF Course 06 ç»§æ‰¿å¼çš„Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 06 ç»§æ‰¿å¼çš„Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/55913.html" title="HF Course 06 ç»§æ‰¿å¼çš„Tokenizer">HF Course 06 ç»§æ‰¿å¼çš„Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T13:27:56.158Z" title="å‘è¡¨äº 2022-12-11 21:27:56">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T04:09:40.352Z" title="æ›´æ–°äº 2022-12-12 12:09:40">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">è¿™ç§æ–¹æ³•æ˜¯åŸºäºæ—§çš„æ¨¡å‹åˆ†è¯å™¨ä¸Šï¼Œé’ˆå¯¹ä½ çš„è¯­æ–™åº“è®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†è¯å™¨çš„æ–¹æ³•ã€‚å¤ºèˆå±äºæ˜¯
è¿™é‡Œæˆ‘ä»¬ä»¥GPTçš„åˆ†è¯å™¨ä¸ºä¾‹ï¼Œå®ƒä½¿ç”¨unigramçš„ç®—æ³•è¿›è¡Œåˆ†è¯
è½½å…¥æ•°æ®12345678910111213from datasets import load_dataset# This can take a few minutes to load, so grab a coffee or tea while you wait!raw_datasets = load_dataset(&quot;code_search_net&quot;, &quot;python&quot;)raw_datasets[&quot;train&quot;]&#x27;&#x27;&#x27;Dataset(&#123;    features: [&#x27;repository_name&#x27;, &#x27;func_path_in_repository&#x27;, &#x27;func_name&#x27;, &#x27;whole_func_string&#x27;, &#x27;language&#x27;,       &#x27;func_code_string&#x27;, &#x27;func_code_tokens&#x27;, &#x27;func_documentation_string&#x27;, &#x27;func_documentation_tokens&#x27;, &#x27;split_name&#x27;,       &#x27;func_code_url&#x27;    ],    num_rows: 412178&#125;)&#x27;&#x27;&#x27;



ç”Ÿæˆå™¨åŠ è½½æ•°æ®ä¸‹é¢çš„æ–¹æ³•ä¼šä¸€æ¬¡åŠ è½½æ‰€æœ‰æ•°æ®
12# Don&#x27;t uncomment the following line unless your dataset is small!# training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]



ä¸€èˆ¬ä½¿ç”¨pythonç”Ÿæˆå™¨
1234training_corpus = (    raw_datasets[&quot;train&quot;][i : i + 1000][&quot;whole_func_string&quot;]    for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000))

å°†åˆ—è¡¨æ¨å¯¼å¼çš„æ–¹æ‹¬å·æ¢æˆåœ†æ‹¬å·å°±å¯ä»¥å˜æˆç”Ÿæˆå™¨äº†ï¼Œå¥½å‰å®³ã€‚

123456&gt;gen = (i for i in range(10))&gt;print(list(gen))&gt;print(list(gen))&gt;&#x27;&#x27;&#x27;&gt;[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;[]&#x27;&#x27;&#x27;

ä½¿ç”¨ä¹‹åä¼šæ¸…ç©ºå†…å­˜ï¼Œå¦‚ä¸Šæ‰€ç¤º

æ›´ä¸€èˆ¬çš„ç”Ÿæˆå™¨
12345def get_training_corpus():    dataset = raw_datasets[&quot;train&quot;]    for start_idx in range(0, len(dataset), 1000):        samples = dataset[start_idx : start_idx + 1000]        yield samples[&quot;whole_func_string&quot;]



train_new_from_iterator()è½½å…¥æ¨¡å‹1234567891011121314from transformers import AutoTokenizerold_tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)example = &#x27;&#x27;&#x27;def add_numbers(a, b):    &quot;&quot;&quot;Add the two numbers `a` and `b`.&quot;&quot;&quot;    return a + b&#x27;&#x27;&#x27;tokens = old_tokenizer.tokenize(example)tokens&#x27;&#x27;&#x27;[&#x27;def&#x27;, &#x27;Ä add&#x27;, &#x27;_&#x27;, &#x27;n&#x27;, &#x27;umbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ä b&#x27;, &#x27;):&#x27;, &#x27;ÄŠ&#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä &quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ä the&#x27;, &#x27;Ä two&#x27;, &#x27;Ä numbers&#x27;, &#x27;Ä `&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ä and&#x27;, &#x27;Ä `&#x27;, &#x27;b&#x27;, &#x27;`&#x27;, &#x27;.&quot;&#x27;, &#x27;&quot;&quot;&#x27;, &#x27;ÄŠ&#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä return&#x27;, &#x27;Ä a&#x27;, &#x27;Ä +&#x27;, &#x27;Ä b&#x27;]&#x27;&#x27;&#x27;

This tokenizer has a few special symbols, like Ä  and ÄŠ, which denote spaces and newlines, respectivelyã€‚
ä¸¤ä¸ªGè¡¨ç¤ºç©ºæ ¼å’Œæ¢è¡Œç¬¦ã€‚ä»–è¿˜ä¸ºå¤šä¸ªç©ºæ ¼åœ¨ä¸€èµ·çš„å•ç‹¬ç¼–ç ï¼Œå¸¦ä¸‹åˆ’çº¿çš„è¯ä¹Ÿä¸è®¤è¯†ï¼Œæ‰€ä»¥ä¸å¤ªåˆé€‚ã€‚
è®­ç»ƒæ–°åˆ†è¯å™¨1tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)


æ³¨æ„ï¼Œåªæœ‰Fastçš„tokenizeræ”¯æŒtrain_new_from_iteratoræ–¹æ³•ï¼Œä»–ä»¬æ˜¯æ ¹æ®rustå†™çš„ã€‚æ²¡æœ‰fastçš„æ˜¯çº¯pythonå†™çš„ã€‚

12345tokens = tokenizer.tokenize(example)tokens&#x27;&#x27;&#x27;[&#x27;def&#x27;, &#x27;Ä add&#x27;, &#x27;_&#x27;, &#x27;numbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ä b&#x27;, &#x27;):&#x27;, &#x27;ÄŠÄ Ä Ä &#x27;, &#x27;Ä &quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ä the&#x27;, &#x27;Ä two&#x27;, &#x27;Ä numbers&#x27;, &#x27;Ä `&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ä and&#x27;, &#x27;Ä `&#x27;, &#x27;b&#x27;, &#x27;`.&quot;&quot;&quot;&#x27;, &#x27;ÄŠÄ Ä Ä &#x27;, &#x27;Ä return&#x27;, &#x27;Ä a&#x27;, &#x27;Ä +&#x27;, &#x27;Ä b&#x27;]&#x27;&#x27;&#x27;

èµ·ç å¤šä¸ªç©ºæ ¼å­¦ä¼šäº†
å­˜å‚¨æ–°åˆ†è¯å™¨1tokenizer.save_pretrained(&quot;code-search-net-tokenizer&quot;)

</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/5596.html" title="HF Course 05 faiss æœç´¢å¼•æ“"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 05 faiss æœç´¢å¼•æ“"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/5596.html" title="HF Course 05 faiss æœç´¢å¼•æ“">HF Course 05 faiss æœç´¢å¼•æ“</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T09:15:41.640Z" title="å‘è¡¨äº 2022-12-11 17:15:41">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T04:01:25.516Z" title="æ›´æ–°äº 2022-12-12 12:01:25">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">åœ¨æˆ‘ä»¬åˆ›å»ºå¥½è‡ªå·±çš„æ•°æ®é›†åï¼Œå¯ä»¥ç”¨faiss å’Œ hf æ¥æœç´¢ä¸€äº›æ•°æ®ã€‚
æˆ‘ä»¬é€šè¿‡multi-qa-mpnet-base-dot-v1æ¨¡å‹embeddingæˆ‘ä»¬çš„æ•°æ®ï¼Œç„¶åé€šè¿‡ faissç»™æ¯ä¸ªembeddingå¾—åˆ°index
æœ€åå°†æˆ‘ä»¬çš„query ç»™tokenizerè½¬æ¢ä¹‹åå–‚ç»™æ¨¡å‹ï¼Œå¾—åˆ°æœ€åŒ¹é…æˆ‘ä»¬é—®é¢˜çš„æ•°æ®ã€‚

Fortunately, thereâ€™s a library called sentence-transformers that is dedicated to creating embeddings. As described in the libraryâ€™s documentation, our use case is an example of asymmetric semantic search because we have a short query whose answer weâ€™d like to find in a longer document, like a an issue comment. The handy model overview table in the documentation indicates that the multi-qa-mpnet-base-dot-v1 checkpoint has the best performance for semantic search, so weâ€™ll use that for our application.
æˆ‘ä»¬ä¸»è¦ä½¿ç”¨äº†sentence-transformers faissä¸¤ä¸ªé¢å¤–åº“å¤„ç†

åŠ è½½æ¨¡å‹12345from transformers import AutoTokenizer, AutoModelmodel_ckpt = &quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;tokenizer = AutoTokenizer.from_pretrained(model_ckpt)model = AutoModel.from_pretrained(model_ckpt)



æ•°æ®å¤„ç†123456789101112131415import torchdevice = torch.device(&quot;cuda&quot;)model.to(device)def cls_pooling(model_output):    return model_output.last_hidden_state[:, 0]    def get_embeddings(text_list):    encoded_input = tokenizer(        text_list, padding=True, truncation=True, return_tensors=&quot;pt&quot;    )    encoded_input = &#123;k: v.to(device) for k, v in encoded_input.items()&#125;    model_output = model(**encoded_input)    return cls_pooling(model_output)



åŠ å…¥ faiss çš„index12345embeddings_dataset = comments_dataset.map(    lambda x: &#123;&quot;embeddings&quot;: get_embeddings(x[&quot;text&quot;]).detach().cpu().numpy()[0]&#125;)embeddings_dataset.add_faiss_index(column=&quot;embeddings&quot;)



æµ‹è¯•1234question = &quot;How can I load a dataset offline?&quot;question_embedding = get_embeddings([question]).cpu().detach().numpy()question_embedding.shape# torch.Size([1, 768])



123scores, samples = embeddings_dataset.get_nearest_examples(    &quot;embeddings&quot;, question_embedding, k=5)



æŸ¥çœ‹ç»“æœ12345import pandas as pdsamples_df = pd.DataFrame.from_dict(samples)samples_df[&quot;scores&quot;] = scoressamples_df.sort_values(&quot;scores&quot;, ascending=False, inplace=True)



1234567for _, row in samples_df.iterrows():    print(f&quot;COMMENT: &#123;row.comments&#125;&quot;)    print(f&quot;SCORE: &#123;row.scores&#125;&quot;)    print(f&quot;TITLE: &#123;row.title&#125;&quot;)    print(f&quot;URL: &#123;row.html_url&#125;&quot;)    print(&quot;=&quot; * 50)    print()



å¯ä»¥æŸ¥çœ‹æœ€åŒ¹é…çš„è¯„è®º
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;&quot;&quot;COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how `transformers` loads models offline fine.@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?SCORE: 25.505046844482422TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)You can now use them offline\`\`\`pythondatasets = load_dataset(&quot;text&quot;, data_files=data_files)\`\`\`We&#x27;ll do a new release soonSCORE: 24.555509567260742TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.----------&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\`\`\`pythonload_dataset(&quot;./my_dataset&quot;)\`\`\`and the dataset script will generate your dataset once and for all.----------About I&#x27;m looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.cf #1724SCORE: 24.14896583557129TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine&gt;&gt; 1. (online machine)&gt;

import datasets
data &#x3D; datasets.load_dataset(â€¦)
data.save_to_disk(&#x2F;YOUR&#x2F;DATASET&#x2F;DIR)
123452. copy the dir from online to the offline machine3. (offline machine)

import datasets
data &#x3D; datasets.load_from_disk(&#x2F;SAVED&#x2F;DATA&#x2F;DIR)
12345678910111213141516171819202122232425262728293031HTH.SCORE: 22.893993377685547TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: here is my way to load a dataset offline, but it **requires** an online machine1. (online machine)\`\`\`import datasetsdata = datasets.load_dataset(...)data.save_to_disk(/YOUR/DATASET/DIR)\`\`\`2. copy the dir from online to the offline machine3. (offline machine)\`\`\`import datasetsdata = datasets.load_from_disk(/SAVED/DATA/DIR)\`\`\`HTH.SCORE: 22.406635284423828TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================&quot;&quot;&quot;

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/4498.html" title="HF Course 04 Dataset"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 04 Dataset"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/4498.html" title="HF Course 04 Dataset">HF Course 04 Dataset</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T07:18:25.982Z" title="å‘è¡¨äº 2022-12-11 15:18:25">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T04:01:21.013Z" title="æ›´æ–°äº 2022-12-12 12:01:21">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">åŠ è½½æœ¬åœ°æ•°æ®


Data format
Loading script
Example



CSV &amp; TSV
csv
load_dataset(&quot;csv&quot;, data_files=&quot;my_file.csv&quot;)


Text files
text
load_dataset(&quot;text&quot;, data_files=&quot;my_file.txt&quot;)


JSON &amp; JSON Lines
json
load_dataset(&quot;json&quot;, data_files=&quot;my_file.jsonl&quot;)


Pickled DataFrames
pandas
load_dataset(&quot;pandas&quot;, data_files=&quot;my_dataframe.pkl&quot;)


åˆ†åˆ«éœ€è¦åšï¼ŒæŒ‡æ˜æ•°æ®ç±»å‹ï¼ŒæŒ‡æ˜æ–‡ä»¶è·¯å¾„
data_fileså‚æ•°The data_files argument of the load_dataset() function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting data_files=&quot;*.json&quot;). See the ğŸ¤— Datasets documentation for more details.

å¯ä»¥åšæ–‡ä»¶è·¯å¾„

å¯ä»¥åšsplitå°†æ•°æ®æ˜ å°„æˆæƒ³è¦çš„å­—å…¸æ ¼å¼

&#96;&#96;&#96;data_files &#x3D; {â€œtrainâ€: â€œSQuAD_it-train.jsonâ€, â€œtestâ€: â€œSQuAD_it-test.jsonâ€}squad_it_dataset &#x3D; load_dataset(â€œjsonâ€, data_files&#x3D;data_files, field&#x3D;â€dataâ€)squad_it_dataset
â€˜â€™â€™DatasetDict({train: Dataset({    features: [â€˜titleâ€™, â€˜paragraphsâ€™],    num_rows: 442})test: Dataset({    features: [â€˜titleâ€™, â€˜paragraphsâ€™],    num_rows: 48})})â€™â€™â€™
12345    ## åŠ è½½æœåŠ¡å™¨æ•°æ®



url &#x3D; â€œhttps://github.com/crux82/squad-it/raw/master/&quot;data_files &#x3D; {    â€œtrainâ€: url + â€œSQuAD_it-train.json.gzâ€,    â€œtestâ€: url + â€œSQuAD_it-test.json.gzâ€,}squad_it_dataset &#x3D; load_dataset(â€œjsonâ€, data_files&#x3D;data_files, field&#x3D;â€dataâ€)
1234567891011# æ•°æ®å¤„ç†## åˆ†éš”ç¬¦å¦‚æœä½ çš„æ•°æ®ä¸æ˜¯ä¼ ç»Ÿçš„CSVæ ¼å¼(ä»¥é€—å·åˆ†å‰²)ï¼Œä½ å¯ä»¥æŒ‡å®šåˆ†éš”ç¬¦
from datasets import load_dataset
data_files &#x3D; {â€œtrainâ€: â€œdrugsComTrain_raw.tsvâ€, â€œtestâ€: â€œdrugsComTest_raw.tsvâ€}
\t is the tab character in Pythondrug_dataset &#x3D; load_dataset(â€œcsvâ€, data_files&#x3D;data_files, delimiter&#x3D;â€\tâ€)
12345## éšæœºé€‰å–æ ·æœ¬
drug_sample &#x3D; drug_dataset[â€œtrainâ€].shuffle(seed&#x3D;42).select(range(1000))
Peek at the first few examplesdrug_sample[:3]
â€˜â€™â€™{â€˜Unnamed: 0â€™: [87571, 178045, 80482], â€˜drugNameâ€™: [â€˜Naproxenâ€™, â€˜Duloxetineâ€™, â€˜Mobicâ€™], â€˜conditionâ€™: [â€˜Gout, Acuteâ€™, â€˜ibromyalgiaâ€™, â€˜Inflammatory Conditionsâ€™], â€˜reviewâ€™: [â€˜â€œlike the previous person mention, I&amp;#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refillsâ€¦..Aleve works!â€â€˜,  â€˜â€œI have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.â€â€˜,  â€˜â€œI have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.â€â€˜], â€˜ratingâ€™: [9.0, 3.0, 10.0], â€˜dateâ€™: [â€˜September 2, 2015â€™, â€˜November 7, 2011â€™, â€˜June 5, 2013â€™], â€˜usefulCountâ€™: [36, 13, 128]}â€™â€™â€™
12345## é‡å‘½å
drug_dataset &#x3D; drug_dataset.rename_column(    original_column_name&#x3D;â€Unnamed: 0â€, new_column_name&#x3D;â€patient_idâ€)drug_datasetâ€˜â€™â€™DatasetDict({    train: Dataset({        features: [â€˜patient_idâ€™, â€˜drugNameâ€™, â€˜conditionâ€™, â€˜reviewâ€™, â€˜ratingâ€™, â€˜dateâ€™, â€˜usefulCountâ€™],        num_rows: 161297    })    test: Dataset({        features: [â€˜patient_idâ€™, â€˜drugNameâ€™, â€˜conditionâ€™, â€˜reviewâ€™, â€˜ratingâ€™, â€˜dateâ€™, â€˜usefulCountâ€™],        num_rows: 53766    })})â€™â€™â€™
12345678910111213è¡¥å……ä¸€ä¸ªåŒ¿åè¡¨è¾¾å¼çš„ç»†èŠ‚&gt; `(lambda base, height: 0.5 * base * height)(4, 8)`&gt;&gt; 16 ## è½¬æ¢å¤§å°å†™
def lowercase_condition(example):    return {â€œconditionâ€: example[â€œconditionâ€].lower()}
drug_dataset.map(lowercase_condition)â€˜â€™â€™AttributeError: â€˜NoneTypeâ€™ object has no attribute â€˜lowerâ€™â€™â€™â€™
123456789è¿™é‡ŒæŠ¥é”™äº†## filter`dataset.filter`
drug_dataset &#x3D; drug_dataset.filter(lambda x: x[â€œconditionâ€] is not None)
drug_dataset &#x3D; drug_dataset.map(lowercase_condition)
Check that lowercasing workeddrug_dataset[â€œtrainâ€][â€œconditionâ€][:3]
â€˜â€™â€™[â€˜left ventricular dysfunctionâ€™, â€˜adhdâ€™, â€˜birth controlâ€™]â€™â€™â€™
1234567è¿‡æ»¤ç­›é€‰åˆæ ¼çš„æ•°æ®æ ·æœ¬## å¢åŠ åˆ—
def compute_review_length(example):    return {â€œreview_lengthâ€: len(example[â€œreviewâ€].split())}
drug_dataset &#x3D; drug_dataset.map(compute_review_length)
Inspect the first training exampledrug_dataset[â€œtrainâ€][0]â€˜â€™â€™{â€˜patient_idâ€™: 206461, â€˜drugNameâ€™: â€˜Valsartanâ€™, â€˜conditionâ€™: â€˜left ventricular dysfunctionâ€™, â€˜reviewâ€™: â€˜â€œIt has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oilâ€â€˜, â€˜ratingâ€™: 9.0, â€˜dateâ€™: â€˜May 20, 2012â€™, â€˜usefulCountâ€™: 27, â€˜review_lengthâ€™: 17}â€™â€™â€™
12345è¡¥å……ä¸€ä¸ªsort

drug_dataset[â€œtrainâ€].sort(â€œreview_lengthâ€)[:3]â€˜â€™â€™{â€˜patient_idâ€™: [103488, 23627, 20558],â€˜drugNameâ€™: [â€˜Loestrin 21 1 &#x2F; 20â€™, â€˜Chlorzoxazoneâ€™, â€˜Nucyntaâ€™],â€˜conditionâ€™: [â€˜birth controlâ€™, â€˜muscle spasmâ€™, â€˜painâ€™],â€˜reviewâ€™: [â€˜â€œExcellent.â€â€˜, â€˜â€œuselessâ€â€˜, â€˜â€œokâ€â€˜],â€˜ratingâ€™: [10.0, 1.0, 6.0],â€˜dateâ€™: [â€˜November 4, 2008â€™, â€˜March 24, 2017â€™, â€˜August 20, 2016â€™],â€˜usefulCountâ€™: [5, 2, 10],â€˜review_lengthâ€™: [1, 1, 1]}â€™â€™â€™
123456789101112131415&gt;sortåº”è¯¥ä¹Ÿæœ‰reverseé€‰é¡¹ï¼Œå¦‚æœçœŸè¦åšEDAè¿˜æ˜¯ç”¨Pandaså¥½äº†, [æŸ¥çœ‹å¯é…ç½®å‚æ•°](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.sort)åœ¨è¡¥å……ä¸€ä¸ª`Dataset.add_column()`An alternative way to add new columns to a dataset is with the `Dataset.add_column()` function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where `Dataset.map()` is not well suited for your analysis.## è§£æhtmlå­—ç¬¦
import html

text &#x3D; â€œI&amp;#039;m a transformer called BERTâ€html.unescape(text)
â€˜â€™â€™â€œIâ€™m a transformer called BERTâ€â€˜â€™â€™
123
drug_dataset &#x3D; drug_dataset.map(lambda x: {â€œreviewâ€: html.unescape(x[â€œreviewâ€])})
12345678910111213# map æ–¹æ³•## batchWhen you specify `batched=True` the function receives a dictionary with the fields of the dataset, but each value is now a *list of values*, and not just a single value. The return value of `Dataset.map()` should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. 
new_drug_dataset &#x3D; drug_dataset.map(    lambda x: {â€œreviewâ€: [html.unescape(o) for o in x[â€œreviewâ€]]}, batched&#x3D;True)
123456789101112131415æ‰¹é‡å¤„ç†ä¸ºTrueçš„è¯ï¼Œæ¯æ¬¡ä¼ è¿›æ¥å°±æ˜¯ä¸€ä¸ªå­—å…¸æ‰¹æ¬¡ã€‚ä¸€èˆ¬æˆ‘ä»¬åšçš„å°±æ˜¯æ›´æ–°è¿™ä¸ªæ•°æ®é›†If youâ€™re running this code in a notebook, youâ€™ll see that this command executes way faster than the previous one. And itâ€™s not because our reviews have already been HTML-unescaped â€” if you re-execute the instruction from the previous section (without `batched=True`), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a `for` loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.ä¹‹å‰å•ä¸ªå¤„ç†çš„ç”¨çš„æ˜¯forå¾ªç¯ï¼Œè¿™é‡Œæ‰¹é‡å¤„ç†å°±å¯ä»¥ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼Œè¦å¿«çš„å¤š## é…åˆtokenizerä½¿ç”¨
def tokenize_and_split(examples):    return tokenizer(        examples[â€œreviewâ€],        truncation&#x3D;True,        max_length&#x3D;128,        return_overflowing_tokens&#x3D;True,    )
result &#x3D; tokenize_and_split(drug_dataset[â€œtrainâ€][0])[len(inp) for inp in result[â€œinput_idsâ€]]
[128, 49]12345ä½¿ç”¨`return_overflowing_tokens`å‚æ•°æ¥æ¥å—æˆªæ–­çš„éƒ¨åˆ†ï¼Œè¿™é‡Œæˆ‘ä»¬177çš„é•¿åº¦å˜æˆäº†128å’Œ49ä¸¤ä»½
tokenized_dataset &#x3D; drug_dataset.map(tokenize_and_split, batched&#x3D;True)
1234567891011121314151617# æ•°æ®ç±»å‹è½¬æ¢## PandasTo enable the conversion between various third-party libraries, ğŸ¤— Datasets provides a `Dataset.set_format()` function. This function only changes the *output format* of the dataset, so you can easily switch to another format without affecting the underlying *data format*, which is Apache Arrow. The formatting is done in place. To demonstrate, letâ€™s convert our dataset to Pandas:`drug_dataset.set_format(&quot;pandas&quot;)`ä¸€èˆ¬ä½¿ç”¨`train_df = drug_dataset[&quot;train&quot;][:]` è·å¾—æ•´ä½“çš„åˆ‡ç‰‡ä½œä¸ºæ–°çš„Dataframe å¯ä»¥è‡ªå·±å°è¯•æ˜¯å¦è¿”å›å¯¹è±¡ä¸ºhfçš„dataset
from datasets import Dataset
freq_dataset &#x3D; Dataset.from_pandas(frequencies)freq_datasetâ€˜â€™â€™Dataset({    features: [â€˜conditionâ€™, â€˜frequencyâ€™],    num_rows: 819})â€™â€™â€™
123456789å¯ä»¥è½¬æ¢å›æ¥# train_test_split
drug_dataset_clean &#x3D; drug_dataset[â€œtrainâ€].train_test_split(train_size&#x3D;0.8, seed&#x3D;42)
Rename the default â€œtestâ€ split to â€œvalidationâ€drug_dataset_clean[â€œvalidationâ€] &#x3D; drug_dataset_clean.pop(â€œtestâ€)
Add the â€œtestâ€ set to our DatasetDictdrug_dataset_clean[â€œtestâ€] &#x3D; drug_dataset[â€œtestâ€]drug_dataset_clean
â€˜â€™â€™DatasetDict({    train: Dataset({        features: [â€˜patient_idâ€™, â€˜drugNameâ€™, â€˜conditionâ€™, â€˜reviewâ€™, â€˜ratingâ€™, â€˜dateâ€™, â€˜usefulCountâ€™, â€˜review_lengthâ€™, â€˜review_cleanâ€™],        num_rows: 110811    })    validation: Dataset({        features: [â€˜patient_idâ€™, â€˜drugNameâ€™, â€˜conditionâ€™, â€˜reviewâ€™, â€˜ratingâ€™, â€˜dateâ€™, â€˜usefulCountâ€™, â€˜review_lengthâ€™, â€˜review_cleanâ€™],        num_rows: 27703    })    test: Dataset({        features: [â€˜patient_idâ€™, â€˜drugNameâ€™, â€˜conditionâ€™, â€˜reviewâ€™, â€˜ratingâ€™, â€˜dateâ€™, â€˜usefulCountâ€™, â€˜review_lengthâ€™, â€˜review_cleanâ€™],        num_rows: 46108    })})â€™â€™â€™
12345678910111213141516171819# ä¿å­˜æ–‡ä»¶| Data format | Function                 || ----------- | ------------------------ || Arrow       | `Dataset.save_to_disk()` || CSV         | `Dataset.to_csv()`       || JSON        | `Dataset.to_json ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/#content-inner">5</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>ğŸ›´å‰å¾€github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>ç½‘ç«™èµ„è®¯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">æ–‡ç« æ•°ç›® :</div><div class="item-count">26</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»å­—æ•° :</div><div class="item-count">45.5k</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™è®¿å®¢æ•° :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»è®¿é—®é‡ :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ€åæ›´æ–°æ—¶é—´ :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-12T09:34:34.102Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è®¡ç®—æœºè§†è§‰</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è‡ªç„¶è¯­è¨€å¤„ç†</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">è®ºæ–‡ç²¾è¯»</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">æµç•…çš„Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">14</span><span class="categoryBar-list-descr">æ‹¥æœ‰ä¸€åˆ‡ å´å˜æˆå¤ªç©º</span></li></ul></div></div>';
      console.log('å·²æŒ‚è½½butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>