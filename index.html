<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-15 22:44:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/28501.html" title="Scrapy 01 tutorial"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Bocchi the Rock/bocchi9.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Scrapy 01 tutorial"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/28501.html" title="Scrapy 01 tutorial">Scrapy 01 tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-15T03:33:39.993Z" title="å‘è¡¨äº 2022-12-15 11:33:39">2022-12-15</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-15T13:40:50.889Z" title="æ›´æ–°äº 2022-12-15 21:40:50">2022-12-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Scrapy/">Scrapy</a></span></div><div class="content">
å¦‚æœvscodeä¸­ä½ çš„ç»ˆç«¯ä¸èƒ½è¯†åˆ«scrapyå¯ä»¥åœ¨ç¯å¢ƒå˜é‡ä¸­åŠ å…¥scrapy.exeçš„è·¯å¾„

å¯åŠ¨å®‰è£…å¥½åï¼Œåœ¨ç›®æ ‡æ–‡ä»¶å¤¹å†…å¯åŠ¨scrapy startproject tutorialå‘½ä»¤ï¼Œå°†ä¼šåˆ›å»ºå¦‚ä¸‹æ–‡ä»¶
12345678910111213141516tutorial/    scrapy.cfg            # deploy configuration file    tutorial/             # project&#x27;s Python module, you&#x27;ll import your code from here        __init__.py        items.py          # project items definition file        middlewares.py    # project middlewares file        pipelines.py      # project pipelines file        settings.py       # project settings file        spiders/          # a directory where you&#x27;ll later put your spiders            __init__.py





åœ¨tutorial/spidersç›®å½•ä¸‹åˆ›å»ºæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªçˆ¬è™«å‘½åä¸ºquotes_spider.py
1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider):    name = &quot;quotes&quot;    def start_requests(self):        urls = [            &#x27;https://quotes.toscrape.com/page/1/&#x27;,            &#x27;https://quotes.toscrape.com/page/2/&#x27;,        ]        for url in urls:            yield scrapy.Request(url=url, callback=self.parse)    def parse(self, response):        page = response.url.split(&quot;/&quot;)[-2]        filename = f&#x27;quotes-&#123;page&#125;.html&#x27;        with open(filename, &#x27;wb&#x27;) as f:            f.write(response.body)        self.log(f&#x27;Saved file &#123;filename&#125;&#x27;)



ç»ˆç«¯ä¸­å¯åŠ¨çˆ¬è™«scrapy crawl quotesä¼šå¾—åˆ°ä¸¤ä¸ªæ–‡ä»¶quotes-1.html and quotes-2.html
scrapy shellåœ¨è§£æä»–ä¸¤ä¹‹å‰ï¼Œæˆ‘ä»¬ä»‹ç» Scrapy shellï¼Œç”¨æ¥è°ƒè¯•æˆ‘ä»¬è¾“å‡º scrapy shell &lt;url&gt;

pip install ipythonä¹‹å åœ¨ä¸Šçº§ç›®å½•ä¸­æ‰¾åˆ°scrapy.cfgæ–‡ä»¶åœ¨settingä¸‹åŠ å…¥
shell = bpython å¦‚æœä½ çš„ipythonä¸èƒ½ç”¨çš„è¯
è¾“å…¥exitå¯ä»¥é€€å‡º

1234567891011121314151617scrapy shell &quot;https://quotes.toscrape.com/page/1/&quot;&#x27;&#x27;&#x27;[ ... Scrapy log here ... ]2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://quotes.toscrape.com/page/1/&gt; (referer: None)[s] Available Scrapy objects:[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;[s]   item       &#123;&#125;[s]   request    &lt;GET https://quotes.toscrape.com/page/1/&gt;[s]   response   &lt;200 https://quotes.toscrape.com/page/1/&gt;[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;[s]   spider     &lt;DefaultSpider &#x27;default&#x27; at 0x7fa91c8af990&gt;[s] Useful shortcuts:[s]   shelp()           Shell help (print this help)[s]   fetch(req_or_url) Fetch request (or URL) and update local objects[s]   view(response)    View response in a browser&#x27;&#x27;&#x27;

ä»¥ä¸Šæ˜¯è¿”å›çš„ä¸€äº›å¯ä»¥æ“ä½œçš„å¯¹è±¡
12response.css(&#x27;title&#x27;)# [&lt;Selector xpath=&#x27;descendant-or-self::title&#x27; data=&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;&gt;]

å¦‚æ­¤å¯ä»¥å®ç°äº¤äº’å¼è¿è¡Œ
cssè¯­æ³•::text12response.css(&#x27;title::text&#x27;).getall()# [&#x27;Quotes to Scrape&#x27;]

12response.css(&#x27;title&#x27;).getall()# [&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;]

get&#x2F;getallè¿”å›ä¸€ä¸ªï¼Œæˆ–è€…å…¨éƒ¨
æ­£åˆ™123456response.css(&#x27;title::text&#x27;).re(r&#x27;Quotes.*&#x27;)# [&#x27;Quotes to Scrape&#x27;]response.css(&#x27;title::text&#x27;).re(r&#x27;Q\w+&#x27;)# [&#x27;Quotes&#x27;]response.css(&#x27;title::text&#x27;).re(r&#x27;(\w+) to (\w+)&#x27;)# [&#x27;Quotes&#x27;, &#x27;Scrape&#x27;]



Xpathå®˜æ–¹æ¨èä½¿ç”¨è¿™ä¸ªï¼Œä½†æˆ‘è§‰å¾—csså†™çš„æ›´æ–¹ä¾¿ä¸€ç‚¹
1234response.xpath(&#x27;//title&#x27;)# [&lt;Selector xpath=&#x27;//title&#x27; data=&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;&gt;]response.xpath(&#x27;//title/text()&#x27;).get()# &#x27;Quotes to Scrape&#x27;



æå–æ•°æ®12345678910111213141516&#x27;&#x27;&#x27;&lt;div class=&quot;quote&quot;&gt;    &lt;span class=&quot;text&quot;&gt;â€œThe world as we have created it is a process of our    thinking. It cannot be changed without changing our thinking.â€&lt;/span&gt;    &lt;span&gt;        by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt;        &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt;    &lt;/span&gt;    &lt;div class=&quot;tags&quot;&gt;        Tags:        &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt;        &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt;        &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt;        &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt;    &lt;/div&gt;&lt;/div&gt;&#x27;&#x27;&#x27;

scrapy shell &#39;https://quotes.toscrape.com&#39;
å•ä¸ªæå–12345response.css(&quot;div.quote&quot;)&#x27;&#x27;&#x27;[&lt;Selector xpath=&quot;descendant-or-self::div[@class and contains(concat(&#x27; &#x27;, normalize-space(@class), &#x27; &#x27;), &#x27; quote &#x27;)]&quot; data=&#x27;&lt;div class=&quot;quote&quot; itemscope itemtype...&#x27;&gt;, &lt;Selector xpath=&quot;descendant-or-self::div[@class and contains(concat(&#x27; &#x27;, normalize-space(@class), &#x27; &#x27;), &#x27; quote &#x27;)]&quot; data=&#x27;&lt;div class=&quot;quote&quot; itemscope itemtype...&#x27;&gt;, ...]&#x27;&#x27;&#x27;


åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†selectorå’Œdata ï¼Œdataå°±æ˜¯æˆ‘ä»¬æ“ä½œçš„åˆ†å¸ƒ

12345678910quote = response.css(&quot;div.quote&quot;)[0]text = quote.css(&quot;span.text::text&quot;).get()text&#x27;&#x27;&#x27;&#x27;â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€&#x27; &#x27;&#x27;&#x27;author = quote.css(&quot;small.author::text&quot;).get()author# &#x27;Albert Einstein&#x27;


response.cssæœå¯»çš„æ ¼å¼ä¸ºâ€™æ ‡ç­¾.æ ‡ç­¾åç§°â€™
quoteä¸ºæˆ‘ä»¬htmlæ–‡ä»¶ä¸­æ‰€æœ‰class&#x3D;quoteçš„æ ‡ç­¾ç»„ï¼Œ
ç»„å†…span.textæ ‡ç­¾ä¸‹ä¸ºåè¨€ã€ç»„å†…small.authorä¸ºä½œè€…

å°ç»„æå–12345678910111213141516&#x27;&#x27;&#x27;&lt;div class=&quot;quote&quot;&gt;    &lt;span class=&quot;text&quot;&gt;â€œThe world as we have created it is a process of our    thinking. It cannot be changed without changing our thinking.â€&lt;/span&gt;    &lt;span&gt;        by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt;        &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt;    &lt;/span&gt;    &lt;div class=&quot;tags&quot;&gt;        Tags:        &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt;        &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt;        &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt;        &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt;    &lt;/div&gt;&lt;/div&gt;&#x27;&#x27;&#x27;



123tags = quote.css(&quot;div.tags a.tag::text&quot;).getall()tags# [&#x27;change&#x27;, &#x27;deep-thoughts&#x27;, &#x27;thinking&#x27;, &#x27;world&#x27;]



å…¨éƒ¨æå–12345678for quote in response.css(&quot;div.quote&quot;):    text = quote.css(&quot;span.text::text&quot;).get()    author = quote.css(&quot;small.author::text&quot;).get()    tags = quote.css(&quot;div.tags a.tag::text&quot;).getall()    print(dict(text=text, author=author, tags=tags))&#x27;&#x27;&#x27;&#123;&#x27;text&#x27;: &#x27;â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€&#x27;, &#x27;author&#x27;: &#x27;Albert Einstein&#x27;, &#x27;tags&#x27;: [&#x27;change&#x27;, &#x27;deep-thoughts&#x27;, &#x27;thinking&#x27;, &#x27;world&#x27;]&#125;&#123;&#x27;text&#x27;: &#x27;â€œIt is our choices, Harry, that show what we truly are, far more than our abilities.â€&#x27;, &#x27;author&#x27;: &#x27;J.K. Rowling&#x27;, &#x27;tags&#x27;: [&#x27;abilities&#x27;, &#x27;choices&#x27;]&#125;&#x27;&#x27;&#x27;



æ•°æ®ä¿å­˜scrapy crawl spiderman -O spn.json
1234567891011121314151617import scrapyclass QuotesSpider(scrapy.Spider):    name = &quot;quotes&quot;    start_urls = [        &#x27;https://quotes.toscrape.com/page/1/&#x27;,        &#x27;https://quotes.toscrape.com/page/2/&#x27;,    ]    def parse(self, response):        for quote in response.css(&#x27;div.quote&#x27;):            yield &#123;                &#x27;text&#x27;: quote.css(&#x27;span.text::text&#x27;).get(),                &#x27;author&#x27;: quote.css(&#x27;small.author::text&#x27;).get(),                &#x27;tags&#x27;: quote.css(&#x27;div.tags a.tag::text&#x27;).getall(),            &#125;



å¯åŠ¨çˆ¬è™«ä¼šè·å¾—å¦‚ä¸‹å†…å®¹:
æ³¨è¦åœ¨çˆ¬è™«çš„æ ¹ç›®å½•å¯åŠ¨çˆ¬è™«
12345&#x27;&#x27;&#x27;2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://quotes.toscrape.com/page/1/&gt;&#123;&#x27;tags&#x27;: [&#x27;life&#x27;, &#x27;love&#x27;], &#x27;author&#x27;: &#x27;AndrÃ© Gide&#x27;, &#x27;text&#x27;: &#x27;â€œIt is better to be hated for what you are than to be loved for what you are not.â€&#x27;&#125;2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://quotes.toscrape.com/page/1/&gt;&#123;&#x27;tags&#x27;: [&#x27;edison&#x27;, &#x27;failure&#x27;, &#x27;inspirational&#x27;, &#x27;paraphrased&#x27;], &#x27;author&#x27;: &#x27;Thomas A. Edison&#x27;, &#x27;text&#x27;: &quot;â€œI have not failed. I&#x27;ve just found 10,000 ways that won&#x27;t work.â€&quot;&#125;&#x27;&#x27;&#x27;



è¾“å‡ºæ ¼å¼scrapy crawl quotes -O quotes.json

-Oå°†ä¼šè¦†å†™åŒåæ–‡ä»¶å·²å­˜åœ¨çš„å†…å®¹ï¼Œ
-oåˆ™ä¼šåœ¨å·²å­˜åœ¨æ–‡ä»¶çš„åé¢å¢åŠ å†…å®¹ï¼Œä½†æ˜¯æ–°æ—§æ ¼å¼å¯èƒ½ä¸åŒï¼Œå¯ä»¥ä½¿ç”¨
scrapy crawl quotes -o quotes.jsonl

æœ‰jsonã€jsonlã€csvã€xmlå››ç§æ ¼å¼
çˆ¬å–æ•´ä¸ªç½‘ç«™123456&#x27;&#x27;&#x27;&lt;ul class=&quot;pager&quot;&gt;    &lt;li class=&quot;next&quot;&gt;        &lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt;    &lt;/li&gt;&lt;/ul&gt;&#x27;&#x27;&#x27;



12response.css(&#x27;li.next a&#x27;).get()# &#x27;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;â†’& ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/55870.html" title="diffusion ç»¼è¿°"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris37.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="diffusion ç»¼è¿°"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/55870.html" title="diffusion ç»¼è¿°">diffusion ç»¼è¿°</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-13T14:06:13.619Z" title="å‘è¡¨äº 2022-12-13 22:06:13">2022-12-13</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-15T13:01:04.564Z" title="æ›´æ–°äº 2022-12-15 21:01:04">2022-12-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/diffusion/">diffusion</a></span></div><div class="content">å¾…æ’ç‰ˆ
ç¬¬ä¸€éƒ¨åˆ†æ˜¯DDPMæ—¶ä»£çš„å›¾åƒç¼–è¾‘ã€‚å› ä¸ºè¿˜æ²¡æœ‰ä»»ä½•çš„å¼•å¯¼ç”ŸæˆæŠ€æœ¯çš„å‡ºç°ï¼Œè¿™ä¸€é˜¶æ®µçš„è®ºæ–‡éƒ½å±äºåˆ©ç”¨è¾“å…¥å›¾åƒå¼•å¯¼ç”Ÿæˆçš„èŒƒå¼ã€‚
ç¬¬äºŒéƒ¨åˆ†æ˜¯åœ¨æ˜¾å¼åˆ†ç±»å™¨å¼•å¯¼ç”ŸæˆæŠ€æœ¯å‡ºç°åï¼ŒåŸºäºCLIPæ¨¡å‹çš„å¤šæ¨¡æ€å¼•å¯¼ç”ŸæˆæŠ€æœ¯çš„è°ƒç ”ã€‚
ç¬¬ä¸‰éƒ¨åˆ†æ˜¯æœ€è¿‘ï¼ˆ2022.11ï¼‰ä¸€ä¸¤ä¸ªæœˆåŸºäºStable-Diffusion&#x2F;Imagenç­‰ä¸€ç³»åˆ—æ¨¡å‹æ‰€äº§ç”Ÿçš„å›¾åƒç¼–è¾‘æŠ€æœ¯çš„è°ƒç ”ã€‚

ä¸åƒäººè¯
ç¬¬ä¸€é˜¶æ®µ: DDPM
åŠ å™ª diffusion å†é™å™ªè¿˜åŸ å…¨å±€ä¿®æ”¹
é€æ­¥å‘ç°å¯¹æ¢¯åº¦çš„æ§åˆ¶å¾ˆé‡è¦ï¼Œäºæ˜¯åŠ å…¥å¯¹æ¢¯åº¦æ§åˆ¶ã€‚DDPM-&gt;IVLR-&gt;SDEdit-&gt;RePaint
æœ€åä»æ‰“è¡¥ä¸æ§åˆ¶ç”Ÿæˆçš„åŸºç¡€ä¸Šï¼Œå¼•å¯¼å‡ºäº†å¯¹å¯¼æ•°çš„æ§åˆ¶
ç¬¬äºŒé˜¶æ®µ: DDIM 
Diffusion Models Beat GANs on Image Synthesis åŠ å…¥10å€çš„å®šå‘æ¢¯åº¦æ§åˆ¶ 
More Control for Free! Image Synthesis with Semantic Diffusion Guidance : CLIP å¯ä»¥å±€éƒ¨ä¿®æ”¹
æƒ³è¦ä½¿ç”¨ä¸€ä¸ªæ–‡æœ¬æ¥å¼•å¯¼å›¾åƒç”Ÿæˆï¼Œæˆ‘ä»¬å¯ä»¥æ¯ä¸€æ­¥éƒ½è®¡ç®—ç°åœ¨çš„å›¾åƒè¡¨å¾å’Œæ–‡æœ¬è¡¨å¾çš„è·ç¦»ï¼Œä½¿ç”¨æ–¹ç¨‹çš„æ¢¯åº¦æ¥è®¡ç®—ç¼©å°è¿™ä¸ªè·ç¦»çš„æ–¹å‘
ä½†å°±åœ¨åå¤©ä¹‹åOpenAIå‘å¸ƒäº†GLIDEï¼Œä½¿ç”¨äº†ä¸‹é¢ä¼šæåˆ°çš„éšå¼åˆ†ç±»å™¨å¼•å¯¼çš„å›¾åƒç”Ÿæˆ
éšç€æ–°çš„æ›´å¼ºå¤§æ›´ä¾¿æ·çš„æ¨¡å‹å¦‚Stable-Diffusion, Imagenç­‰å¦‚é›¨åæ˜¥ç¬‹èˆ¬æ¶Œç°ï¼Œä¸Šé¢çš„å„é¡¹å·¥ä½œå¯èƒ½åªå‰©ä¸‹äº†å€Ÿé‰´æ„ä¹‰ã€‚
Classifier-Free Diffusion Guidance :åŸºäºéšå¼åˆ†ç±»å™¨çš„æ–‡ç”Ÿå›¾å¤§æ¨¡å‹
æ— åˆ†ç±»å™¨å¼•å¯¼å¯ä»¥è¯´æ˜¯GLIDE&#x2F;Stable-Diffusion&#x2F;Imagençš„åšæ³•çš„ç›´æ¥å¥ åŸºå·¥ä½œä¹‹ä¸€
ç¬¬ä¸‰é˜¶æ®µ: 
åœ¨éšå¼åˆ†ç±»å™¨ä¸Šå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è°ƒæ§ç”Ÿæˆ

ç¬¬ä¸€ç§æ˜¯æ ¹æ®æ‰©æ•£æ¨¡å‹è¿­ä»£å»å™ªçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹çš„ä½é¢‘ç»†èŠ‚ä¸Šç»§ç»­ç”Ÿæˆã€‚è¿™ç§åšæ³•è™½ç„¶èƒ½ä¿ç•™å¤§éƒ¨åˆ†å‡ ä½•ç‰¹å¾ï¼Œä½†æ˜¯ä¹ŸåŒæ ·æ— æ³•è°ƒæ§å‡ ä½•ç‰¹å¾ã€‚

Imagic: Text-Based Real Image Editing with Diffusion Models:
å…·ä½“æ¥è¯´ï¼ŒImagicå°†æ¦‚å¿µç»‘å®šè¿™ä»¶äº‹æ‹†æˆäº†ä¸‰ä¸ªæ­¥éª¤ï¼Œå¯¹äºè¾“å…¥å›¾åƒxå’Œæˆ‘ä»¬å¸Œæœ›ç”Ÿæˆçš„ç›®æ ‡æè¿°æ–‡æœ¬text_targetæ¥è¯´ï¼š
1ï¼šæˆ‘ä»¬é¦–å…ˆå†»ç»“æ•´ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨æ¨¡å‹è®­ç»ƒæ—¶çš„ç”Ÿæˆç›®æ ‡æ¥å¾®è°ƒtext_targetçš„æ–‡æœ¬è¡¨å¾ï¼Œä½¿å…¶æ¥è¿‘äºå›¾åƒçš„è¡¨å¾ã€‚
2ï¼šæˆ‘ä»¬æ”¾å¼€æ•´ä¸ªæ¨¡å‹çš„æƒé‡æ›´æ–°ï¼Œä¾ç„¶ä½¿ç”¨è®­ç»ƒæ—¶çš„ç”Ÿæˆç›®æ ‡ï¼Œä½†è¿™æ¬¡å…¨æ¨¡å‹å¾®è°ƒã€‚æ¨¡å‹çš„è¾“å…¥æ˜¯å›¾åƒxå’Œæˆ‘ä»¬å¾®è°ƒåçš„æ–‡æœ¬è¡¨å¾ã€‚è¿™ä¸€æ­¥æ˜¯å› ä¸ºå“ªæ€•æˆ‘ä»¬è®©ç›®æ ‡æ–‡æœ¬è¡¨å¾å’ŒåŸå›¾çš„è¡¨å¾æ¥è¿‘äº†ï¼Œä¹Ÿä¸èƒ½ä¿è¯æˆ‘ä»¬è¾“å…¥è®©æˆ‘ä»¬å¾®è°ƒåçš„ç›®æ ‡æ–‡æœ¬è¡¨å¾å¯ä»¥ç”Ÿæˆæˆ‘ä»¬çš„åŸå›¾ï¼Œæ‰€ä»¥æˆ‘ä»¬å†æ¬¡å°†è¿™ä¸¤ä¸ªæ¦‚å¿µä¸€èµ·è®­ç»ƒï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¾®è°ƒåçš„ç›®æ ‡æ–‡æœ¬è¡¨å¾ç”Ÿæˆæˆ‘ä»¬çš„åŸå›¾
3ï¼šæ—¢ç„¶æˆ‘ä»¬å·²ç»å°†åŸå›¾å’Œå¾®è°ƒåçš„æ–°æ–‡æœ¬è¡¨å¾ç»‘å®šèµ·æ¥äº†ï¼Œç°åœ¨æˆ‘ä»¬å†ä½¿ç”¨åŸæœ¬çš„ç›®æ ‡æ–‡æœ¬è¡¨å¾ä¸å¾®è°ƒåçš„æ–‡æœ¬è¡¨å¾åšæ’å€¼ï¼Œæ¥å¯¹åŸå›¾åƒæ–½åŠ å½±å“å³å¯ã€‚

è®­ç»ƒå¥½å›¾å½¢è¾“å‡ºé”ä½å‚æ•°-&gt; å¾®è°ƒå…¨å‚æ•°é€‚åº”æ–‡æœ¬è¾“å‡º-&gt; å¼€æ”¾å…¨å‚æ•°ä¸¤ä¸ªä¸€èµ·è®­ç»ƒ -&gt;å°†ä¸¤ä¸ªæ¦‚å¿µæ†ç»‘å¹¶å¼€æ”¾ä¿®æ”¹
ç®€å•æ¥è®²å¯ä»¥å°†å¾®è°ƒåçš„ç›®æ ‡æ–‡æœ¬è¡¨å¾è¿‘ä¼¼å½“ä½œåŸå›¾åƒåŸç”Ÿçš„æ–‡æœ¬è¡¨å¾ï¼Œé‚£ä¹ˆæœ€åä¸€æ­¥ä½¿ç”¨ç›®æ ‡è¡¨å¾å¯¹åŸç”Ÿçš„è¡¨å¾æ–½åŠ å½±å“å°±éå¸¸è‡ªç„¶äº†

DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation
å…·ä½“æ¥è¯´ä½œè€…æå‡ºäº†ä½¿ç”¨ç¨€ç¼ºè¯åŠ ç§ç±»è¯å¦‚â€œbeikkpic dogâ€çš„ç»„åˆæ–‡æœ¬æ¥å¾®è°ƒä¸€ç»„ç…§ç‰‡å’Œè¿™ä¸ªæ–‡æœ¬çš„ç»‘å®šã€‚ä½†æ˜¯ä»…ä»…ç”¨å°‘é‡çš„ç…§ç‰‡æ¥å¾®è°ƒä¸€ä¸ªæœ‰ç€å¤§é‡å‚æ•°çš„æ¨¡å‹å¾ˆæ˜æ˜¾ä¼šå¸¦æ¥æä¸ºä¸¥é‡çš„è¿‡æ‹Ÿåˆã€‚å¹¶ä¸”è¿˜ä¼šå¸¦æ¥ä¸€ä¸ªè¯­è¨€æ¨¡å‹é‡Œç‰¹åˆ«å¸¸è§çš„äº‹æƒ…â€“ç¾éš¾æ€§é—å¿˜ã€‚è¿™ä¸¤ä¸ªé—®é¢˜çš„è¡¨ç°ä¸€ä¸ªæ˜¯ç»‘å®šè¯çš„å½¢æ€å¾ˆéš¾å˜æ¢ï¼Œå°±å¦‚ä¸Šç¯‡çš„Unituneä¸€æ ·ã€‚å¦ä¸€ä¸ªé—®é¢˜æ˜¯å¯¹ç§ç±»è¯é‡Œé¢çš„ç§ç±»ç”Ÿæˆä¹Ÿä¼šå¿«é€Ÿå¤±å»å¤šæ ·æ€§å’Œå˜åŒ–æ€§ã€‚äºæ˜¯é’ˆå¯¹è¿™ä¸ªé—®é¢˜ä½œè€…é’ˆå¯¹æ€§åœ°æå‡ºäº†ä¸€ä¸ªå«è‡ªèº«ç±»åˆ«å…ˆéªŒä¿å­˜æŸå¤±çš„æŸå¤±å‡½æ•°ã€‚
è¿™ä¸ªå‡½æ•°çš„è®¾è®¡æ˜¯åœ¨ç”¨æˆ·æä¾›ä¸€ä¸ªæŒ‡å®šçš„ç±»åˆ«å’Œè¿™ä¸ªç±»åˆ«çš„ä¸€ç»„å›¾ç‰‡ï¼ˆå¦‚è‡ªå®¶çš„å® ç‰©ç‹—çš„å¤šå¼ ç…§ç‰‡ï¼‰åï¼Œæ¨¡å‹åŒæ—¶ä½¿ç”¨â€œç‰¹æ®Šè¯+ç±»åˆ«â€å¯¹ç”¨æˆ·ç…§ç‰‡è®­ç»ƒå’Œâ€œç±»åˆ«â€ä¸æ¨¡å‹ç”Ÿæˆçš„è¯¥ç±»åˆ«å›¾è®­ç»ƒã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æ¨¡å‹å¯ä»¥åœ¨å°†ç‰¹å®šçš„ç…§ç‰‡ä¸»ä½“ä¸ç‰¹æ®Šè¯ç»‘å®šçš„æ—¶å€™å¯ä»¥ä¸€èµ·å­¦åˆ°å’Œå…¶ç±»åˆ«çš„å…³ç³»ï¼Œå¹¶ä¸”åŒæ—¶è¯¥ç±»åˆ«çš„ä¿¡æ¯åœ¨ä¸æ–­çš„è¢«é‡ç”³ä»¥å¯¹æŠ—ç”¨æˆ·ç…§ç‰‡ä¿¡æ¯çš„å†²å‡»ã€‚ä½œè€…åœ¨è®­ç»ƒçš„æ—¶å€™ç‰¹æ„å°†è¿™ä¸¤ä¸ªæŸå¤±ä»¥ä¸€æ¯”ä¸€çš„æ¯”ä¾‹è®­ç»ƒäº†200ä¸ªè½®æ¬¡å·¦å³ã€‚(å•å¡GPU 15åˆ†é’Ÿå·¦å³å°±å¯ä»¥)
Prompt-to-Prompt Image Editing with Cross-Attention Control
è¿™ç¯‡æ–‡ç« çš„æ´è§æ¥è‡ªäºä¸€ä¸ªé‡è¦æ€è€ƒï¼šå³å¤šæ¨¡æ€é‡Œæ–‡ç”Ÿå›¾çš„æ–‡æœ¬æ˜¯å¦‚ä½•å¯¹ç”Ÿæˆè¿‡ç¨‹æ–½åŠ å½±å“çš„ï¼Ÿ

åŸºäºéšå¼åˆ†ç±»å™¨çš„æ–‡å›¾æ¨¡å‹æ˜¯é€šè¿‡è®­ç»ƒä¸€ä¸ªæ—¢å¯ä»¥åšæ— æ¡ä»¶ç”Ÿæˆçš„æ¢¯åº¦é¢„ä¼°ï¼Œä¹Ÿå¯ä»¥åšæ¡ä»¶ç”Ÿæˆçš„æ¢¯åº¦é¢„ä¼°çš„æ¨¡å‹å®ç°çš„ã€‚è€Œå…¶ä¸­è¿™ä¸ªæ¡ä»¶äº¤äº’çš„æ–¹å¼åœ¨Imagenå’ŒStable-Diffusioné‡Œéƒ½æ˜¯é€šè¿‡cross-attentionå®ç°ä¿¡æ¯èåˆçš„ã€‚é‚£ä¹ˆå¾ˆæ˜æ˜¾ï¼Œæˆ‘ä»¬çš„ç€çœ¼ç‚¹ä¹Ÿåº”è¯¥åœ¨cross-attentionä¸Šã€‚

è€Œä½œè€…çš„æ´è§åˆ™åœ¨äºï¼šæˆ‘ä»¬è¾“å…¥çš„æ–‡æœ¬å’Œåƒç´ ä¹‹é—´å­˜åœ¨ç€ä¸€ä¸ªç©ºé—´å¯¹åº”çš„å…³ç³»ã€‚é€šè¿‡è°ƒæ§æ³¨æ„åŠ›å’Œåƒç´ é—´çš„æ˜ å°„ã€‚æˆ‘ä»¬èƒ½å¤Ÿå¯¹å›¾åƒçš„ä¸åŒåŒºåŸŸå®æ–½å‡†ç¡®çš„å¼•å¯¼ã€‚
![](..&#x2F;..&#x2F;article_img&#x2F;paper_img&#x2F;diffusion&#x2F;bear attn.png)
æœ‰äº†ä»¥ä¸Šæ´è§æ®æ­¤è¿›è¡Œå›¾åƒå¼•å¯¼ç”Ÿæˆå°±å¾ˆç›´è§‚äº†ï¼Œä½œè€…å°†å…¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦åœºæ™¯ï¼šå•è¯æ›¿æ¢ï¼ˆæ¯”å¦‚åœ¨ä¸Šå›¾é‡Œå°†ç†Šæ¢æˆçŒ«åˆ™å°†çŒ«è¿™ä¸ªtokenå¯¹åº”çš„mapæ¢æˆç†Šçš„mapï¼‰ï¼Œå•è¯å¢æ·»ï¼ˆåœ¨åŸæœ‰çš„mapä¸Šå¢åŠ æ–°çš„å•è¯çš„mapï¼‰ï¼Œæ³¨æ„åŠ›é‡åŠ æƒï¼ˆå¦‚æœæƒ³æ”¾å¤§æˆ–å‡å¼±æŸä¸ªè¯å¯¹åŸå›¾çš„å¼•å¯¼æ•ˆæœåˆ™å¯¹å…¶mapä¹˜ä¸Šæ–°çš„æƒé‡å€¼ï¼Œå¦‚é™ä½ä¸‹é›ªçš„æ•ˆæœå¼€èŠ±çš„ç¨‹åº¦ç­‰ï¼‰
åŸé“¾
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/7194.html" title="HF Course 09 Custom Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 09 Custom Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/7194.html" title="HF Course 09 Custom Tokenizer">HF Course 09 Custom Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-12T08:15:06.518Z" title="å‘è¡¨äº 2022-12-12 16:15:06">2022-12-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:31:52.517Z" title="æ›´æ–°äº 2022-12-12 22:31:52">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">
ä¸ä¹‹å‰ç»§æ‰¿å¼çš„åˆ†è¯å™¨ä¸åŒï¼Œè¿™è¯æˆ‘ä»¬å°†ä»è¯­æ–™åº“ä¸­è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„åˆ†è¯å™¨
é¦–å…ˆæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªWordPieceç±»å‹çš„åˆ†è¯å™¨

åŠ è½½æ–‡æ¡£12345678910111213from datasets import load_datasetdataset = load_dataset(&quot;wikitext&quot;, name=&quot;wikitext-2-raw-v1&quot;, split=&quot;train&quot;)def get_training_corpus():    for i in range(0, len(dataset), 1000):        yield dataset[i : i + 1000][&quot;text&quot;]# ä¹Ÿå¯ä»¥ä»æœ¬åœ°æ‰“å¼€æ–‡æ¡£with open(&quot;wikitext-2.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:    for i in range(len(dataset)):        f.write(dataset[i][&quot;text&quot;] + &quot;\n&quot;)



åŠ è½½æ„ä»¶1234567891011from tokenizers import (    decoders,    models,    normalizers,    pre_tokenizers,    processors,    trainers,    Tokenizer,)tokenizer = Tokenizer(models.WordPiece(unk_token=&quot;[UNK]&quot;))

æˆ‘ä»¬ä»tokenizeråº“ä¸­åŠ è½½ç‰¹æ®Šçš„modelæ„ä»¶ï¼Œæ¥ä½¿ç”¨WordPieceæ–¹æ³•
è®¾å®šé‡åˆ°æ²¡è§è¿‡çš„è¯æ ‡è®°ä¸º[UNK]ï¼Œ åŒæ—¶å¯ä»¥è®¾ç½®max_input_chars_per_wordä½œä¸ºæœ€å¤§è¯é•¿
è®¾ç½®Normalizerè¿™é‡Œæˆ‘ä»¬é€‰æ‹©bertçš„è®¾ç½®ï¼ŒåŒ…æ‹¬: 
æ‰€æœ‰å­—æ¯å°å†™ã€strip_accentsé™¤å»é‡éŸ³ã€åˆ é™¤æ§åˆ¶å­—ç¬¦ã€å°†æ‰€æœ‰å¤šä¸ªç©ºæ ¼è®¾ç½®ä¸ºå•ä¸ªç©ºæ ¼ã€æ±‰å­—å‘¨å›´æ”¾ç½®ç©ºæ ¼ã€‚
123456789tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)# ä½ ä¹Ÿå¯ä»¥è‡ªå®šä¹‰tokenizer.normalizer = normalizers.Sequence(    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])print(tokenizer.normalizer.normalize_str(&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;))# hello how are u?

ä¸Šé¢è‡ªå®šä¹‰ä¸­æˆ‘ä»¬ä½¿ç”¨Sequenceæ–¹æ³•å®šä¹‰æˆ‘ä»¬è‡ªå·±çš„è§„èŒƒåŒ–è§„åˆ™
Pre-tokenizationå’Œä¸Šé¢ä¸€æ ·å¯ä»¥é€šè¿‡tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()å¥—ç”¨bertçš„è®¾ç½®
ä¸‹é¢æ˜¯customç‰ˆæœ¬
12345tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)), (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;



.Whitespace()æ˜¯å¯¹æ ‡ç‚¹ç©ºæ ¼åˆ†éš”ï¼Œä½ å¯ç”¨ä¸‹é¢çš„åˆ†éš”
1234pre_tokenizer = pre_tokenizers.WhitespaceSplit()pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&quot;Let&#x27;s&quot;, (0, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre-tokenizer.&#x27;, (14, 28))]&#x27;&#x27;&#x27;



æ¨èä½¿ç”¨Sequenceæ–¹æ³•ç»„åˆä½ çš„é¢„åˆ†è¯
1234567pre_tokenizer = pre_tokenizers.Sequence(    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()])pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)), (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;



Trainerè®­ç»ƒä¹‹å‰æˆ‘ä»¬éœ€è¦åŠ å…¥ç‰¹æ®Štokenå› ä¸ºä»–ä¸åœ¨ä½ çš„è¯åº“ä¹‹ä¸­
12special_tokens = [&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)


As well as specifying the vocab_size and special_tokens, we can set the min_frequency (the number of times a token must appear to be included in the vocabulary) or change the continuing_subword_prefix (if we want to use something different from ##).
æ”¹æŸä¸ªtokenå¿…é¡»å‡ºç°å¤šå°‘æ¬¡ã€æ”¹è¿æ¥å‰ç¼€##ä¸ºåˆ«çš„



å¼€å§‹è®­ç»ƒ
12345tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)# å¦å¤–çš„ç‰ˆæœ¬tokenizer.model = models.WordPiece(unk_token=&quot;[UNK]&quot;)tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)

ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ç”¨ä¸Šé¢å®šä¹‰çš„ç”Ÿæˆå™¨
ç¬¬äºŒç§ä¼ å…¥â€wikitext-2.txtâ€æ–‡ä»¶
åˆ°æ­¤æˆ‘ä»¬tokenizerå°±å…·æœ‰äº†ä¸€èˆ¬tokenizerçš„æ‰€æœ‰æ–¹æ³•å¦‚encode
1234encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)# [&#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;]



Post-processingæœ€åæˆ‘ä»¬éœ€è¦åŒ…è£¹æˆ‘ä»¬çš„tokenåˆ°ç‰¹æ®Šçš„æ ¼å¼å¦‚: [CLS]â€¦[SEP]â€¦[SEP]
é¦–å…ˆæˆ‘ä»¬è·å–æ‰€éœ€çš„ç‰¹æ®Štokençš„ä¸‹æ ‡
12345cls_token_id = tokenizer.token_to_id(&quot;[CLS]&quot;)sep_token_id = tokenizer.token_to_id(&quot;[SEP]&quot;)print(cls_token_id, sep_token_id)# (2, 3)



æ¥ä¸‹æ¥å¤„ç†æˆ‘ä»¬çš„æ¨¡æ¿
12345tokenizer.post_processor = processors.TemplateProcessing(    single=f&quot;[CLS]:0 $A:0 [SEP]:0&quot;,    pair=f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;,    special_tokens=[(&quot;[CLS]&quot;, cls_token_id), (&quot;[SEP]&quot;, sep_token_id)],)

æ¨¡æ¿æˆ‘ä»¬éœ€è¦è®¾ç½®ä¸¤ç§æ¨¡å¼:

singleâ€“å•ä¸ªå¥å­æƒ…å†µä¸‹

[0,0,0,0]


pair

[0,0,0,1,1,1]


æœ€åæŒ‡å®šç‰¹æ®Štokençš„id


æŸ¥çœ‹
12345678910encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)# [&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer...&quot;, &quot;on a pair of sentences.&quot;)print(encoding.tokens)print(encoding.type_ids)&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;...&#x27;, &#x27;[SEP]&#x27;, &#x27;on&#x27;, &#x27;a&#x27;, &#x27;pair&#x27;, &#x27;of&#x27;, &#x27;sentences&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]&#x27;&#x27;&#x27;



decoderæ¥ä¸‹æ¥å¯¹è§£ç å™¨åšä¸€å®šè®¾ç½®
1234tokenizer.decoder = decoders.WordPiece(prefix=&quot;##&quot;)tokenizer.decode(encoding.ids)# &quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;



ä¿å­˜ &amp; åŠ è½½ Custom Tokenizertokenizer.save(&quot;tokenizer.json&quot;)
new_tokenizer = Tokenizer.from_file(&quot;tokenizer.json&quot;)
è½¬æˆFast TokenizerTo use this tokenizer in ğŸ¤— Transformers, we have to wrap it in a PreTrainedTokenizerFast. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, BertTokenizerFast). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.

å¯ä»¥ç»§æ‰¿ä½ çš„ç‰¹å®šç±»BertTokenizerFastï¼Œä¹Ÿå¯ä»¥ç”¨æ³›ç±»PreTrainedTokenizerFast

1234567891011from transformers import PreTrainedTokenizerFastwrapped_tokenizer = PreTrainedTokenizerFast(    tokenizer_object=tokenizer,    # tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively    unk_token=&quot;[UNK]&quot;,    pad_token=&quot;[PAD]&quot;,    cls_token=&quot;[CLS]&quot;,    sep_token=&quot;[SEP]&quot;,    mask_token=&quot;[MASK]&quot;,)

è¿™é‡Œå¯ä»¥ä»æ–‡ä»¶ä¸­åŠ è½½ä½ çš„tokenizerè®¾ç½®ã€ä¹Ÿå¯ç›´æ¥èµ‹å€¼ã€æ³¨æ„ä½ çš„ç‰¹æ®Šç¬¦å·å¿…é¡»é‡æ–°å®šä¹‰
BPEç±»å‹çš„åˆ†è¯å™¨1234567tokenizer = Tokenizer(models.BPE())tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test pre-tokenization!&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;s&quot;, (3, 5)), (&#x27;Ä test&#x27;, (5, 10)), (&#x27;Ä pre&#x27;, (10, 14)), (&#x27;-&#x27;, (14, 15)), (&#x27;tokenization&#x27;, (15, 27)), (&#x27;!&#x27;, (27, 28))]&#x27;&#x27;&#x27;



GPT2åªéœ€è¦å¼€å§‹å’Œç»“æŸçš„ç‰¹æ®Štoken
12345678910trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[&quot;&lt;|endoftext|&gt;&quot;])tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)tokenizer.model = models.BPE()tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)&#x27;&#x27;&#x27;[&#x27;L&#x27;, &#x27;et&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;Ä test&#x27;, &#x27;Ä this&#x27;, &#x27;Ä to&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;





12345678tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)sentence = &quot;Let&#x27;s test this tokenizer.&quot;encoding = tokenizer.encode(sentence)start, end = encoding.offsets[4]sentence[start:end]# &#x27; test&#x27;

The trim_offsets = False option indicates to the post-processor that we should leave the offsets of tokens that begin with â€˜Ä â€™ as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Letâ€™s have a look at the result with the text we just encoded, where &#39;Ä test&#39; is the token at index 4

trim_offsetsè®¾å®šæ˜¯å¦ä¿®æ­£å­—ç¬¦çš„ç©ºæ ¼ä½ç½®è¿›å…¥åç§»é‡

123tokenizer.decoder = decoders.ByteLevel()tokenizer.decode(encoding.ids)# &quot;Let&#x27;s test this tokenizer.&quot;



åŒ…è£…
123456789101112from transformers import PreTrainedTokenizerFastwrapped_tokenizer = PreTrainedTokenizerFast(    tokenizer_object=tokenizer,    bos_token=&quot;&lt;|endoftext|&gt;&quot;,    eos_token=&quot;&lt;|endoftext|&gt;&quot;,)# æˆ–è€…from transformers import GPT2TokenizerFastwrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)



Unigramç±»å‹çš„åˆ†è¯å™¨12345678910111213tokenizer = Tokenizer(models.Unigram())from tokenizers import Regextokenizer.normalizer = normalizers.Sequence(    [        normalizers.Replace(&quot;``&quot;, &#x27;&quot;&#x27;),        normalizers.Replace(&quot;&#x27;&#x27;&quot;, &#x27;&quot;&#x27;),        normalizers.NFKD(),        normalizers.StripAccents(),        normalizers.Replace(Regex(&quot; &#123;2,&#125;&quot;), &quot; &quot;),    ])

ç¬¬ä¸€ã€äºŒä¸ªnormå°†ç¬¦å·æ›¿æ¢ï¼Œæœ€åä¸€ä¸ªå°†å¤šä¸ªç©ºæ ¼æ›¿æ¢æˆä¸€ä¸ª
1234tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test the pre-tokenizer!&quot;)# [(&quot;â–Let&#x27;s&quot;, (0, 5)), (&#x27;â–test&#x27;, (5, 10)), (&#x27;â–the&#x27;, (10, 14)), (&#x27;â–pre-tokenizer!&#x27;, (14, 29))]



123456789special_tokens = [&quot;&lt;cls&gt;&quot;, &quot;&lt;sep&gt;&quot;, &quot;&lt;unk&gt;&quot;, &quot;&lt;pad&gt;&quot;, &quot;&lt;mask&gt;&quot;, &quot;&lt;s&gt;&quot;,  ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/7193.html" title="HF Course 08 Tokenizeråº•å±‚ç®—æ³•"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 08 Tokenizeråº•å±‚ç®—æ³•"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/7193.html" title="HF Course 08 Tokenizeråº•å±‚ç®—æ³•">HF Course 08 Tokenizeråº•å±‚ç®—æ³•</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-12T03:35:18.892Z" title="å‘è¡¨äº 2022-12-12 11:35:18">2022-12-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:30:42.647Z" title="æ›´æ–°äº 2022-12-12 22:30:42">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">æµç¨‹ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬çš„tokenizeræœ‰å¦‚ä¸‹æµç¨‹

è§„èŒƒåŒ–è§„èŒƒåŒ–æ˜¯å¯¹å­—ç¬¦åšå¤§å°å†™å¤„ç†ä¹‹ç±»çš„æˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹APIæŸ¥çœ‹åº•å±‚çš„normalizationæ–¹æ³•
123456789from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)print(type(tokenizer.backend_tokenizer))# &lt;class &#x27;tokenizers.Tokenizer&#x27;&gt;print(tokenizer.backend_tokenizer.normalizer.normalize_str(&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;))# &#x27;hello how are u?&#x27;



é¢„åˆ†è¯é€šè¿‡å¦‚ä¸‹apiæŸ¥çœ‹åˆ†è¯å™¨æ˜¯å¦‚ä½•åšpre_tokenizeçš„
123tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)&#x27;&#x27;&#x27;&#x27;[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;how&#x27;, (7, 10)), (&#x27;are&#x27;, (11, 14)), (&#x27;you&#x27;, (16, 19)), (&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;

å¯ä»¥çœ‹åˆ°åé¢çš„åç§»é‡åæ ‡ï¼Œè¿™ä¹Ÿæ˜¯ä¸Šä¸€èŠ‚offset-mappingçš„ç”±æ¥
ä¸åŒçš„é¢„åˆ†è¯gpt
1234tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)&#x27;&#x27;&#x27;[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;Ä how&#x27;, (6, 10)), (&#x27;Ä are&#x27;, (10, 14)), (&#x27;Ä &#x27;, (14, 15)), (&#x27;Ä you&#x27;, (15, 19)),(&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;&#x27;



t5
123tokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)# [(&#x27;â–Hello,&#x27;, (0, 6)), (&#x27;â–how&#x27;, (7, 10)), (&#x27;â–are&#x27;, (11, 14)), (&#x27;â–you?&#x27;, (16, 20))]



ä¸‰ç§åˆ†è¯ç®—æ³•æ€»è§ˆå¦‚ä¸Šï¼Œä¸åŒçš„æ¨¡å‹é€‚ç”¨ä¸åŒçš„åˆ†è¯ç®—æ³•
sentencepieceå®ƒç»å¸¸ä¸unigramç®—æ³•ä¸€èµ·ï¼Œä¸”å¹¶ä¸éœ€è¦é¢„åˆ†è¯ï¼Œæ˜¯ç‰¹æ”»ä¸­æ–‡æ—¥æ–‡ï¼Œè¿™ç§æ— æ³•åˆ†è¯çš„è¯­è¨€çš„
ç®—æ³•é¢„è§ˆ


Model
BPE
WordPiece
Unigram



Training
Starts from a small vocabulary and learns rules to merge tokens
Starts from a small vocabulary and learns rules to merge tokens
Starts from a large vocabulary and learns rules to remove tokens


Training step
Merges the tokens corresponding to the most common pair
Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent
Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus


Learns
Merge rules and a vocabulary
Just a vocabulary
A vocabulary with a score for each token


Encoding
Splits a word into characters and applies the merges learned during training
Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word
Finds the most likely split into tokens, using the scores learned during training


BPEç®€è¿°BPEæ˜¯Byte-Pair Encoding çš„ç®€å†™ï¼Œä»–æœ‰ä¸‰æ­¥

å°†corpusæ‰€æœ‰ç‹¬ä¸€æ— äºŒå­—ç¬¦æ‹†å‡ºæ¥ï¼Œå¦‚è‹±æ–‡ä¸­çš„26ä¸ªå­—æ¯ï¼Œæ ‡ç‚¹å’Œå…¶ä»–ç‰¹æ®Šç¬¦å·
åœ¨æœ‰åŸºç¡€å­—ç¬¦çš„åŸºç¡€ä¸Šï¼Œä»¥é¢‘ç‡ä½œä¸ºé€‰å–æ ‡å‡†ï¼Œå°†ä¸¤ä¸ªå­—ç¬¦åŒ¹é…åœ¨ä¸€èµ·ï¼Œé€‰æ‹©é¢‘ç‡æœ€é«˜çš„è¯è¿›è¡Œå…¥åº“
é‡å¤ç¬¬äºŒæ­¥ç›´åˆ°æ»¡è¶³ä½ è®¾å®šçš„è¯åº“å¤§å°


The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they donâ€™t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.
GPTå’Œrobertaä½¿ç”¨çš„æ˜¯æ¯”ç‰¹çº§åˆ«çš„å­—ç¬¦ï¼Œå°±æ˜¯0100è¿™ç§ï¼Œè¿™å°±æ˜¯ä»–ä»¬çš„åŸºç¡€è¯­æ–™åº“ï¼Œç„¶ååœ¨åŸºç¡€ä¸Šèåˆå‡ºæ¥è¯è¿›è¡Œæ„å»ºè¯åº“

å®ä¾‹ä¸‹é¢è¿›è¡Œå®ä¾‹è§£æè®¾å®šè¯­æ–™åº“å¦‚ä¸‹
è¯­æ–™åº“: &quot;hug&quot;, &quot;pug&quot;, &quot;pun&quot;, &quot;bun&quot;, &quot;hugs&quot;
è¯é¢‘: (&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
1# (&quot;h&quot; &quot;u&quot; &quot;g&quot;, 10), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, 5)



ç¬¬ä¸€è½®
æœ€å¤šçš„æ˜¯ ugçš„ç»„åˆï¼Œ20æ¬¡
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;]Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;

ç¬¬äºŒè½®
æœ€å¤šæ˜¯un
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;]Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;

ç¬¬ä¸‰è½®
æœ€å¤šçš„æ˜¯hug
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;

â€¦å¦‚ä½•å¾ªç¯åˆ°è®¾å®šçš„è¯åº“å¤§å°
ç®€è¦ä»£ç è¯­æ–™åº“
123456corpus = [    &quot;This is the Hugging Face Course.&quot;,    &quot;This chapter is about tokenization.&quot;,    &quot;This section shows several tokenizer algorithms.&quot;,    &quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;,]



ç»Ÿè®¡è¯é¢‘
12345678910111213141516171819from transformers import AutoTokenizerfrom collections import defaultdicttokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)word_freqs = defaultdict(int)for text in corpus:    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)    new_words = [word for word, offset in words_with_offsets]    for word in new_words:        word_freqs[word] += 1print(word_freqs)&#x27;&#x27;&#x27;defaultdict(int, &#123;&#x27;This&#x27;: 3, &#x27;Ä is&#x27;: 2, &#x27;Ä the&#x27;: 1, &#x27;Ä Hugging&#x27;: 1, &#x27;Ä Face&#x27;: 1, &#x27;Ä Course&#x27;: 1, &#x27;.&#x27;: 4, &#x27;Ä chapter&#x27;: 1,    &#x27;Ä about&#x27;: 1, &#x27;Ä tokenization&#x27;: 1, &#x27;Ä section&#x27;: 1, &#x27;Ä shows&#x27;: 1, &#x27;Ä several&#x27;: 1, &#x27;Ä tokenizer&#x27;: 1, &#x27;Ä algorithms&#x27;: 1,    &#x27;Hopefully&#x27;: 1, &#x27;,&#x27;: 1, &#x27;Ä you&#x27;: 1, &#x27;Ä will&#x27;: 1, &#x27;Ä be&#x27;: 1, &#x27;Ä able&#x27;: 1, &#x27;Ä to&#x27;: 1, &#x27;Ä understand&#x27;: 1, &#x27;Ä how&#x27;: 1,    &#x27;Ä they&#x27;: 1, &#x27;Ä are&#x27;: 1, &#x27;Ä trained&#x27;: 1, &#x27;Ä and&#x27;: 1, &#x27;Ä generate&#x27;: 1, &#x27;Ä tokens&#x27;: 1&#125;)&#x27;&#x27;&#x27;


é¦–å…ˆè½½å…¥gptçš„åˆ†è¯å™¨ï¼Œåšé¢„åˆ†è¯
å†è½½å…¥collectionä¸­çš„defaultdictè®¾å®šä¸ºintç±»å‹

åŸºç¡€è¯æ±‡è¡¨
123456789101112alphabet = []for word in word_freqs.keys():    for letter in word:        if letter not in alphabet:            alphabet.append(letter)alphabet.sort()print(alphabet)&#x27;&#x27;&#x27;[ &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;r&#x27;, &#x27;s&#x27;,  &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;Ä &#x27;]&#x27;&#x27;&#x27;

åŠ ä¸ªè¡¨å¤´vocab = [&quot;&lt;|endoftext|&gt;&quot;] + alphabet.copy()
å°†å•è¯æ˜ å°„ä¸º{â€˜wordâ€™: [â€˜wâ€™, â€˜oâ€™, â€˜râ€™, â€˜dâ€™]}çš„å½¢å¼è¿›è¡Œè®­ç»ƒ
1234splits = &#123;word: [c for c in word] for word in word_freqs.keys()&#125;# æˆ‘è§‰å¯ä»¥æ”¹ä¸€ä¸‹`splits = &#123;word: list(word) for word in word_freqs.keys()&#125;`



å­—æ¯å¯¹é¢‘ç‡å‡½æ•°
1234567891011121314151617181920212223242526272829303132333435363738394041def compute_pair_freqs(splits):    pair_freqs = defaultdict(int)    for word, freq in word_freqs.items():        split = splits[word]	# å–å¾—wordå¯¹åº”çš„å€¼å¦‚[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]        if len(split) == 1:            continue        for i in range(len(split) - 1):            pair = (split[i], split[i + 1])            pair_freqs[pair] += freq	# è®°å½•å­—æ¯å¯¹çš„é¢‘ç‡    return pair_freqs# ç¤ºä¾‹pair_freqs = compute_pair_freqs(splits)for i, key in enumerate(pair_freqs.keys()):    print(f&quot;&#123;key&#125;: &#123;pair_freqs[key]&#125;&quot;)    if i &gt;= 5:        break&#x27;&#x27;&#x27;(&#x27;T&#x27;, &#x27;h&#x27;): 3(&#x27;h&#x27;, &#x27;i&#x27;): 3(&#x27;i&#x27;, &#x27;s&#x27;): 5(&#x27;Ä &#x27;, &#x27;i&#x27;): 2(&#x27;Ä &#x27;, &#x27;t&#x27;): 7(&#x27;t&#x27;, &#x27;h&#x27;): 3&#x27;&#x27;&#x27;# å–æœ€å¤§å€¼best_pair = &quot;&quot;max_freq = Nonefor pair, freq in pair_freqs.items():    if max_freq is None or max_freq &lt; freq:        best_pair = pair        max_freq = freqprint(best_pair, max_freq)# (&#x27;Ä &#x27;, &#x27;t&#x27;) 7# åˆå¹¶å…¥åº“merges = &#123;(&quot;Ä &quot;, &quot;t&quot;): &quot;Ä t&quot;&#125;vocab.append(&quot;Ä t&quot;)



å°†å­—ç¬¦å¯¹æ„å»ºè¿›æ–°çš„åŸºç¡€è¯è¡¨split (ä¸æ˜¯vocab)
12345678910111213141516171819def merge_pair(a, b, splits):    for word in word_freqs:        split = splits[word] # å–å¾—wordå¯¹åº”çš„å€¼å¦‚[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]        if len(split) == 1:            continue        i = 0        while i &lt; len(split) - 1:            if split[i] == a and split[i + 1] == b:             	# æ‰¾åˆ°è¯å¯¹çš„ä½ç½®ï¼Œå°†abå­—ç¬¦ä¸²è¿æ¥èµ·æ¥ï¼Œåšä¸ªåˆ—è¡¨å­˜èµ·æ¥                split = split[:i] + [a + b] + split[i + 2 :]            else:                i += 1         splits[word] = split # æ›´æ–° [&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;] -&gt; [&#x27;wo&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]    return splitssplits = merge_pair(&quot;Ä &quot;, &quot;t&quot;, splits)print(splits[&quot;Ä trained&quot;])# [&#x27;Ä t&#x27;, &#x27;r&#x27;, &#x27;a&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]



æ„å»ºå¾ªç¯
1234567891011121314151617181920vocab_size = 50while len(vocab) &lt; v ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/37742.html" title="HF Course 07 NER QA Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 07 NER QA Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/37742.html" title="HF Course 07 NER QA Tokenizer">HF Course 07 NER QA Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T14:53:10.235Z" title="å‘è¡¨äº 2022-12-11 22:53:10">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:29:48.810Z" title="æ›´æ–°äº 2022-12-12 22:29:48">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">è®°å¾—æ’ç‰ˆ åˆ†å‰²çº¿å¾…å®Œæˆ
QAéƒ¨åˆ†


æœ¬ç« æˆ‘ä»¬éœ€è¦å¯¹åšç‰¹æ®Šçš„tokenizerä»¥é€‚åº”NERå’ŒQAä»»åŠ¡æ•°æ®çš„ç‰¹æ®Šæ€§

Fast Tokenizer1234567from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)example = &quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;encoding = tokenizer(example)print(type(encoding))# &lt;class &#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;&gt;

åˆ†è¯åè¿”å›çš„ç»“æœç±»å‹ä¸ç®€å•æ˜¯å­—å…¸çš„æ˜ å°„
è¿˜åŒ…å«å¾ˆå¤šæ–¹æ³•
123456789101112tokenizer.is_fast, encoding.is_fast(True,True)encoding.tokens()&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;, &#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]&#x27;&#x27;&#x27; encoding.word_ids()&#x27;&#x27;&#x27;[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]&#x27;&#x27;&#x27;

word_ids()æ–¹æ³•å¯çœ‹åˆ°åˆ†è¯çš„ç»“æœæ¥è‡ªå“ªä¸ªå•è¯
æœ€åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨word_to_chars() or token_to_chars() and char_to_word() or char_to_token() æŸ¥çœ‹å•è¯
123start, end = encoding.word_to_chars(3)example[start:end]# Sylvain



NER
åœ¨NERä¸­æˆ‘ä»¬ä»¥åç§»é‡çš„æ ‡è®°æ¥é”å®šåŸæ–‡çš„å­—ç¬¦

pipelineæ–¹æ³•é¦–å…ˆæŸ¥çœ‹pipelineæ–¹æ³•çš„neræµç¨‹
12345678910111213from transformers import pipelinetoken_classifier = pipeline(&quot;token-classification&quot;)token_classifier(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 35&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;, &#x27;start&#x27;: 35, &#x27;end&#x27;: 40&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;, &#x27;start&#x27;: 41, &#x27;end&#x27;: 45&#125;, &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;



ç®€æ´ç‰ˆ
12345678from transformers import pipelinetoken_classifier = pipeline(&quot;token-classification&quot;, aggregation_strategy=&quot;simple&quot;)token_classifier(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)&#x27;&#x27;&#x27;[&#123;&#x27;entity_group&#x27;: &#x27;PER&#x27;, &#x27;score&#x27;: 0.9981694, &#x27;word&#x27;: &#x27;Sylvain&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity_group&#x27;: &#x27;ORG&#x27;, &#x27;score&#x27;: 0.97960204, &#x27;word&#x27;: &#x27;Hugging Face&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 45&#125;, &#123;&#x27;entity_group&#x27;: &#x27;LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;

aggregation_strategyæœ‰ä¸åŒçš„å‚æ•°ï¼Œsimpleæ˜¯åˆ†è¯åçš„å¹³å‡åˆ†æ•°
å¦‚ä¸Šé¢çš„sylvainåˆ†æ•°æ¥è‡ª æ­£å¸¸ç‰ˆçš„å››é¡¹å¹³å‡&#39;S&#39;, &#39;##yl&#39;, &#39;##va&#39;, &#39;##in&#39;

&quot;first&quot;, where the score of each entity is the score of the first token of that entity (so for â€œSylvainâ€ it would be 0.993828, the score of the token S)
&quot;max&quot;, where the score of each entity is the maximum score of the tokens in that entity (so for â€œHugging Faceâ€ it would be 0.98879766, the score of â€œFaceâ€)
&quot;average&quot;, where the score of each entity is the average of the scores of the words composing that entity (so for â€œSylvainâ€ there would be no difference from the &quot;simple&quot; strategy, but â€œHugging Faceâ€ would have a score of 0.9819, the average of the scores for â€œHuggingâ€, 0.975, and â€œFaceâ€, 0.98879)

logitsè¿™é‡Œé€šè¿‡è¿”å›çš„ç»“æœä½¿ç”¨argmax(-1)å¾—åˆ°æ˜ å°„çš„åˆ†ç±»
123456789from transformers import AutoTokenizer, AutoModelForTokenClassificationmodel_checkpoint = &quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)example = &quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;inputs = tokenizer(example, return_tensors=&quot;pt&quot;)outputs = model(**inputs)



12345print(inputs[&quot;input_ids&quot;].shape)print(outputs.logits.shape)&#x27;&#x27;&#x27;torch.Size([1, 19])torch.Size([1, 19, 9])&#x27;&#x27;&#x27;



123456789101112131415161718import torchprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()predictions = outputs.logits.argmax(dim=-1)[0].tolist()print(predictions)# [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]model.config.id2label&#x27;&#x27;&#x27;&#123;0: &#x27;O&#x27;, 1: &#x27;B-MISC&#x27;, 2: &#x27;I-MISC&#x27;, 3: &#x27;B-PER&#x27;, 4: &#x27;I-PER&#x27;, 5: &#x27;B-ORG&#x27;, 6: &#x27;I-ORG&#x27;, 7: &#x27;B-LOC&#x27;, 8: &#x27;I-LOC&#x27;&#125;&#x27;&#x27;&#x27;



åç§»é‡postprocessingç»„ç»‡ä¸€ä¸‹æ ¼å¼ï¼Œå¤ç°ä¸Šé¢çš„å†…å®¹
123456789101112131415161718192021results = []tokens = inputs.tokens()for idx, pred in enumerate(predictions):    label = model.config.id2label[pred]    if label != &quot;O&quot;:        results.append(            &#123;&quot;entity&quot;: label, &quot;score&quot;: probabilities[idx][pred], &quot;word&quot;: tokens[idx]&#125;        )print(results)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;&#125;]&#x27;&#x27;&#x27;



åç§»é‡ offsets_mapping
123456inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)inputs_with_offsets[&quot;offset_mapping&quot;]&#x27;&#x27;&#x27;[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]&#x27;&#x27;&#x27;

è¿™é‡Œçš„19å¯¹å…ƒç»„å°±æ˜¯å¯¹åº”19ä¸ªåˆ†è¯åtokençš„ä¸‹æ ‡
1[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;,&#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]

æ¯”å¦‚(0,0)æ˜¯ç•™ç»™[CLS]çš„ï¼›	æ¯”å¦‚ç¬¬å…­ä¸ªtokenå¯¹åº”çš„æ˜¯ ##ly  é‚£ä¹ˆä»–åœ¨åŸæ–‡ä¸­çš„æ ‡æ³¨å°±æ˜¯ï¼ˆ12,14ï¼‰ï¼Œå¦‚ä¸‹
12example[12:14]# yl



ç»§ç»­æˆ‘ä»¬çš„å¤ç°pipeline
123456789101112131415161718192021222324252627282930results = []inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)tokens = inputs_with_offsets.tokens()offsets = inputs_with_offsets[&quot;offset_mapping&quot;]for idx, pred in enumerate(predictions):    label = model.config.id2label[pred]    if label != &quot;O&quot;:        start, end = offsets[idx]        results.append(            &#123;                &quot;entity&quot;: label,                &quot;score&quot;: probabilities[idx][pred],                &quot;word&quot;: tokens[idx],                &quot;start&quot;: start,                &quot;end&quot;: end,            &#125;        )print(results)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity&#x27;: &#x ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/55913.html" title="HF Course 06 ç»§æ‰¿å¼çš„Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 06 ç»§æ‰¿å¼çš„Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/55913.html" title="HF Course 06 ç»§æ‰¿å¼çš„Tokenizer">HF Course 06 ç»§æ‰¿å¼çš„Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-12-11T13:27:56.158Z" title="å‘è¡¨äº 2022-12-11 21:27:56">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-12-12T14:28:18.936Z" title="æ›´æ–°äº 2022-12-12 22:28:18">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">è¿™ç§æ–¹æ³•æ˜¯åŸºäºæ—§çš„æ¨¡å‹åˆ†è¯å™¨ä¸Šï¼Œé’ˆå¯¹ä½ çš„è¯­æ–™åº“è®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†è¯å™¨çš„æ–¹æ³•ã€‚å¤ºèˆå±äºæ˜¯
è¿™é‡Œæˆ‘ä»¬ä»¥GPTçš„åˆ†è¯å™¨ä¸ºä¾‹ï¼Œå®ƒä½¿ç”¨unigramçš„ç®—æ³•è¿›è¡Œåˆ†è¯
è½½å…¥æ•°æ®12345678910111213from datasets import load_dataset# This can take a few minutes to load, so grab a coffee or tea while you wait!raw_datasets = load_dataset(&quot;code_search_net&quot;, &quot;python&quot;)raw_datasets[&quot;train&quot;]&#x27;&#x27;&#x27;Dataset(&#123;    features: [&#x27;repository_name&#x27;, &#x27;func_path_in_repository&#x27;, &#x27;func_name&#x27;, &#x27;whole_func_string&#x27;, &#x27;language&#x27;,       &#x27;func_code_string&#x27;, &#x27;func_code_tokens&#x27;, &#x27;func_documentation_string&#x27;, &#x27;func_documentation_tokens&#x27;, &#x27;split_name&#x27;,       &#x27;func_code_url&#x27;    ],    num_rows: 412178&#125;)&#x27;&#x27;&#x27;



ç”Ÿæˆå™¨åŠ è½½æ•°æ®ä¸‹é¢çš„æ–¹æ³•ä¼šä¸€æ¬¡åŠ è½½æ‰€æœ‰æ•°æ®
12# Don&#x27;t uncomment the following line unless your dataset is small!# training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]



ä¸€èˆ¬ä½¿ç”¨pythonç”Ÿæˆå™¨
1234training_corpus = (    raw_datasets[&quot;train&quot;][i : i + 1000][&quot;whole_func_string&quot;]    for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000))

å°†åˆ—è¡¨æ¨å¯¼å¼çš„æ–¹æ‹¬å·æ¢æˆåœ†æ‹¬å·å°±å¯ä»¥å˜æˆç”Ÿæˆå™¨äº†ï¼Œå¥½å‰å®³ã€‚

123456&gt;gen = (i for i in range(10))&gt;print(list(gen))&gt;print(list(gen))&gt;&#x27;&#x27;&#x27;&gt;[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;[]&#x27;&#x27;&#x27;

ä½¿ç”¨ä¹‹åä¼šæ¸…ç©ºå†…å­˜ï¼Œå¦‚ä¸Šæ‰€ç¤º

æ›´ä¸€èˆ¬çš„ç”Ÿæˆå™¨
12345def get_training_corpus():    dataset = raw_datasets[&quot;train&quot;]    for start_idx in range(0, len(dataset), 1000):        samples = dataset[start_idx : start_idx + 1000]        yield samples[&quot;whole_func_string&quot;]



train_new_from_iterator()è½½å…¥æ¨¡å‹1234567891011121314from transformers import AutoTokenizerold_tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)example = &#x27;&#x27;&#x27;def add_numbers(a, b):    &quot;&quot;&quot;Add the two numbers `a` and `b`.&quot;&quot;&quot;    return a + b&#x27;&#x27;&#x27;tokens = old_tokenizer.tokenize(example)tokens&#x27;&#x27;&#x27;[&#x27;def&#x27;, &#x27;Ä add&#x27;, &#x27;_&#x27;, &#x27;n&#x27;, &#x27;umbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ä b&#x27;, &#x27;):&#x27;, &#x27;ÄŠ&#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä &quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ä the&#x27;, &#x27;Ä two&#x27;, &#x27;Ä numbers&#x27;, &#x27;Ä `&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ä and&#x27;, &#x27;Ä `&#x27;, &#x27;b&#x27;, &#x27;`&#x27;, &#x27;.&quot;&#x27;, &#x27;&quot;&quot;&#x27;, &#x27;ÄŠ&#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä &#x27;, &#x27;Ä return&#x27;, &#x27;Ä a&#x27;, &#x27;Ä +&#x27;, &#x27;Ä b&#x27;]&#x27;&#x27;&#x27;

This tokenizer has a few special symbols, like Ä  and ÄŠ, which denote spaces and newlines, respectivelyã€‚
ä¸¤ä¸ªGè¡¨ç¤ºç©ºæ ¼å’Œæ¢è¡Œç¬¦ã€‚ä»–è¿˜ä¸ºå¤šä¸ªç©ºæ ¼åœ¨ä¸€èµ·çš„å•ç‹¬ç¼–ç ï¼Œå¸¦ä¸‹åˆ’çº¿çš„è¯ä¹Ÿä¸è®¤è¯†ï¼Œæ‰€ä»¥ä¸å¤ªåˆé€‚ã€‚
è®­ç»ƒæ–°åˆ†è¯å™¨1tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)


æ³¨æ„ï¼Œåªæœ‰Fastçš„tokenizeræ”¯æŒtrain_new_from_iteratoræ–¹æ³•ï¼Œä»–ä»¬æ˜¯æ ¹æ®rustå†™çš„ã€‚æ²¡æœ‰fastçš„æ˜¯çº¯pythonå†™çš„ã€‚

12345tokens = tokenizer.tokenize(example)tokens&#x27;&#x27;&#x27;[&#x27;def&#x27;, &#x27;Ä add&#x27;, &#x27;_&#x27;, &#x27;numbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ä b&#x27;, &#x27;):&#x27;, &#x27;ÄŠÄ Ä Ä &#x27;, &#x27;Ä &quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ä the&#x27;, &#x27;Ä two&#x27;, &#x27;Ä numbers&#x27;, &#x27;Ä `&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ä and&#x27;, &#x27;Ä `&#x27;, &#x27;b&#x27;, &#x27;`.&quot;&quot;&quot;&#x27;, &#x27;ÄŠÄ Ä Ä &#x27;, &#x27;Ä return&#x27;, &#x27;Ä a&#x27;, &#x27;Ä +&#x27;, &#x27;Ä b&#x27;]&#x27;&#x27;&#x27;

èµ·ç å¤šä¸ªç©ºæ ¼å­¦ä¼šäº†
å­˜å‚¨æ–°åˆ†è¯å™¨1tokenizer.save_pretrained(&quot;code-search-net-tokenizer&quot;)

</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/#content-inner">5</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>ğŸ›´å‰å¾€github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>ç½‘ç«™èµ„è®¯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">æ–‡ç« æ•°ç›® :</div><div class="item-count">27</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»å­—æ•° :</div><div class="item-count">49.9k</div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™è®¿å®¢æ•° :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ¬ç«™æ€»è®¿é—®é‡ :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">æœ€åæ›´æ–°æ—¶é—´ :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-15T14:44:59.079Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è®¡ç®—æœºè§†è§‰</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">è‡ªç„¶è¯­è¨€å¤„ç†</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">è®ºæ–‡ç²¾è¯»</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">æµç•…çš„Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">13</span><span class="categoryBar-list-descr">æ‹¥æœ‰ä¸€åˆ‡ å´å˜æˆå¤ªç©º</span></li></ul></div></div>';
      console.log('å·²æŒ‚è½½butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>