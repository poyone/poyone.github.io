<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-02 10:27:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/28702.html" title="CV 02 Vit 叶子图片分类"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka3.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CV 02 Vit 叶子图片分类"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/28702.html" title="CV 02 Vit 叶子图片分类">CV 02 Vit 叶子图片分类</a><div class="article-meta-wrap"><span class="article-meta"><i class="fas fa-thumbtack sticky"></i><span class="sticky">置顶</span><span class="article-meta-separator">|</span></span><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-25T01:33:08.146Z" title="发表于 2022-11-25 09:33:08">2022-11-25</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-02T02:24:56.637Z" title="更新于 2022-12-02 10:24:56">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/CV/">CV</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/CV/">CV</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Trick/">Trick</a></span></div><div class="content">前言Vit——Vision Transformer
这里通过kaggle的叶子分类任务来使用预训练(Pre-train)模型Vit来提升我们的任务表示
1.观察模型&amp;处理数据1.1 模型探索无论是基于python的特性(适配各个领域的包)，还是NLP里大行其道的Pre-train范式，拥有快速了解一个包的特性以适用于我们工作的能力，将极大的提升我们工作的效率和结果。所以下面我们来快速体验一下HuggingFace给出的模型范例，并针对我们的任务进行相应的数据处理。
1234567891011121314151617from transformers import ViTFeatureExtractor, ViTForImageClassificationfrom PIL import Imageimport requestsurl = &#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;image = Image.open(requests.get(url, stream=True).raw)feature_extractor = ViTFeatureExtractor.from_pretrained(&#x27;google/vit-base-patch16-224&#x27;)model = ViTForImageClassification.from_pretrained(&#x27;google/vit-base-patch16-224&#x27;)inputs = feature_extractor(images=image, return_tensors=&quot;pt&quot;)outputs = model(**inputs)logits = outputs.logits# model predicts one of the 1000 ImageNet classespredicted_class_idx = logits.argmax(-1).item()print(&quot;Predicted class:&quot;, model.config.id2label[predicted_class_idx])

上面的代码可以自行运行
1.1.1 示例解读
上十行代码: 首先通过requests库拿到一张图片并用image生成图片形式，下面两行加载了Vit16的特征提取器和HF特供的图片分类适配模型

下面我们看看 特征提取后的输入(inputs)
1234567891011121314151617181920212223242526272829# inputs 输出如下&#x27;&#x27;&#x27;&#123;&#x27;pixel_values&#x27;: tensor([[[[ 0.1137,  0.1686,  0.1843,  ..., -0.1922, -0.1843, -0.1843],          [ 0.1373,  0.1686,  0.1843,  ..., -0.1922, -0.1922, -0.2078],          [ 0.1137,  0.1529,  0.1608,  ..., -0.2314, -0.2235, -0.2157],          ...,          [ 0.8353,  0.7882,  0.7333,  ...,  0.7020,  0.6471,  0.6157],          [ 0.8275,  0.7961,  0.7725,  ...,  0.5843,  0.4667,  0.3961],          [ 0.8196,  0.7569,  0.7569,  ...,  0.0745, -0.0510, -0.1922]],         [[-0.8039, -0.8118, -0.8118,  ..., -0.8902, -0.8902, -0.8980],          [-0.7882, -0.7882, -0.7882,  ..., -0.8745, -0.8745, -0.8824],          [-0.8118, -0.8039, -0.7882,  ..., -0.8902, -0.8902, -0.8902],          ...,          [-0.2706, -0.3176, -0.3647,  ..., -0.4275, -0.4588, -0.4824],          [-0.2706, -0.2941, -0.3412,  ..., -0.4824, -0.5451, -0.5765],          [-0.2784, -0.3412, -0.3490,  ..., -0.7333, -0.7804, -0.8353]],         [[-0.5451, -0.4667, -0.4824,  ..., -0.7412, -0.6941, -0.7176],          [-0.5529, -0.5137, -0.4902,  ..., -0.7412, -0.7098, -0.7412],          [-0.5216, -0.4824, -0.4667,  ..., -0.7490, -0.7490, -0.7647],          ...,          [ 0.5686,  0.5529,  0.4510,  ...,  0.4431,  0.3882,  0.3255],          [ 0.5451,  0.4902,  0.5137,  ...,  0.3020,  0.2078,  0.1294],          [ 0.5686,  0.5608,  0.5137,  ..., -0.2000, -0.4275, -0.5294]]]])&#125;          &#x27;&#x27;&#x27;inputs[&#x27;pixel_values&#x27;].size()# torch.Size([1, 3, 224, 224])

可以看到是一个字典类型的tensor数据，其维度为(b, C, W, H)
因此我们喂给模型的数据也得是四维的结构

接下来看看模型吐出来的结果
1234567891011121314151617181920212223242526# outputs 输入如下&#x27;&#x27;&#x27;MaskedLMOutput(loss=tensor(0.4776, grad_fn=&lt;DivBackward0&gt;), logits=tensor([[[[-0.0630, -0.0475, -0.1557,  ...,  0.0950,  0.0216, -0.0084],          [-0.1219, -0.0329, -0.0849,  ..., -0.0152, -0.0143, -0.0663],          [-0.1063, -0.0925, -0.0350,  ...,  0.0238, -0.0206, -0.2159],          ...,          [ 0.2204,  0.0593, -0.2771,  ...,  0.0819,  0.0535, -0.1783],          [-0.0302, -0.1537, -0.1370,  ..., -0.1245, -0.1181, -0.0070],          [ 0.0875,  0.0626, -0.0693,  ...,  0.1331,  0.1088, -0.0835]],         [[ 0.1977, -0.2163,  0.0469,  ...,  0.0802, -0.0414,  0.0552],          [ 0.1125, -0.0369,  0.0175,  ...,  0.0598, -0.0843,  0.0774],          [ 0.1559, -0.0994, -0.0055,  ..., -0.0215,  0.2452, -0.0603],          ...,          [ 0.0603,  0.1887,  0.2060,  ...,  0.0415, -0.0383,  0.0990],          [ 0.2106,  0.0992, -0.1562,  ..., -0.1254, -0.0603,  0.0685],          [ 0.0256,  0.1578,  0.0304,  ..., -0.0894,  0.0659,  0.1493]],         [[-0.0348, -0.0362, -0.1617,  ...,  0.0527,  0.1927,  0.1431],          [-0.0447,  0.0137, -0.0798,  ...,  0.1057, -0.0299, -0.0742],          [-0.0725,  0.1473, -0.0118,  ..., -0.1284,  0.0010, -0.0773],          ...,          [-0.0315,  0.1065, -0.1130,  ...,  0.0091, -0.0650,  0.0688],          [ 0.0314,  0.1034, -0.0964,  ...,  0.0144,  0.0532, -0.0415],          [-0.0205,  0.0046, -0.0987,  ...,  0.1317, -0.0065, -0.1617]]]],       grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None) &#x27;&#x27;&#x27;

可以看到有loss、logits、hidden_states、attentions，而我们的范例只取了logits作为结果输出。这里并不是说其他的部分没用，是只取适配下游任务的输出即可。详情可研究Vit的论文

最后通过argmax函数和model.config.id2label得出标签相对应的文字
argmax就是返回最大值的位置下标、model.config.id2label配置了对应标签的名称，也知道了最后的classifier层是1000维的


1.1.2 小结通过以上探索，我们可以得出：

输入的维度为(batch_size, 3, 224, 224)
最后的classifier需由1000改成我们叶子的类别数

1.2 数据处理接下来我们将探索数据的特性，并修改以适应我们的模型
1.2.1 EDA即Exploratory Data Analysis
首先导入所需包
12345678910111213141516171819202122232425# 导入各种包import torchimport torch.nn as nnfrom torch.nn import functional as Fimport randomimport copyfrom fastprogress.fastprogress import master_bar, progress_barfrom torch.cuda.amp import autocastimport pandas as pdimport numpy as npfrom torch.utils.data import Dataset, DataLoader, TensorDatasetfrom torchvision import transformsfrom sklearn.model_selection import KFoldfrom PIL import Imageimport osimport matplotlib.pyplot as pltimport torchvision.models as modelsfrom torch.optim.lr_scheduler import CosineAnnealingLRfrom transformers import (AdamW, get_scheduler)from transformers import ViTFeatureExtractor, ViTForImageClassification

查看初始数据
train_df = pd.read_csv(&#39;/kaggle/input/classify-leaves/train.csv&#39;)


使用下面代码给分类配上序号
123456789101112def num_map(file_path):    data_df = pd.read_csv(&#x27;/kaggle/input/classify-leaves/train.csv&#x27;)        categories = data_df.label.unique().tolist()    categories_zip = list(zip( range(len(categories)) , categories))    categories_dict = &#123;v:k for k, v in categories_zip&#125;        data_df[&#x27;num_label&#x27;] = data_df.label.map(categories_dict)        return data_dfshow_df = num_map(&#x27;/kaggle/input/classify-leaves/train.csv&#x27;)show_df.to_csv(&#x27;train_valid_dataset.csv&#x27;)





1.2.2 图片数据查看12345678path = &#x27;/kaggle/input/classify-leaves/&#x27;img = Image.open(path+train_df.image[1])# plt.figure(&quot;Image&quot;) # 图像窗口名称plt.imshow(img)plt.axis(&#x27;off&#x27;) # 关掉坐标轴为 offplt.title(&#x27;image&#x27;) # 图像题目plt.show()



这里我们做一下维度转换 即 [0, 1, 2]  换成 [2, 1, 0], 并只取某一个通道 看看
12345# np.asarray(img).shape# 可以看到图片反了，正确的顺序是.transpose([2, 0, 1])img_trans = np.asarray(img).transpose([2,1,0])plt.imshow(img_trans[0])plt.show()





2.Preprocessing接下来我们分别要做 数据增强、数据类定义、数据加载器测试
2.1.1 先来算个平均值标准差这里算的mean跟std是为了Normalize我们的数据使训练更稳定
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import osimport cv2import numpy as npimport mathdef get_image_list(img_dir, isclasses=False):    &quot;&quot;&quot;将图像的名称列表    args: img_dir:存放图片的目录          isclasses:图片是否按类别存放标志    return: 图片文件名称列表    &quot;&quot;&quot;    img_list = []    # 路径下图像是否按类别分类存放    if isclasses:        img_file = os.listdir(img_dir)        for class_name in img_file:            if not os.path.isfile(os.path.join(img_dir, class_name)):                     class_img_list = os.listdir(os.path.join(img_dir, class_name))                img_list.extend(class_img_list)             else:        img_list = os.listdir(img_dir)    print(img_list)    print(&#x27;image numbers: &#123;&#125;&#x27;.format(len(img_list)))    return img_listdef get_image_pixel_mean(img_dir, img_list, img_size):    &quot;&quot;&quot;求数据集图像的R、G、B均值    args: img_dir:          img_list:          img_size:    &quot;&quot;&quot;    R_sum = 0    G_sum = 0    B_sum = 0    count = 0    # 循环读取所有图片    for img_name in img_list:        img_path = os.path.join(img_dir, img_name)        if not os.path.isdir(img_path):            image = cv2.imread(img_path)            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)            image = cv2.resize(image, (img_size, img_size))      # &lt;class &#x27;numpy.ndarray&#x27;&gt;            R_sum += image[:, :, 0].mean()            G_sum += image[:, :, 1].mean()            B_sum += image[:, :, 2].mean()            count += 1    R_mean = R_sum / count    G_mean = G_sum / count    B_mean = B_sum / count    print(&#x27;R_mean:&#123;&#125;, G_mean:&#123;&#125;, B_mean:&#123;&#125;&#x27;.format(R_mean,G_mean,B_mean))    RGB_mean = [R_mean, G_mean, B_mean]    return RGB_meandef get_image_pixel_std(img_dir, img_mean, img_list, img_size):    R_squared_mean = 0    G_squared_mean = 0    B_squared_mean = 0    count = 0    image_mean = np.array(img_mean)    # 循环读取所有图片    for img_name in img_list:        img_path = os.path.join(img_dir, img_name)        if not os.path.isdir(img_path):            image = cv2.imread(img_path)    # 读取图片            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)            image = cv2.resize(image, (img_size, img_size))      # &lt;class &#x27;numpy.ndarray&#x27;&gt;            image = image - image_mean    # 零均值            # 求单张图片的方差            R_squared_mean += np.mean(np.square(image[:, :, 0]).flatten())            G_squared_mean += np.mean(np.square(image[:, :, 1]).flatten())            B_squared_mean += np.mean(np.square(image[:, :, 2]).flatten())            count += 1    R_std = math.sqrt(R_squared_mean / count)    G_std = math.sqrt(G_squared_mean / count)    B_std = math.sqrt(B_squared_mean / count)    print(&#x27;R_std:&#123;&#125;, G_std:&#123;&#125;, B_std:&#123;&#125;&#x27;.format(R_std, G_std, B_std))    RGB_std = [R_std, G_std, B_std]    return R ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/53023.html" title="CV 01 CNN MNIST识别"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka3.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CV 01 CNN MNIST识别"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/53023.html" title="CV 01 CNN MNIST识别">CV 01 CNN MNIST识别</a><div class="article-meta-wrap"><span class="article-meta"><i class="fas fa-thumbtack sticky"></i><span class="sticky">置顶</span><span class="article-meta-separator">|</span></span><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-23T06:44:36.261Z" title="发表于 2022-11-23 14:44:36">2022-11-23</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-27T05:35:20.706Z" title="更新于 2022-11-27 13:35:20">2022-11-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/CV/">CV</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/CV/">CV</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Pytorch-%E5%9F%BA%E7%A1%80/">Pytorch 基础</a></span></div><div class="content">前言本文将通过CNN 让计算机识别MNIST数据集中的手写数字 以此来介绍Pytorch的基本使用方法：

Pytorch中的数据类型——tensor
Pytorch中的数据集、数据加载器——Dataset、DataLoader
Pytorch中的基础类模型——torch.nn.Module

以及程序设计上的一些小技巧。
1. tensor1.1 概念tensor一词译为张量，一般我们所接触的矩阵是二维的，称为二阶张量、向量称为一阶张量、标量称为零阶张量。接下来我们通过Numpy库了解一下张量。（这里并非数学上严格的定义，感性理解一下即可）
1.1.1 二阶张量 矩阵123456789101112131415161718192021222324# 首先我们举一个三行八列的矩阵import numpy as npa = np.arange(1,25).reshape(3,8)&#x27;&#x27;&#x27;array([[ 1,  2,  3,  4,  5,  6,  7,  8],       [ 9, 10, 11, 12, 13, 14, 15, 16],       [17, 18, 19, 20, 21, 22, 23, 24]])&#x27;&#x27;&#x27;b = np.ones_like(a).T&#x27;&#x27;&#x27;	array([[1, 1, 1],           [1, 1, 1],           [1, 1, 1],           [1, 1, 1],           [1, 1, 1],           [1, 1, 1],           [1, 1, 1],           [1, 1, 1]])&#x27;&#x27;&#x27;          # 我们创建以上两个矩阵，接下来我们把他们做点乘a@b&#x27;&#x27;&#x27;	array([[ 36,  36,  36],           [100, 100, 100],           [164, 164, 164]]) &#x27;&#x27;&#x27;

上面我们创建了两个矩阵a为三行八列，b为八行三列，两者做点积得到一个三行三列的矩阵。
我们拉到列表的角度解释这个矩阵，我们将所有数据都包含在一个大列表之内，大列表里有三个小列表，每个列表内有八个元素，
即三个小列表代表三行，每个列表的八个元素代表八个维度。

这里举个小栗子帮助理解一下维度：
我们在三年级二班给各位同学做信息登记，每个人所需要填写【姓名、年龄、身高、体重】四项内容，我们把每个人的信息记为一条数据，那么我们就可以说这条数据有四个特征，它的维度为四。
通常来讲我们把特征记为feature，称这条数据有四个特征。
现在整个班级的信息都填写好了应该是如何的形状，我们假设有32个人：
【【张三、7、130、 70】
​	【李四、7、131、 71】
​	  。。。
​	【李小明、7、129、70】】 如何我们得到一个32行4列的矩阵，记为（32,4）
接下来我们把视角拉倒整个三年级，我们假设有7个班级：
那么我们得到的数据维度应该是（7,32,4），表示我们有7个班级的数据，每个班级的数据维度（32,4）。
此后我们都称这个“班级为batch“ ，至此我们从二维的矩阵上升到三维的张量。

1.1.2 张量下面我们将介绍常用的张量形式
1234567891011121314151617181920212223242526272829303132333435363738394041424344# 首先介绍下一张图片通常的数据格式，我们使用Numpy来伪造一下数据np.random.randn(1,3,28,28)&#x27;&#x27;&#x27;array([[[[-1.03301822, -0.26956785, -1.81246034, ..., -0.2025034 ,          -0.24770628,  0.45183312],         [ 1.09102807,  0.92955389,  0.07537901, ...,  0.69203358,          -0.17726632, -0.74610015],         [ 0.40508712, -1.2507095 ,  0.68702445, ..., -0.12526432,           0.0390443 ,  1.00993313],         ...,         [ 1.96843042, -2.43286678,  0.08543089, ..., -1.57232148,           0.92844287, -0.25532137],         [ 0.46919141, -0.13700029,  1.78645959, ...,  0.01334257,           1.31030895, -0.22523819],         [ 0.63897933,  0.54846445, -0.64030391, ...,  0.92298892,          -0.50840421,  1.34232325]],        [[-0.01892086,  0.1456131 , -0.08903806, ...,  1.68250139,           1.2097305 , -0.2680935 ],         [ 0.92759263,  0.22665021,  1.28734004, ...,  0.09925943,           1.30039407,  3.34710594],         [ 0.53486942, -0.56230181, -1.92117215, ...,  1.33047469,          -1.19211895, -0.03081918],         ...,         [ 0.2539067 , -2.13160564,  0.27519544, ..., -0.62223126,           0.5818296 ,  0.07102949],         [-0.7524386 , -0.71244818,  0.88997093, ...,  0.16566338,           0.80577231, -3.35350436],         [ 0.99558393, -2.32335969, -2.87512549, ...,  1.16290939,           2.24089232,  0.22083378]],        [[ 1.35970859,  0.7961136 ,  0.09896652, ...,  1.82609401,          -0.49607535,  0.23424012],         [-0.22283053, -1.35535905, -0.55896315, ...,  1.68093489,           0.80969216,  0.63538616],         [-0.88285682,  0.59389887, -1.05559301, ..., -0.00719476,          -0.25654492, -1.40716977],         ...,         [ 0.44508688, -0.05650302, -2.97674436, ...,  1.25730001,          -1.66409024,  0.96057644],         [-1.3237267 , -0.27798159, -1.8947621 , ...,  1.96216661,          -0.10569547, -0.8446272 ],         [ 0.22525617,  0.75040916,  0.72823974, ..., -1.93525763,          -0.74464397,  0.55771249]]]]) &#x27;&#x27;&#x27;

上面就是一张图片**W(width)为28，H(heigh)**为28，有RGB三个通道，batch为1的图片(这个batch里面只有一张图片)的数据表示形式。
当然上面的数据太过复杂，我们以下面W和H都为4的数据继续讲解一下各个数据的意义。
123456789101112131415np.random.randn(1,3,4,4)&#x27;&#x27;&#x27;array([[[[ 0.43748082, -0.65000689,  0.13972451, -0.40213376],            [ 0.09342289, -0.83655856,  0.51844492,  0.96505144],            [ 0.68421876,  1.05527391, -0.30821748, -1.89826909],            [-0.36654524,  0.22642376,  0.16545107,  0.00401234]],            [[-0.13032482,  0.68182741, -0.52511016,  0.75875314],             [-1.39072336, -0.22848391, -1.64733525,  0.3339502 ],             [-1.06568103, -0.58455172, -0.02874822, -0.64499225],             [-0.23380602, -0.74809941, -0.71214339, -0.44950305]],            [[-1.51112191,  0.49145194, -0.01839728, -1.52788219],             [ 0.93370593,  0.96444176, -0.67434299, -1.8492484 ],             [ 0.51140855, -0.58682968, -1.16261225, -0.65782238],             [ 0.8643421 ,  0.79983446, -0.92330871, -2.45649675]]]]) &#x27;&#x27;&#x27;


一号位batch&#x3D;1表示只有一张图片

若batch&#x3D;3，我们下面降到的模型依次取本批次内【0】号(3,4,4)、【1】(3,4,4)、【2】(3,4,4)进行处理


二号位Channel&#x3D;3 表示有三个通道分别是RGB

如上面 0.43748082….0.00401234，就表示在R通道内，这张图片的颜色数据
G和B通道同理，让三者叠加就可以表示颜色的明暗，从而勾勒画面，渲染颜色


最后的两位就表示长宽，每个元素表示像素点的明暗程度。如R通道的第一个元素0.43748082就表示这个像素点有多红
(红也有程度对吧)


1.2 Pytorch中tensor的APIPytorch中tensor号称是跟Numpy及其相似的操作方式，有Numpy的学习基础的话几乎不用付出学习成本来适应tensor。但是实际情况就经常出现各种警告。无论如何，tensor可以享受到GPU的加速运算，总的来说也够友好，下面我们将介绍其常用的API
首先是随机函数，基本跟Numpy是一样的。
12345678910111213141516rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)&#x27;&#x27;&#x27;Random Tensor: tensor([[0.7453, 0.7993, 0.8484],        [0.3592, 0.3243, 0.7226]])Ones Tensor: tensor([[1., 1., 1.],        [1., 1., 1.]])Zeros Tensor: tensor([[0., 0., 0.],        [0., 0., 0.]]) &#x27;&#x27;&#x27;

接下来介绍tensor对象的一些属性，其中device默认就是使用cpu，表示我们数据在cpu上。
1234567891011tensor = torch.rand(3,4)&#x27;&#x27;&#x27;tensor([[0.8165, 0.1909, 0.6631, 0.3062],           [0.0178, 0.5158, 0.0267, 0.9819],           [0.6103, 0.7354, 0.7933, 0.2770]]) &#x27;&#x27;&#x27;tensor.shape   		# 将返回 torch.Size([3, 4])tensor.dtype		# 将返回 torch.float32tensor.device		# 将返回 cpudevice = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;		# 这句话经常来指定数据处理的设备

torch.cat也是一个常用的函数，用来链接数据。
下面以第一行为例，cat函数将[0.8165, 0.1909, 0.6631, 0.3062]与[0.8165, 0.1909, 0.6631, 0.3062]、[0.8165, 0.1909, 0.6631, 0.3062,]连接，这是因为dim&#x3D;1表示在第一维度，其视角内的可操作单位为0.8165, 0.1909, 0.6631, 0.3062这些元素，dim&#x3D;0则可操作的基本单位为tensor(这里的tensor表示上面的三行四列的实例张量)
123456789torch.cat([tensor, tensor, tensor], dim=1)&#x27;&#x27;&#x27;tensor([[0.8165, 0.1909, 0.6631, 0.3062, 0.8165, 0.1909, 0.6631, 0.3062, 0.8165,         0.1909, 0.6631, 0.3062],        [0.0178, 0.5158, 0.0267, 0.9819, 0.0178, 0.5158, 0.0267, 0.9819, 0.0178,         0.5158, 0.0267, 0.9819],        [0.6103, 0.7354, 0.7933, 0.2770, 0.6103, 0.7354, 0.7933, 0.2770, 0.6103,         0.7354, 0.7933, 0.2770]]) &#x27;&#x27;&#x27;

另外介绍一下常用的类型转换
123456t = torch.rand(3,4)n = np.random.randn(3,4)t.to_list()					# 将tensor类型转换为listt.numpy()					# 转换为Numpy类型torch.from_numpy(n)			# 从Numpy转换为tensor

最后最常用的就是下面两句
1234data = list(range(1,10))torch.tensor(data) 				# 返回tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])可使用dtype=torch.float32换成浮点数torch.Tensor(data)				# 返回 tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])



2. Dataset DataLoader 都说数据科学家一般的时间都花在数据处理上，一点不假。前面花了这么大篇幅讲价tensor，接下来我们将介绍Pytorch中存储数据的
Dataset和数据加载器DataLoader
2.1 你的数据类虽然我们使用的MNIST数据集已经可以直接通过Pytorch的API调用，如下
from torchvision import datasets
datasets.MNIST(root=&#39;../dataset/mnist/&#39;, train=True, download=True, transform=transform)

root表示存储或者加载数据的路径
train表示是否只加载训练部分的数据集，不设定默认加载全部数据集
download字面意思
transform指代这批数据使用什么转换形式，一般来说是一种数据增强方式，以后会专门介绍

我们还是来具体解释下通常要自定义使用的dataset。
定义符合你要求的数据集有三步必须操作：

定义你自己的数据集类并继承自torch.utils.data.Dataset
需要包含__len__方法返回长度
需要包含__getitem__方法，按照下标取得数据

以上配置也都是为了配合DataLoader的使用，下面我们定义一个dataset类
1234567891011121314151617181920from torch.utils.data import Datasetclass TitanicDataSets(Dataset):    def __init__(self,flag):        xy = preprocess(pd.read_csv(&quot;Titanic.csv&quot;), flag=&quot;train&quot;)                if flag == &quot;train&quot;:            self.x_data = xy.iloc[:, :-1][:800]            self.y_data = xy.iloc[:, -1][:800]            self.len = self.x_data.shape[0]        if flag == &quot;valid&quot;:            self.x_data = xy[0][800:892]            self.y_data = xy[1][800:892]            self.len = self.x_data.shape[0]    def __getitem__(self, index):        return self.x_data[index], self.y_data[index]    def __len__(self):        return self.len

上面我们引入kaggle著名的泰坦尼克号幸存者预测比赛使用的数据集，其中x_data获得的是前八百行乘客的信息，y_data记录的就是是否存活。

一般来说我们也将数据集分为train和valid两部分，因为最后我们需要预测的数据集并不会有是否存活的标签，所以通过训练模型参数以拟合train部分的数据，以valid为本次训练的结果导向以修正模型参数，最终预测，就是我们的目的。

如上面所示，Python中的语句就是这么简洁明了，我们在初始部分读取数据集，然后根据传入的flag决定是处理train还是valid的部分数据，最后我们赋予这个类像列表那样的获取下标和切片能力(__getitem__方法)、以及返回长度的能力(__len__方法)
2.2 数据加载器Pytorch的数据加载器DataLoader简单易用，下面介绍它部分常用参数。
12train_dataset = TitanicDataSets(flag=&quot;train&quot;)train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)


dataset 表示它所处理的数据，一般是你定义的dataset类，或者具有下标取值，和返回长度的数据类型也可以
batch_size 表示一词传给模型多少条数据
shuffle 表示是否打乱
num_workers 表示使用你cpu的几个核进行读取

可以使用下面的语句查看dataloader返回给你的数据形状
12samples = next(iter(train_loader))samples[:2]								# 查看本批次(batch)的前两个样本([0]号，[1]号)



3. 模型对于模型，以我的理解，数据虽然是死的，但是理解它的方式是活的；模型是活的，但是组合它的方式并没有那么灵活。这里之所以说是组合，说点题外话，是因为如今预训练模型大行其道，大模型在各个任务上不断刷新纪录(SOTA)，小型机构很难有力量去训练这种大模型，于是在大模型上修修改改以适应下游任务的方式，只能使用这种像是Transformer的方式不断变形组合，总感觉缺了点活力。(奠定Pre-train的Bert就是在Transformer基础上提出来的)。
3.1 模型定义下面开始定义我们的CNN模型
123456789101112131415161718class Net(torch.nn.Module):    def __init__(self):        super().__init__()        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=3, padding=1)        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=3, padding=1)        self.pooling = torch.nn.MaxPool2d(2)        self.fc = torch.nn.Linear(320, 10)      def forward(self, x):        # flatten dat ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/38639.html" title="Pandas 工作流"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris42.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pandas 工作流"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/38639.html" title="Pandas 工作流">Pandas 工作流</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-02T01:41:38.674Z" title="发表于 2022-12-02 09:41:38">2022-12-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-02T02:10:02.237Z" title="更新于 2022-12-02 10:10:02">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Python/">Python</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Pandas/">Pandas</a></span></div><div class="content">待完成
示例引入

map applymap apply
map apply都是对series操作
map只能操作 len count 基础函数
apply 能传入自定义函数 以axis&#x3D;1或者0分别按一个series操作


applymap 对DataFrame的地图炮
applymap对所有元素统一操作



str + apply
pandas对象带的对字符串函数
apply自定义处理字符串函数

sort
sort_values
by 哪一列
key + lambda 对值处理后的排序
Apply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.





groupby + apply
groupby	分组，返回一个group对象 配合apply使用
transform 分组处理返回值，并保持表格不变

拼接
merge
join 
how&#x3D; inner left right outer


cat

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/13310.html" title="Transformer From Scratch"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris33.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer From Scratch"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/13310.html" title="Transformer From Scratch">Transformer From Scratch</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-01T15:53:44.718Z" title="发表于 2022-12-01 23:53:44">2022-12-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-01T16:06:26.341Z" title="更新于 2022-12-02 00:06:26">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Dive-Into-Paper/">Dive Into Paper</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">待完成
pad使用torch.nested
mask的使用

架构分四个大块

encoder 
encoder-layer


decoder
decoder-layer



细节三种mask

encoder-mask
decoder-mask
cross-mask （en-decoder mask）

Embedding句子表示为 [token1， token2, …tokens]
句子1 &#x3D; [token_1， token_2, …token_x]
句子2 &#x3D; [token_1， token_2, …token_y]   x 不一定等于 y
token 构造123456789101112131415161718192021222324252627282930313233343536373839404142import torch.nn.functional as Fimport torchsrc_vocab_size = 16tgt_vocab_size = 16batch_size = 4max_len = 6src_len = torch.randint(2,7, (batch_size,))tgt_len = torch.randint(2,7, (batch_size,))&#x27;&#x27;&#x27;结果如下tensor([8, 6, 5, 9]) 此batch的第一个句子长8，第二个6tensor([6, 5, 9, 5])&#x27;&#x27;&#x27;# 接下来我们把不定长的句子padding&#x27;&#x27;&#x27;randint 生成(i,)形状的数据padding 0次或者 max_len - len(i) 的次数unsqueeze 增加一个维度&#x27;&#x27;&#x27;src_seq = [F.pad(torch.randint(1, src_vocab_size, (i,)), (0, max_len-i)).unsqueeze(0) for i in src_len]tgt_seq = [F.pad(torch.randint(1, tgt_vocab_size, (i,)), (0, max_len-i)).unsqueeze(0) for i in tgt_len]# 将整个batch的句子整合src_seq = torch.cat(src_seq)tgt_seq = torch.cat(tgt_seq)&#x27;&#x27;&#x27;如下tensor([[12, 15, 10,  5,  3, 14],        [ 5,  7,  9,  3, 12,  1],        [ 3,  1,  1,  9,  3,  4],        [ 9,  6,  0,  0,  0,  0]])        tensor([[11, 12, 11,  3,  5, 15],        [ 7,  9, 11,  0,  0,  0],        [12,  6, 13, 11,  0,  0],        [13,  3,  0,  0,  0,  0]])&#x27;&#x27;&#x27;



词向量空间 映射123456789101112131415161718192021222324252627282930import torch.nn as nnd_model = 8src_embedding = nn.Embedding(src_vocab_size+1, d_model)tgt_embedding = nn.Embedding(tgt_vocab_size+1, d_model)src_embedding.weight # shape为(17, 8)&#x27;&#x27;&#x27;第零行为pad的数据Parameter containing:tensor([[ 2.4606,  1.7139, -0.2859, -0.5058,  0.6229, -0.0470,  2.1517,  0.2996],        [ 0.0077, -0.4292, -0.2397,  1.2366, -0.3061,  0.9196, -1.4222, -1.6431],        [-0.6378, -0.7809, -0.4206,  0.5759, -1.4899,  1.2241,  0.9220, -0.6333],        [ 0.0303, -1.4113,  0.9164, -0.1200,  1.7224, -0.4996, -1.6708, -1.8563],        [ 0.0235,  0.0155, -0.1292, -0.9274, -1.1351, -0.9155,  0.4391, -0.0437],        [ 0.8498,  0.4709, -0.9168, -2.1307,  0.1840,  0.3554, -0.3986,  1.2806],        [ 0.7256,  1.2303, -0.8280, -0.2173,  0.8939,  2.4122,  0.4820, -1.9615],        [-0.8607,  2.4886, -0.8877, -0.8852,  0.3905,  0.9511, -0.3732,  0.4872],        [ 0.4882, -0.4518, -0.1945,  0.2857, -0.6832, -0.4870, -1.7165, -2.0987],        [-0.0512,  0.2692, -1.0003,  0.7896,  0.5004,  0.3594, -1.5923, -1.5618],        [ 0.4012,  0.1614,  1.8939,  0.3862, -0.6733, -1.2442, -0.6540, -1.6772],        [ 1.4784,  2.7430,  0.0159,  0.5944, -1.0025,  1.0843,  0.4580, -0.6515],        [ 0.3905,  0.6118, -0.1256, -0.6725,  1.2366,  0.8272,  0.0838, -1.5124],        [-0.1470,  0.2149, -1.4561,  1.8008,  0.7764, -0.8517, -0.3204, -0.2550],        [-1.1534, -0.6837, -1.7165, -1.7905, -1.5423,  1.8812, -0.1794, -0.2357],        [ 1.3046,  1.5021,  1.4846,  1.0622,  1.4066,  0.7299,  0.7929, -1.0107],        [-0.3920,  0.7482,  1.5976,  1.7429, -0.4683,  0.2286,  0.1320, -0.5826]],       requires_grad=True)&#x27;&#x27;&#x27;scr_embedding(scr_seq[0]) # 将取出对应的[12, 15, 10,  5,  3, 14]行



Position Embedding按公式写就行
123456789101112131415pe = torch.zeros(max_len, d_model)pos = torch.arange(0, max_len).unsqueeze(1) # 形状为(max_len, 1)idx = torch.pow(10000, torch.arange(0, 8, 2).unsqueeze(0)/ d_model )  # 形状为 (1, 4)pe[:, 0::2] = torch.sin(pos / idx)	# 触发广播 (max_len, 4)pe[:, 1::2] = torch.sin(pos / idx)&#x27;&#x27;&#x27;此处演示只加奇数列的效果tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],        [ 0.8415,  0.0000,  0.0998,  0.0000,  0.0100,  0.0000,  0.0010,  0.0000],        [ 0.9093,  0.0000,  0.1987,  0.0000,  0.0200,  0.0000,  0.0020,  0.0000],        [ 0.1411,  0.0000,  0.2955,  0.0000,  0.0300,  0.0000,  0.0030,  0.0000],        [-0.7568,  0.0000,  0.3894,  0.0000,  0.0400,  0.0000,  0.0040,  0.0000],        [-0.9589,  0.0000,  0.4794,  0.0000,  0.0500,  0.0000,  0.0050,  0.0000]])&#x27;&#x27;&#x27;



Multi-head Attention123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import torchimport torch.nn as nnimport mathimport copydef clones(module, N):    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])class MultiheadAttention(nn.Module):    def __init__(self, d_model, n_head, QKV_O = 4, drop_rate=0.1):        super().__init__()        assert d_model % n_head == 0        self.d_model = d_model        self.n_head = n_head        self.d_k = d_model // n_head        self.dropout = nn.Dropout(p=drop_rate)        self.linear = clones(nn.Linear(d_model, d_model) ,QKV_O)        def forward(self, inputs, mask=None):        batch_size = inputs.shape[0]        # if mask is not None:        #     mask = torch.triu(torch.ones(inputs), diagonal=1)        #     mask = mask.squeeze()        data = inputs  # (batch_size, channel, w, h)         Q_heads = self.linear[0](data).view(batch_size, -1, self.n_head, self.d_k)        K_heads = self.linear[1](data).view(batch_size, -1, self.n_head, self.d_k)        V_heads = self.linear[2](data).view(batch_size, -1, self.n_head, self.d_k)        V_att, scores_att= ScaledAttention(Q_heads, K_heads, V_heads, self.d_k,  mask, self.dropout)        V_att = V_att.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_k)        V_att = self.linear[3](V_att)        K_lin = K_heads.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_k)        return  K_lin, V_att, # scores_attdef ScaledAttention(query, key, value, d_k, mask=None, dropout=None):    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # (batch_size, channel, head, d_k)    if mask is not None:        scores = scores.masked_fill(mask == 0, -1e9)    scores = scores.softmax(dim=-1)    if dropout is not None:        scores = dropout(scores)    value = torch.matmul(scores, value)    return value, scoresclass MaskedMultiheadAttention(nn.Module):    def __init__(self, d_model, n_head, QKV_O = 4, drop_rate=0.1):        super().__init__()        assert d_model % n_head == 0        self.d_model = d_model        self.n_head = n_head        self.d_k = d_model // n_head        self.dropout = nn.Dropout(p=drop_rate)        self.linear = clones(nn.Linear(d_model, d_model) ,QKV_O)        def forward(self, Q, K, V, mask=None):        batch_size = Q.shape[0]        if mask is not None: # 这里加入 encoder的K,V            Q_heads = self.linear[0](Q.flatten(-2, -1)).view(batch_size, -1, self.n_head, self.d_k)            K_heads = self.linear[1](K.flatten(-2, -1)).view(batch_size, -1, self.n_head, self.d_k)            V_heads = self.linear[2](V.flatten(-2, -1)).view(batch_size, -1, self.n_head, self.d_k)            Q_mask = Q_heads + mask.view(batch_size, -1, self.n_head, self.d_k)                        scores = torch.matmul(Q_mask, K_heads.transpose(-2, -1)) / math.sqrt(self.d_k)            scores = scores.softmax(dim=-1)            V_att = torch.matmul(scores, V_heads)            V_att = V_att.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_k)            V_att = self.linear[3](V_att)            return V_att                else :      # 这里所有的data都是由decoder进来的            data = Q.flatten(-2, -1)  # (batch_size, channel, w, h)            Q_heads = self.linear[0](data).view(batch_size, -1, self.n_head, self.d_k)            K_heads = self.linear[1](data).view(batch_size, -1, self.n_head, self.d_k)            V_heads = self.linear[2](data).view(batch_size, -1, self.n_head, self.d_k)            V_att, scores_att= ScaledAttention(Q_heads, K_heads, V_heads, self.d_k,  mask, self.dropout)            V_att = V_att.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_k)            V_att = self.linear[3](V_att)            decoder_Q = V_att            return decoder_Qif __name__ == &#x27;__main__&#x27;:    source = torch.randn(4, 1, 28, 28)    mul = MultiheadAttention(784, 7)    mul(source)



Encoder Layerencoder_mask构建一个与scores维度相同的mask 通过scores.masked_fill(mask==0, -1e9)  将mask上为0的位置，映射到scores上填充为-1e9

scores矩阵由Q K矩阵相乘获得

Q (batch_size, src_max_len, d_model) 经多头映射为 (batch_size, -1, n_head, d_model&#x2F; n_head), 
最后交换做成(batch_size, n_head, -1, d_model&#x2F; n_head)

K的维度与Q相同， 然后在缩放点积注意力中交换最后两个维度，最后与Q相乘

得到 scores 的维度为, (batch_size, n_head, -1,  d_model&#x2F; n_head) @ (batch_size, n_head, d_model&#x2F; n_head , -1 )



src_mask 可以做成维度(batch_siez, scr_len, tgt_len)的维度


代码
1234567891011121314151617181920212223242526272829303132333435363738394041import torchimport torch.nn as nnfrom Multihead_Attention import MultiheadAttention, clonesclass EncoderLayer(nn.Module):    def __init__(self, d_model, n_head, drop_rate=0.1):        super().__init__()        self.mul = MultiheadAttention(d_model, n_head, drop_rate=drop_rate)        self.norm = nn.LayerNorm(d_model)        self.ff = nn.Sequential( nn.Linear(d_model, d_model*4),                                 nn.ReLU(),                                 nn.Linear(d_model*4, d_model))     def forward(self, x):        # 没写embedding 跟 位置编码        x = x.flatten(-2,-1)        K, V_1 = self.mul(x)        V_1 = self.norm(V_1 + x)        V_2= self.ff(V_1)        V_2 = self.norm(V_1 + V_2)        return K, V_2class Encoder(nn.Module):    def __init__(self, model, N):        super().__init__()        self.layers = clones(model, N)    def forward(self, x):        for laye ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/15638.html" title="Matplotlib 概述"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/Asuka/Asuka27.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Matplotlib 概述"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/15638.html" title="Matplotlib 概述">Matplotlib 概述</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-01T13:42:30.368Z" title="发表于 2022-12-01 21:42:30">2022-12-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-01T15:58:04.519Z" title="更新于 2022-12-01 23:58:04">2022-12-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Python/">Python</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/matplotlib/">matplotlib</a></span></div><div class="content">待完成本文简单概述Matplotlib绘图库三个主要的类

Axes
Figure
Artist

通过以上三个类来了解作图的逻辑。推荐学习官网的示例
Axes首先介绍Axes，他可以理解为坐标系，他的子类是aixs坐标轴(见名知意很好理解)。一下我们通过一个示例理解
1234import matplotlib.pyplot as pltfig, axs = plt.subplots()  axs.plot([1, 2, 3, 4], [1, 4, 2, 3])



以上我们引入matplotlib中的pyplot别名为plt
通过调用plt.subplots() 返回两个对象 fig( figure)，axs( axes)。
AxisAxis顾名思义数轴，可以定义刻度，单位等
Figure整体画布
Artist图画元素, 线条的形状等
面向对象语法axes123plt.subplot() 	# 初始化一个axesplt.subplots(m, n) 	# m行n列axes



1234567fig , axes = plt.subplots(2,1, figsize=(6,6))  # 两行一列，axes[0].bar(sm1, sm2)  # 柱状图axes[1].plot(sm1, sm2) # 折线图plt.show()



12345fig , axes = plt.subplots(2,2, figsize=(6,6))axes[0, 0]axes[0, 1] # 以坐标调用



Figureplt.figure()返回一个画布
可以设定很多参数
12345678910def figure(num=None,  # autoincrement if None, else integer from 1-N           figsize=None,  # defaults to rc figure.figsize #多少英寸           dpi=None,  # defaults to rc figure.dpi #清晰度           facecolor=None,  # defaults to rc figure.facecolor #背景色           edgecolor=None,  # defaults to rc figure.edgecolor # 边缘色           frameon=True,           FigureClass=Figure,           clear=False,           **kwargs           ):



全局参数123456789mpl.rcParams[&#x27;font.family&#x27;] = [&#x27;Heiti SC&#x27;] # 字体mpl.rcParams[&#x27;figure.dpi&#x27;] = 300 # 清晰度mpl.rcParams.get(&#x27;figure.figsize&#x27;) # 获得画布尺寸para = &#123;&#x27;figure.dpi&#x27;: 500, &#x27;figure.figsize&#x27;: [10, 10]&#125; mpl.rcParams.update(para) # 通过上面的参数进行一次更新

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/10656.html" title="句意相似度 PipeLine总结"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris41.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="句意相似度 PipeLine总结"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/10656.html" title="句意相似度 PipeLine总结">句意相似度 PipeLine总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-27T02:47:31.323Z" title="发表于 2022-11-27 10:47:31">2022-11-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-02T02:09:36.113Z" title="更新于 2022-12-02 10:09:36">2022-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%8F%A5%E6%84%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6/">句意相似度</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Pipeline/">Pipeline</a></span></div><div class="content">主要进行训练框架优化

端到端 ML 实施（训练、验证、预测、评估）
轻松适应您自己的数据集
促进其他基于 BERT 的模型（BERT、ALBERT、…）的快速实验
使用有限的计算资源进行快速训练（混合精度、梯度累积……）
多 GPU 执行
分类决策的阈值选择（不一定是 0.5）
冻结 BERT 层，只更新分类层权重或更新所有权重
种子设置，可复现结果

PipeLine导包12345678910111213import torchimport torch.nn as nnimport osimport copyimport torch.optim as optimimport randomimport numpy as npimport pandas as pdfrom torch.utils.data import DataLoader, Datasetfrom torch.cuda.amp import autocast, GradScalerfrom tqdm.auto import tqdmfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmupfrom datasets import load_dataset, load_metric

Dataset123456789101112131415161718192021222324252627282930313233343536class CustomDataset(Dataset):    def __init__(self, data, maxlen, with_labels=True, bert_model=&#x27;albert-base-v2&#x27;):        self.data = data  # pandas dataframe        #Initialize the tokenizer        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)          self.maxlen = maxlen        self.with_labels = with_labels     def __len__(self):        return len(self.data)    def __getitem__(self, index):        #根据索引索取DataFrame中句子1余句子2        sent1 = str(self.data.loc[index, &#x27;sentence1&#x27;])        sent2 = str(self.data.loc[index, &#x27;sentence2&#x27;])        # 对句子对分词，得到input_ids、attention_mask和token_type_ids        encoded_pair = self.tokenizer(sent1, sent2,                                       padding=&#x27;max_length&#x27;,  # 填充到最大长度                                      truncation=True,  # 根据最大长度进行截断                                      max_length=self.maxlen,                                        return_tensors=&#x27;pt&#x27;)  # 返回torch.Tensor张量                token_ids = encoded_pair[&#x27;input_ids&#x27;].squeeze(0)  # tensor token ids        attn_masks = encoded_pair[&#x27;attention_mask&#x27;].squeeze(0)  # padded values对应为 &quot;0&quot; ，其他token为1        token_type_ids = encoded_pair[&#x27;token_type_ids&#x27;].squeeze(0)  #第一个句子的值为0，第二个句子的值为1 # 只有一句全为0        if self.with_labels:  # True if the dataset has labels            label = self.data.loc[index, &#x27;label&#x27;]            return token_ids, attn_masks, token_type_ids, label          else:            return token_ids, attn_masks, token_type_ids

建议，进行测试
12sample = next(iter(DataLoader(tr_dataset, batch_size=2)))sample

12tr_model = SentencePairClassifier(freeze_bert=True)tr_model(sample[0], sample[1], sample[2])

就是方便最后的维度转换，squeeze、flatten、view；甚至可以用reshape方法
模型定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class SentencePairClassifier(nn.Module):    def __init__(self, bert_model=&quot;albert-base-v2&quot;, freeze_bert=False):        super(SentencePairClassifier, self).__init__()        #  初始化预训练模型Bert xxx        self.bert_layer = AutoModel.from_pretrained(bert_model)        #  encoder 隐藏层大小        if bert_model == &quot;albert-base-v2&quot;:  # 12M 参数            hidden_size = 768        elif bert_model == &quot;albert-large-v2&quot;:  # 18M 参数            hidden_size = 1024        elif bert_model == &quot;albert-xlarge-v2&quot;:  # 60M 参数            hidden_size = 2048        elif bert_model == &quot;albert-xxlarge-v2&quot;:  # 235M 参数            hidden_size = 4096        elif bert_model == &quot;bert-base-uncased&quot;: # 110M 参数            hidden_size = 768        elif bert_model == &quot;roberta-base&quot;: #             hidden_size = 768        # 固定Bert层 更新分类输出层        if freeze_bert:            for p in self.bert_layer.parameters():                p.requires_grad = False                        self.dropout = nn.Dropout(p=0.1)        # 分类输出        self.cls_layer = nn.Linear(hidden_size, 1)    @autocast()  # 混合精度训练    def forward(self, input_ids, attn_masks, token_type_ids):        &#x27;&#x27;&#x27;        Inputs:            -input_ids : Tensor  containing token ids            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2        &#x27;&#x27;&#x27;        # 输入给Bert，获取上下文表示        # cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)        # last_hidden_state,pooler_output,all_hidden_states 12层        # 将last layer hidden-state of the [CLS] 输入到 classifier layer        # - last_hidden_state 的向量平均        # - 取all_hidden_states最后四层，然后做平均 weighted 平均        # - last_hidden_state+lstm        # 获取输出        logits = self.cls_layer(self.dropout(outputs[&#x27;pooler_output&#x27;]))        return logits

固定随机种子12345678910def set_seed(seed):    &quot;&quot;&quot; 固定随机种子，保证结果复现    &quot;&quot;&quot;    torch.manual_seed(seed)    torch.cuda.manual_seed_all(seed)    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    np.random.seed(seed)    random.seed(seed)    os.environ[&#x27;PYTHONHASHSEED&#x27;] = str(seed)

训练和评估12!mkdir models 	#可以在之前补充绝对路径!mkdir results



123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):    best_loss = np.Inf    best_ep = 1    nb_iterations = len(train_loader)    print_every = nb_iterations // 5  # 打印频率    iters = []    train_losses = []    val_losses = []    scaler = GradScaler()    for ep in range(epochs):        net.train()        running_loss = 0.0        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):            # 转为cuda张量            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)                # 混合精度加速训练            with autocast():                # Obtaining the logits from the model                logits = net(seq, attn_masks, token_type_ids)                # Computing loss                loss = criterion(logits.squeeze(-1), labels.float())                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged            # Backpropagating the gradients            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.            scaler.scale(loss).backward()            if (it + 1) % iters_to_accumulate == 0:                # Optimization step                # scaler.step() first unscales the gradients of the optimizer&#x27;s assigned params.                # If these gradients do not contain infs or NaNs, opti.step() is then called,                # otherwise, opti.step() is skipped.                scaler.step(opti)                # Updates the scale for next iteration.                scaler.update()                # 根据迭代次数调整学习率。                lr_scheduler.step()                # 梯度清零                opti.zero_grad()            running_loss += loss.item()            if (it + 1) % print_every == 0:  # Print training loss information                print()                print(f&quot;Iteration &#123;it+1&#125;/&#123;nb_iterations&#125; of epoch &#123;ep+1&#125; complete. \                Loss : &#123;running_loss / print_every&#125; &quot;)                running_loss = 0.0        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss        print()        print(f&quot;Epoch &#123;ep+1&#125; complete! Validation Loss : &#123;val_loss&#125;&quot;)        if val_loss &lt; best_loss:            print(&quot;Best validation loss improved from &#123;&#125; to &#123;&#125;&quot;.format(best_loss, val_loss))            print()            net_copy = copy.deepcopy(net)  # # 保存最优模型            best_loss = val_loss            best_ep = ep + 1    # 保存模型    path_to_model=f&#x27;models/&#123;bert_model&#125;_lr_&#123;lr&#125;_val_loss_&#123;round(best_loss, 5)&#125;_ep_&#123;best_ep&#125;.pt&#x27;    torch.save(net_copy.state_dict(), path_to_model)    print(&quot;The model has been saved in &#123;&#125;&quot;.format(path_to_model))    del loss    torch.cuda.empty_cache() # 清空显存    def evaluate_loss(net, device, criterion, dataloader):    &quot;&quot;&quot;    评估输出    &quot;&quot;&quot;    net.eval()    mean_loss = 0    count = 0    with torch.no_grad():        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):            seq, attn_masks, token_type_ids, labels = \                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)            logits = net(seq, attn_masks, token_type_ids)            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()            count += 1    return mean_loss / count


注意autocast和累计梯度 这两种加速计算的方法

evaluate的时候要注意数据的维度，标签的类型


超参数 &amp; 开始训练1234567bert_model = &quot;albert-base-v2&quot;  # &#x27;albert-base-v2&#x27;, &#x27;albert-large-v2&#x27;freeze_bert = False  # 是否冻结Bertmaxlen = 128  # 最大长度bs = 16  # batch sizeiters_to_accumulate = 2  # 梯度累加lr = 2e-5  # learning rateepochs = 2  # 训练轮数



123456789101112131415161718192021222324252627282930#  固定随机种子 便于复现set_seed(1) # 2022 # 创建训练集与验证集print(&quot;Reading training data...&quot;)train_set = CustomDataset(df_train, maxlen, bert_model)print(&quot;Reading validation data...&quot;)val_set = CustomDataset(df_val, maxlen, bert_model)# 常见训练集与验证集DataLoadertrain_loader = DataLoader(train_set, batch_size=bs, num_workers=0)val_loader = DataLoader(val_set, ba ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客 <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">12</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">21k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-02T02:27:36.225Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">拥有一切 却变成太空</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>