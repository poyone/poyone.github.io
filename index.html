<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Attention Is A Talent</title><meta name="author" content="Poy One"><meta name="copyright" content="Poy One"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Attention Is A Talent">
<meta property="og:url" content="https://poyone.github.io/index.html">
<meta property="og:site_name" content="Attention Is A Talent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp">
<meta property="article:author" content="Poy One">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/poyone1222/eris/eris11.webp"><link rel="canonical" href="https://poyone.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention Is A Talent',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-12 17:34:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Attention Is A Talent</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Attention Is A Talent</h1><div id="site_social_icons"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/posts/7194.html" title="HF Course 09 Custom Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 09 Custom Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/7194.html" title="HF Course 09 Custom Tokenizer">HF Course 09 Custom Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-12T08:15:06.518Z" title="发表于 2022-12-12 16:15:06">2022-12-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-12T09:30:53.736Z" title="更新于 2022-12-12 17:30:53">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">
与之前继承式的分词器不同，这词我们将从语料库中训练一个全新的分词器
首先我们设置一个WordPiece类型的分词器

加载文档12345678910111213from datasets import load_datasetdataset = load_dataset(&quot;wikitext&quot;, name=&quot;wikitext-2-raw-v1&quot;, split=&quot;train&quot;)def get_training_corpus():    for i in range(0, len(dataset), 1000):        yield dataset[i : i + 1000][&quot;text&quot;]# 也可以从本地打开文档with open(&quot;wikitext-2.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:    for i in range(len(dataset)):        f.write(dataset[i][&quot;text&quot;] + &quot;\n&quot;)



加载构件1234567891011from tokenizers import (    decoders,    models,    normalizers,    pre_tokenizers,    processors,    trainers,    Tokenizer,)tokenizer = Tokenizer(models.WordPiece(unk_token=&quot;[UNK]&quot;))

我们从tokenizer库中加载特殊的model构件，来使用WordPiece方法
设定遇到没见过的词标记为[UNK]， 同时可以设置max_input_chars_per_word作为最大词长
设置Normalizer这里我们选择bert的设置，包括: 
所有字母小写、strip_accents除去重音、删除控制字符、将所有多个空格设置为单个空格、汉字周围放置空格。
123456789tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)# 你也可以自定义tokenizer.normalizer = normalizers.Sequence(    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])print(tokenizer.normalizer.normalize_str(&quot;Héllò hôw are ü?&quot;))# hello how are u?

上面自定义中我们使用Sequence方法定义我们自己的规范化规则
Pre-tokenization和上面一样可以通过tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()套用bert的设置
下面是custom版本
12345tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)), (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;



.Whitespace()是对标点空格分隔，你可用下面的分隔
1234pre_tokenizer = pre_tokenizers.WhitespaceSplit()pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&quot;Let&#x27;s&quot;, (0, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre-tokenizer.&#x27;, (14, 28))]&#x27;&#x27;&#x27;



推荐使用Sequence方法组合你的预分词
1234567pre_tokenizer = pre_tokenizers.Sequence(    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()])pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test my pre-tokenizer.&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;&quot;, (3, 4)), (&#x27;s&#x27;, (4, 5)), (&#x27;test&#x27;, (6, 10)), (&#x27;my&#x27;, (11, 13)), (&#x27;pre&#x27;, (14, 17)), (&#x27;-&#x27;, (17, 18)), (&#x27;tokenizer&#x27;, (18, 27)), (&#x27;.&#x27;, (27, 28))]&#x27;&#x27;&#x27;



Trainer训练之前我们需要加入特殊token因为他不在你的词库之中
12special_tokens = [&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)


As well as specifying the vocab_size and special_tokens, we can set the min_frequency (the number of times a token must appear to be included in the vocabulary) or change the continuing_subword_prefix (if we want to use something different from ##).
改某个token必须出现多少次、改连接前缀##为别的



开始训练
12345tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)# 另外的版本tokenizer.model = models.WordPiece(unk_token=&quot;[UNK]&quot;)tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)

第一种方法是用上面定义的生成器
第二种传入”wikitext-2.txt”文件
到此我们tokenizer就具有了一般tokenizer的所有方法如encode
1234encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)# [&#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;]



Post-processing最后我们需要包裹我们的token到特殊的格式如: [CLS]…[SEP]…[SEP]
首先我们获取所需的特殊token的下标
12345cls_token_id = tokenizer.token_to_id(&quot;[CLS]&quot;)sep_token_id = tokenizer.token_to_id(&quot;[SEP]&quot;)print(cls_token_id, sep_token_id)# (2, 3)



接下来处理我们的模板
12345tokenizer.post_processor = processors.TemplateProcessing(    single=f&quot;[CLS]:0 $A:0 [SEP]:0&quot;,    pair=f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;,    special_tokens=[(&quot;[CLS]&quot;, cls_token_id), (&quot;[SEP]&quot;, sep_token_id)],)

模板我们需要设置两种模式:

single–单个句子情况下

[0,0,0,0]


pair

[0,0,0,1,1,1]


最后指定特殊token的id


查看
12345678910encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)# [&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer...&quot;, &quot;on a pair of sentences.&quot;)print(encoding.tokens)print(encoding.type_ids)&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;let&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;test&#x27;, &#x27;this&#x27;, &#x27;tok&#x27;, &#x27;##eni&#x27;, &#x27;##zer&#x27;, &#x27;...&#x27;, &#x27;[SEP]&#x27;, &#x27;on&#x27;, &#x27;a&#x27;, &#x27;pair&#x27;, &#x27;of&#x27;, &#x27;sentences&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]&#x27;&#x27;&#x27;



decoder接下来对解码器做一定设置
1234tokenizer.decoder = decoders.WordPiece(prefix=&quot;##&quot;)tokenizer.decode(encoding.ids)# &quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;



保存 &amp; 加载 Custom Tokenizertokenizer.save(&quot;tokenizer.json&quot;)
new_tokenizer = Tokenizer.from_file(&quot;tokenizer.json&quot;)
转成Fast TokenizerTo use this tokenizer in 🤗 Transformers, we have to wrap it in a PreTrainedTokenizerFast. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, BertTokenizerFast). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.

可以继承你的特定类BertTokenizerFast，也可以用泛类PreTrainedTokenizerFast

1234567891011from transformers import PreTrainedTokenizerFastwrapped_tokenizer = PreTrainedTokenizerFast(    tokenizer_object=tokenizer,    # tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively    unk_token=&quot;[UNK]&quot;,    pad_token=&quot;[PAD]&quot;,    cls_token=&quot;[CLS]&quot;,    sep_token=&quot;[SEP]&quot;,    mask_token=&quot;[MASK]&quot;,)

这里可以从文件中加载你的tokenizer设置、也可直接赋值、注意你的特殊符号必须重新定义
BPE类型的分词器1234567tokenizer = Tokenizer(models.BPE())tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test pre-tokenization!&quot;)&#x27;&#x27;&#x27;[(&#x27;Let&#x27;, (0, 3)), (&quot;&#x27;s&quot;, (3, 5)), (&#x27;Ġtest&#x27;, (5, 10)), (&#x27;Ġpre&#x27;, (10, 14)), (&#x27;-&#x27;, (14, 15)), (&#x27;tokenization&#x27;, (15, 27)), (&#x27;!&#x27;, (27, 28))]&#x27;&#x27;&#x27;



GPT2只需要开始和结束的特殊token
12345678910trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[&quot;&lt;|endoftext|&gt;&quot;])tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)tokenizer.model = models.BPE()tokenizer.train([&quot;wikitext-2.txt&quot;], trainer=trainer)encoding = tokenizer.encode(&quot;Let&#x27;s test this tokenizer.&quot;)print(encoding.tokens)&#x27;&#x27;&#x27;[&#x27;L&#x27;, &#x27;et&#x27;, &quot;&#x27;&quot;, &#x27;s&#x27;, &#x27;Ġtest&#x27;, &#x27;Ġthis&#x27;, &#x27;Ġto&#x27;, &#x27;ken&#x27;, &#x27;izer&#x27;, &#x27;.&#x27;]&#x27;&#x27;&#x27;





12345678tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)sentence = &quot;Let&#x27;s test this tokenizer.&quot;encoding = tokenizer.encode(sentence)start, end = encoding.offsets[4]sentence[start:end]# &#x27; test&#x27;

The trim_offsets = False option indicates to the post-processor that we should leave the offsets of tokens that begin with ‘Ġ’ as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Let’s have a look at the result with the text we just encoded, where &#39;Ġtest&#39; is the token at index 4

trim_offsets设定是否修正字符的空格位置进入偏移量

123tokenizer.decoder = decoders.ByteLevel()tokenizer.decode(encoding.ids)# &quot;Let&#x27;s test this tokenizer.&quot;



包装
123456789101112from transformers import PreTrainedTokenizerFastwrapped_tokenizer = PreTrainedTokenizerFast(    tokenizer_object=tokenizer,    bos_token=&quot;&lt;|endoftext|&gt;&quot;,    eos_token=&quot;&lt;|endoftext|&gt;&quot;,)# 或者from transformers import GPT2TokenizerFastwrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)



Unigram类型的分词器12345678910111213tokenizer = Tokenizer(models.Unigram())from tokenizers import Regextokenizer.normalizer = normalizers.Sequence(    [        normalizers.Replace(&quot;``&quot;, &#x27;&quot;&#x27;),        normalizers.Replace(&quot;&#x27;&#x27;&quot;, &#x27;&quot;&#x27;),        normalizers.NFKD(),        normalizers.StripAccents(),        normalizers.Replace(Regex(&quot; &#123;2,&#125;&quot;), &quot; &quot;),    ])

第一、二个norm将符号替换，最后一个将多个空格替换成一个
1234tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Let&#x27;s test the pre-tokenizer!&quot;)# [(&quot;▁Let&#x27;s&quot;, (0, 5)), (&#x27;▁test&#x27;, (5, 10)), (&#x27;▁the&#x27;, (10, 14)), (&#x27;▁pre-tokenizer!&#x27;, (14, 29))]



123456789special_tokens = [&quot;&lt;cls&gt;&quot;, &quot;&lt;sep&gt;&quot;, &quot;&lt;unk&gt;&quot;, &quot;&lt;pad&gt;&quot;, &quot;&lt;mask&gt;&quot;, &quot;&lt;s&gt;&quot;,  ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/7193.html" title="HF Course 08 Tokenizer底层算法"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 08 Tokenizer底层算法"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/7193.html" title="HF Course 08 Tokenizer底层算法">HF Course 08 Tokenizer底层算法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-12T03:35:18.892Z" title="发表于 2022-12-12 11:35:18">2022-12-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-12T08:16:00.524Z" title="更新于 2022-12-12 16:16:00">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">流程一般来说我们的tokenizer有如下流程

规范化规范化是对字符做大小写处理之类的我们可以通过如下API查看底层的normalization方法
123456789from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)print(type(tokenizer.backend_tokenizer))# &lt;class &#x27;tokenizers.Tokenizer&#x27;&gt;print(tokenizer.backend_tokenizer.normalizer.normalize_str(&quot;Héllò hôw are ü?&quot;))# &#x27;hello how are u?&#x27;



预分词通过如下api查看分词器是如何做pre_tokenize的
123tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)&#x27;&#x27;&#x27;&#x27;[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;how&#x27;, (7, 10)), (&#x27;are&#x27;, (11, 14)), (&#x27;you&#x27;, (16, 19)), (&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;

可以看到后面的偏移量坐标，这也是上一节offset-mapping的由来
不同的预分词gpt
1234tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)&#x27;&#x27;&#x27;[(&#x27;Hello&#x27;, (0, 5)), (&#x27;,&#x27;, (5, 6)), (&#x27;Ġhow&#x27;, (6, 10)), (&#x27;Ġare&#x27;, (10, 14)), (&#x27;Ġ&#x27;, (14, 15)), (&#x27;Ġyou&#x27;, (15, 19)),(&#x27;?&#x27;, (19, 20))]&#x27;&#x27;&#x27;&#x27;



t5
123tokenizer = AutoTokenizer.from_pretrained(&quot;t5-small&quot;)tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(&quot;Hello, how are  you?&quot;)# [(&#x27;▁Hello,&#x27;, (0, 6)), (&#x27;▁how&#x27;, (7, 10)), (&#x27;▁are&#x27;, (11, 14)), (&#x27;▁you?&#x27;, (16, 20))]



三种分词算法总览如上，不同的模型适用不同的分词算法
sentencepiece它经常与unigram算法一起，且并不需要预分词，是特攻中文日文，这种无法分词的语言的
算法预览


Model
BPE
WordPiece
Unigram



Training
Starts from a small vocabulary and learns rules to merge tokens
Starts from a small vocabulary and learns rules to merge tokens
Starts from a large vocabulary and learns rules to remove tokens


Training step
Merges the tokens corresponding to the most common pair
Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent
Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus


Learns
Merge rules and a vocabulary
Just a vocabulary
A vocabulary with a score for each token


Encoding
Splits a word into characters and applies the merges learned during training
Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word
Finds the most likely split into tokens, using the scores learned during training


BPE简述BPE是Byte-Pair Encoding 的简写，他有三步

将corpus所有独一无二字符拆出来，如英文中的26个字母，标点和其他特殊符号
在有基础字符的基础上，以频率作为选取标准，将两个字符匹配在一起，选择频率最高的词进行入库
重复第二步直到满足你设定的词库大小


The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don’t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.
GPT和roberta使用的是比特级别的字符，就是0100这种，这就是他们的基础语料库，然后在基础上融合出来词进行构建词库

实例下面进行实例解析设定语料库如下
语料库: &quot;hug&quot;, &quot;pug&quot;, &quot;pun&quot;, &quot;bun&quot;, &quot;hugs&quot;
词频: (&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)
1# (&quot;h&quot; &quot;u&quot; &quot;g&quot;, 10), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, 5)



第一轮
最多的是 ug的组合，20次
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;]Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;

第二轮
最多是un
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;]Corpus: (&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)&#x27;&#x27;

第三轮
最多的是hug
123&#x27;&#x27;&#x27;Vocabulary: [&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)&#x27;&#x27;&#x27;

…如何循环到设定的词库大小
简要代码语料库
123456corpus = [    &quot;This is the Hugging Face Course.&quot;,    &quot;This chapter is about tokenization.&quot;,    &quot;This section shows several tokenizer algorithms.&quot;,    &quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;,]



统计词频
12345678910111213141516171819from transformers import AutoTokenizerfrom collections import defaultdicttokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)word_freqs = defaultdict(int)for text in corpus:    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)    new_words = [word for word, offset in words_with_offsets]    for word in new_words:        word_freqs[word] += 1print(word_freqs)&#x27;&#x27;&#x27;defaultdict(int, &#123;&#x27;This&#x27;: 3, &#x27;Ġis&#x27;: 2, &#x27;Ġthe&#x27;: 1, &#x27;ĠHugging&#x27;: 1, &#x27;ĠFace&#x27;: 1, &#x27;ĠCourse&#x27;: 1, &#x27;.&#x27;: 4, &#x27;Ġchapter&#x27;: 1,    &#x27;Ġabout&#x27;: 1, &#x27;Ġtokenization&#x27;: 1, &#x27;Ġsection&#x27;: 1, &#x27;Ġshows&#x27;: 1, &#x27;Ġseveral&#x27;: 1, &#x27;Ġtokenizer&#x27;: 1, &#x27;Ġalgorithms&#x27;: 1,    &#x27;Hopefully&#x27;: 1, &#x27;,&#x27;: 1, &#x27;Ġyou&#x27;: 1, &#x27;Ġwill&#x27;: 1, &#x27;Ġbe&#x27;: 1, &#x27;Ġable&#x27;: 1, &#x27;Ġto&#x27;: 1, &#x27;Ġunderstand&#x27;: 1, &#x27;Ġhow&#x27;: 1,    &#x27;Ġthey&#x27;: 1, &#x27;Ġare&#x27;: 1, &#x27;Ġtrained&#x27;: 1, &#x27;Ġand&#x27;: 1, &#x27;Ġgenerate&#x27;: 1, &#x27;Ġtokens&#x27;: 1&#125;)&#x27;&#x27;&#x27;


首先载入gpt的分词器，做预分词
再载入collection中的defaultdict设定为int类型

基础词汇表
123456789101112alphabet = []for word in word_freqs.keys():    for letter in word:        if letter not in alphabet:            alphabet.append(letter)alphabet.sort()print(alphabet)&#x27;&#x27;&#x27;[ &#x27;,&#x27;, &#x27;.&#x27;, &#x27;C&#x27;, &#x27;F&#x27;, &#x27;H&#x27;, &#x27;T&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;r&#x27;, &#x27;s&#x27;,  &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;y&#x27;, &#x27;z&#x27;, &#x27;Ġ&#x27;]&#x27;&#x27;&#x27;

加个表头vocab = [&quot;&lt;|endoftext|&gt;&quot;] + alphabet.copy()
将单词映射为{‘word’: [‘w’, ‘o’, ‘r’, ‘d’]}的形式进行训练
1234splits = &#123;word: [c for c in word] for word in word_freqs.keys()&#125;# 我觉可以改一下`splits = &#123;word: list(word) for word in word_freqs.keys()&#125;`



字母对频率函数
1234567891011121314151617181920212223242526272829303132333435363738394041def compute_pair_freqs(splits):    pair_freqs = defaultdict(int)    for word, freq in word_freqs.items():        split = splits[word]	# 取得word对应的值如[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]        if len(split) == 1:            continue        for i in range(len(split) - 1):            pair = (split[i], split[i + 1])            pair_freqs[pair] += freq	# 记录字母对的频率    return pair_freqs# 示例pair_freqs = compute_pair_freqs(splits)for i, key in enumerate(pair_freqs.keys()):    print(f&quot;&#123;key&#125;: &#123;pair_freqs[key]&#125;&quot;)    if i &gt;= 5:        break&#x27;&#x27;&#x27;(&#x27;T&#x27;, &#x27;h&#x27;): 3(&#x27;h&#x27;, &#x27;i&#x27;): 3(&#x27;i&#x27;, &#x27;s&#x27;): 5(&#x27;Ġ&#x27;, &#x27;i&#x27;): 2(&#x27;Ġ&#x27;, &#x27;t&#x27;): 7(&#x27;t&#x27;, &#x27;h&#x27;): 3&#x27;&#x27;&#x27;# 取最大值best_pair = &quot;&quot;max_freq = Nonefor pair, freq in pair_freqs.items():    if max_freq is None or max_freq &lt; freq:        best_pair = pair        max_freq = freqprint(best_pair, max_freq)# (&#x27;Ġ&#x27;, &#x27;t&#x27;) 7# 合并入库merges = &#123;(&quot;Ġ&quot;, &quot;t&quot;): &quot;Ġt&quot;&#125;vocab.append(&quot;Ġt&quot;)



将字符对构建进新的基础词表split (不是vocab)
12345678910111213141516171819def merge_pair(a, b, splits):    for word in word_freqs:        split = splits[word] # 取得word对应的值如[&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]        if len(split) == 1:            continue        i = 0        while i &lt; len(split) - 1:            if split[i] == a and split[i + 1] == b:             	# 找到词对的位置，将ab字符串连接起来，做个列表存起来                split = split[:i] + [a + b] + split[i + 2 :]            else:                i += 1         splits[word] = split # 更新 [&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;d&#x27;] -&gt; [&#x27;wo&#x27;, &#x27;r&#x27;, &#x27;d&#x27;]    return splitssplits = merge_pair(&quot;Ġ&quot;, &quot;t&quot;, splits)print(splits[&quot;Ġtrained&quot;])# [&#x27;Ġt&#x27;, &#x27;r&#x27;, &#x27;a&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]



构建循环
1234567891011121314151617181920vocab_size = 50while len(vocab) &lt; v ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/37742.html" title="HF Course 07 NER QA Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 07 NER QA Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/37742.html" title="HF Course 07 NER QA Tokenizer">HF Course 07 NER QA Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-11T14:53:10.235Z" title="发表于 2022-12-11 22:53:10">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-12T04:09:35.967Z" title="更新于 2022-12-12 12:09:35">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">记得排版 分割线待完成
QA部分


本章我们需要对做特殊的tokenizer以适应NER和QA任务数据的特殊性

Fast Tokenizer1234567from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)example = &quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;encoding = tokenizer(example)print(type(encoding))# &lt;class &#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;&gt;

分词后返回的结果类型不简单是字典的映射
还包含很多方法
123456789101112tokenizer.is_fast, encoding.is_fast(True,True)encoding.tokens()&#x27;&#x27;&#x27;[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;, &#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]&#x27;&#x27;&#x27; encoding.word_ids()&#x27;&#x27;&#x27;[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]&#x27;&#x27;&#x27;

word_ids()方法可看到分词的结果来自哪个单词
最后我们可以使用word_to_chars() or token_to_chars() and char_to_word() or char_to_token() 查看单词
123start, end = encoding.word_to_chars(3)example[start:end]# Sylvain



NER
在NER中我们以偏移量的标记来锁定原文的字符

pipeline方法首先查看pipeline方法的ner流程
12345678910111213from transformers import pipelinetoken_classifier = pipeline(&quot;token-classification&quot;)token_classifier(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 35&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;, &#x27;start&#x27;: 35, &#x27;end&#x27;: 40&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;, &#x27;start&#x27;: 41, &#x27;end&#x27;: 45&#125;, &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;



简洁版
12345678from transformers import pipelinetoken_classifier = pipeline(&quot;token-classification&quot;, aggregation_strategy=&quot;simple&quot;)token_classifier(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)&#x27;&#x27;&#x27;[&#123;&#x27;entity_group&#x27;: &#x27;PER&#x27;, &#x27;score&#x27;: 0.9981694, &#x27;word&#x27;: &#x27;Sylvain&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity_group&#x27;: &#x27;ORG&#x27;, &#x27;score&#x27;: 0.97960204, &#x27;word&#x27;: &#x27;Hugging Face&#x27;, &#x27;start&#x27;: 33, &#x27;end&#x27;: 45&#125;, &#123;&#x27;entity_group&#x27;: &#x27;LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;word&#x27;: &#x27;Brooklyn&#x27;, &#x27;start&#x27;: 49, &#x27;end&#x27;: 57&#125;]&#x27;&#x27;&#x27;

aggregation_strategy有不同的参数，simple是分词后的平均分数
如上面的sylvain分数来自 正常版的四项平均&#39;S&#39;, &#39;##yl&#39;, &#39;##va&#39;, &#39;##in&#39;

&quot;first&quot;, where the score of each entity is the score of the first token of that entity (so for “Sylvain” it would be 0.993828, the score of the token S)
&quot;max&quot;, where the score of each entity is the maximum score of the tokens in that entity (so for “Hugging Face” it would be 0.98879766, the score of “Face”)
&quot;average&quot;, where the score of each entity is the average of the scores of the words composing that entity (so for “Sylvain” there would be no difference from the &quot;simple&quot; strategy, but “Hugging Face” would have a score of 0.9819, the average of the scores for “Hugging”, 0.975, and “Face”, 0.98879)

logits这里通过返回的结果使用argmax(-1)得到映射的分类
123456789from transformers import AutoTokenizer, AutoModelForTokenClassificationmodel_checkpoint = &quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)example = &quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;inputs = tokenizer(example, return_tensors=&quot;pt&quot;)outputs = model(**inputs)



12345print(inputs[&quot;input_ids&quot;].shape)print(outputs.logits.shape)&#x27;&#x27;&#x27;torch.Size([1, 19])torch.Size([1, 19, 9])&#x27;&#x27;&#x27;



123456789101112131415161718import torchprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()predictions = outputs.logits.argmax(dim=-1)[0].tolist()print(predictions)# [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]model.config.id2label&#x27;&#x27;&#x27;&#123;0: &#x27;O&#x27;, 1: &#x27;B-MISC&#x27;, 2: &#x27;I-MISC&#x27;, 3: &#x27;B-PER&#x27;, 4: &#x27;I-PER&#x27;, 5: &#x27;B-ORG&#x27;, 6: &#x27;I-ORG&#x27;, 7: &#x27;B-LOC&#x27;, 8: &#x27;I-LOC&#x27;&#125;&#x27;&#x27;&#x27;



偏移量postprocessing组织一下格式，复现上面的内容
123456789101112131415161718192021results = []tokens = inputs.tokens()for idx, pred in enumerate(predictions):    label = model.config.id2label[pred]    if label != &quot;O&quot;:        results.append(            &#123;&quot;entity&quot;: label, &quot;score&quot;: probabilities[idx][pred], &quot;word&quot;: tokens[idx]&#125;        )print(results)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;&#125;, &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;&#125;]&#x27;&#x27;&#x27;



偏移量 offsets_mapping
123456inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)inputs_with_offsets[&quot;offset_mapping&quot;]&#x27;&#x27;&#x27;[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]&#x27;&#x27;&#x27;

这里的19对元组就是对应19个分词后token的下标
1[&#x27;[CLS]&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;S&#x27;, &#x27;##yl&#x27;, &#x27;##va&#x27;, &#x27;##in&#x27;, &#x27;and&#x27;, &#x27;I&#x27;, &#x27;work&#x27;, &#x27;at&#x27;, &#x27;Hu&#x27;, &#x27;##gging&#x27;, &#x27;Face&#x27;, &#x27;in&#x27;,&#x27;Brooklyn&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]

比如(0,0)是留给[CLS]的；	比如第六个token对应的是 ##ly  那么他在原文中的标注就是（12,14），如下
12example[12:14]# yl



继续我们的复现pipeline
123456789101112131415161718192021222324252627282930results = []inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)tokens = inputs_with_offsets.tokens()offsets = inputs_with_offsets[&quot;offset_mapping&quot;]for idx, pred in enumerate(predictions):    label = model.config.id2label[pred]    if label != &quot;O&quot;:        start, end = offsets[idx]        results.append(            &#123;                &quot;entity&quot;: label,                &quot;score&quot;: probabilities[idx][pred],                &quot;word&quot;: tokens[idx],                &quot;start&quot;: start,                &quot;end&quot;: end,            &#125;        )print(results)&#x27;&#x27;&#x27;[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;, &#x27;start&#x27;: 11, &#x27;end&#x27;: 12&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;, &#x27;start&#x27;: 12, &#x27;end&#x27;: 14&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;, &#x27;start&#x27;: 14, &#x27;end&#x27;: 16&#125;, &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;, &#x27;start&#x27;: 16, &#x27;end&#x27;: 18&#125;, &#123;&#x27;entity&#x27;: &#x ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/55913.html" title="HF Course 06 继承式的Tokenizer"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris11.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 06 继承式的Tokenizer"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/55913.html" title="HF Course 06 继承式的Tokenizer">HF Course 06 继承式的Tokenizer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-11T13:27:56.158Z" title="发表于 2022-12-11 21:27:56">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-12T04:09:40.352Z" title="更新于 2022-12-12 12:09:40">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">这种方法是基于旧的模型分词器上，针对你的语料库训练一个新的分词器的方法。夺舍属于是
这里我们以GPT的分词器为例，它使用unigram的算法进行分词
载入数据12345678910111213from datasets import load_dataset# This can take a few minutes to load, so grab a coffee or tea while you wait!raw_datasets = load_dataset(&quot;code_search_net&quot;, &quot;python&quot;)raw_datasets[&quot;train&quot;]&#x27;&#x27;&#x27;Dataset(&#123;    features: [&#x27;repository_name&#x27;, &#x27;func_path_in_repository&#x27;, &#x27;func_name&#x27;, &#x27;whole_func_string&#x27;, &#x27;language&#x27;,       &#x27;func_code_string&#x27;, &#x27;func_code_tokens&#x27;, &#x27;func_documentation_string&#x27;, &#x27;func_documentation_tokens&#x27;, &#x27;split_name&#x27;,       &#x27;func_code_url&#x27;    ],    num_rows: 412178&#125;)&#x27;&#x27;&#x27;



生成器加载数据下面的方法会一次加载所有数据
12# Don&#x27;t uncomment the following line unless your dataset is small!# training_corpus = [raw_datasets[&quot;train&quot;][i: i + 1000][&quot;whole_func_string&quot;] for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000)]



一般使用python生成器
1234training_corpus = (    raw_datasets[&quot;train&quot;][i : i + 1000][&quot;whole_func_string&quot;]    for i in range(0, len(raw_datasets[&quot;train&quot;]), 1000))

将列表推导式的方括号换成圆括号就可以变成生成器了，好厉害。

123456&gt;gen = (i for i in range(10))&gt;print(list(gen))&gt;print(list(gen))&gt;&#x27;&#x27;&#x27;&gt;[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;[]&#x27;&#x27;&#x27;

使用之后会清空内存，如上所示

更一般的生成器
12345def get_training_corpus():    dataset = raw_datasets[&quot;train&quot;]    for start_idx in range(0, len(dataset), 1000):        samples = dataset[start_idx : start_idx + 1000]        yield samples[&quot;whole_func_string&quot;]



train_new_from_iterator()载入模型1234567891011121314from transformers import AutoTokenizerold_tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)example = &#x27;&#x27;&#x27;def add_numbers(a, b):    &quot;&quot;&quot;Add the two numbers `a` and `b`.&quot;&quot;&quot;    return a + b&#x27;&#x27;&#x27;tokens = old_tokenizer.tokenize(example)tokens&#x27;&#x27;&#x27;[&#x27;def&#x27;, &#x27;Ġadd&#x27;, &#x27;_&#x27;, &#x27;n&#x27;, &#x27;umbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ġb&#x27;, &#x27;):&#x27;, &#x27;Ċ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ġthe&#x27;, &#x27;Ġtwo&#x27;, &#x27;Ġnumbers&#x27;, &#x27;Ġ`&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ġand&#x27;, &#x27;Ġ`&#x27;, &#x27;b&#x27;, &#x27;`&#x27;, &#x27;.&quot;&#x27;, &#x27;&quot;&quot;&#x27;, &#x27;Ċ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġ&#x27;, &#x27;Ġreturn&#x27;, &#x27;Ġa&#x27;, &#x27;Ġ+&#x27;, &#x27;Ġb&#x27;]&#x27;&#x27;&#x27;

This tokenizer has a few special symbols, like Ġ and Ċ, which denote spaces and newlines, respectively。
两个G表示空格和换行符。他还为多个空格在一起的单独编码，带下划线的词也不认识，所以不太合适。
训练新分词器1tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)


注意，只有Fast的tokenizer支持train_new_from_iterator方法，他们是根据rust写的。没有fast的是纯python写的。

12345tokens = tokenizer.tokenize(example)tokens&#x27;&#x27;&#x27;[&#x27;def&#x27;, &#x27;Ġadd&#x27;, &#x27;_&#x27;, &#x27;numbers&#x27;, &#x27;(&#x27;, &#x27;a&#x27;, &#x27;,&#x27;, &#x27;Ġb&#x27;, &#x27;):&#x27;, &#x27;ĊĠĠĠ&#x27;, &#x27;Ġ&quot;&quot;&quot;&#x27;, &#x27;Add&#x27;, &#x27;Ġthe&#x27;, &#x27;Ġtwo&#x27;, &#x27;Ġnumbers&#x27;, &#x27;Ġ`&#x27;, &#x27;a&#x27;, &#x27;`&#x27;, &#x27;Ġand&#x27;, &#x27;Ġ`&#x27;, &#x27;b&#x27;, &#x27;`.&quot;&quot;&quot;&#x27;, &#x27;ĊĠĠĠ&#x27;, &#x27;Ġreturn&#x27;, &#x27;Ġa&#x27;, &#x27;Ġ+&#x27;, &#x27;Ġb&#x27;]&#x27;&#x27;&#x27;

起码多个空格学会了
存储新分词器1tokenizer.save_pretrained(&quot;code-search-net-tokenizer&quot;)

</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/posts/5596.html" title="HF Course 05 faiss 搜索引擎"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 05 faiss 搜索引擎"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/5596.html" title="HF Course 05 faiss 搜索引擎">HF Course 05 faiss 搜索引擎</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-11T09:15:41.640Z" title="发表于 2022-12-11 17:15:41">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-12T04:01:25.516Z" title="更新于 2022-12-12 12:01:25">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">在我们创建好自己的数据集后，可以用faiss 和 hf 来搜索一些数据。
我们通过multi-qa-mpnet-base-dot-v1模型embedding我们的数据，然后通过 faiss给每个embedding得到index
最后将我们的query 给tokenizer转换之后喂给模型，得到最匹配我们问题的数据。

Fortunately, there’s a library called sentence-transformers that is dedicated to creating embeddings. As described in the library’s documentation, our use case is an example of asymmetric semantic search because we have a short query whose answer we’d like to find in a longer document, like a an issue comment. The handy model overview table in the documentation indicates that the multi-qa-mpnet-base-dot-v1 checkpoint has the best performance for semantic search, so we’ll use that for our application.
我们主要使用了sentence-transformers faiss两个额外库处理

加载模型12345from transformers import AutoTokenizer, AutoModelmodel_ckpt = &quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;tokenizer = AutoTokenizer.from_pretrained(model_ckpt)model = AutoModel.from_pretrained(model_ckpt)



数据处理123456789101112131415import torchdevice = torch.device(&quot;cuda&quot;)model.to(device)def cls_pooling(model_output):    return model_output.last_hidden_state[:, 0]    def get_embeddings(text_list):    encoded_input = tokenizer(        text_list, padding=True, truncation=True, return_tensors=&quot;pt&quot;    )    encoded_input = &#123;k: v.to(device) for k, v in encoded_input.items()&#125;    model_output = model(**encoded_input)    return cls_pooling(model_output)



加入 faiss 的index12345embeddings_dataset = comments_dataset.map(    lambda x: &#123;&quot;embeddings&quot;: get_embeddings(x[&quot;text&quot;]).detach().cpu().numpy()[0]&#125;)embeddings_dataset.add_faiss_index(column=&quot;embeddings&quot;)



测试1234question = &quot;How can I load a dataset offline?&quot;question_embedding = get_embeddings([question]).cpu().detach().numpy()question_embedding.shape# torch.Size([1, 768])



123scores, samples = embeddings_dataset.get_nearest_examples(    &quot;embeddings&quot;, question_embedding, k=5)



查看结果12345import pandas as pdsamples_df = pd.DataFrame.from_dict(samples)samples_df[&quot;scores&quot;] = scoressamples_df.sort_values(&quot;scores&quot;, ascending=False, inplace=True)



1234567for _, row in samples_df.iterrows():    print(f&quot;COMMENT: &#123;row.comments&#125;&quot;)    print(f&quot;SCORE: &#123;row.scores&#125;&quot;)    print(f&quot;TITLE: &#123;row.title&#125;&quot;)    print(f&quot;URL: &#123;row.html_url&#125;&quot;)    print(&quot;=&quot; * 50)    print()



可以查看最匹配的评论
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;&quot;&quot;COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how `transformers` loads models offline fine.@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?SCORE: 25.505046844482422TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)You can now use them offline\`\`\`pythondatasets = load_dataset(&quot;text&quot;, data_files=data_files)\`\`\`We&#x27;ll do a new release soonSCORE: 24.555509567260742TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.----------&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\`\`\`pythonload_dataset(&quot;./my_dataset&quot;)\`\`\`and the dataset script will generate your dataset once and for all.----------About I&#x27;m looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.cf #1724SCORE: 24.14896583557129TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine&gt;&gt; 1. (online machine)&gt;

import datasets
data &#x3D; datasets.load_dataset(…)
data.save_to_disk(&#x2F;YOUR&#x2F;DATASET&#x2F;DIR)
123452. copy the dir from online to the offline machine3. (offline machine)

import datasets
data &#x3D; datasets.load_from_disk(&#x2F;SAVED&#x2F;DATA&#x2F;DIR)
12345678910111213141516171819202122232425262728293031HTH.SCORE: 22.893993377685547TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================COMMENT: here is my way to load a dataset offline, but it **requires** an online machine1. (online machine)\`\`\`import datasetsdata = datasets.load_dataset(...)data.save_to_disk(/YOUR/DATASET/DIR)\`\`\`2. copy the dir from online to the offline machine3. (offline machine)\`\`\`import datasetsdata = datasets.load_from_disk(/SAVED/DATA/DIR)\`\`\`HTH.SCORE: 22.406635284423828TITLE: Discussion using datasets in offline modeURL: https://github.com/huggingface/datasets/issues/824==================================================&quot;&quot;&quot;

</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/posts/4498.html" title="HF Course 04 Dataset"><img class="post_bg" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/Eris32.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HF Course 04 Dataset"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/4498.html" title="HF Course 04 Dataset">HF Course 04 Dataset</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-11T07:18:25.982Z" title="发表于 2022-12-11 15:18:25">2022-12-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-12T04:01:21.013Z" title="更新于 2022-12-12 12:01:21">2022-12-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universe/">Universe</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Huggingface/">Huggingface</a></span></div><div class="content">加载本地数据


Data format
Loading script
Example



CSV &amp; TSV
csv
load_dataset(&quot;csv&quot;, data_files=&quot;my_file.csv&quot;)


Text files
text
load_dataset(&quot;text&quot;, data_files=&quot;my_file.txt&quot;)


JSON &amp; JSON Lines
json
load_dataset(&quot;json&quot;, data_files=&quot;my_file.jsonl&quot;)


Pickled DataFrames
pandas
load_dataset(&quot;pandas&quot;, data_files=&quot;my_dataframe.pkl&quot;)


分别需要做，指明数据类型，指明文件路径
data_files参数The data_files argument of the load_dataset() function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting data_files=&quot;*.json&quot;). See the 🤗 Datasets documentation for more details.

可以做文件路径

可以做split将数据映射成想要的字典格式

&#96;&#96;&#96;data_files &#x3D; {“train”: “SQuAD_it-train.json”, “test”: “SQuAD_it-test.json”}squad_it_dataset &#x3D; load_dataset(“json”, data_files&#x3D;data_files, field&#x3D;”data”)squad_it_dataset
‘’’DatasetDict({train: Dataset({    features: [‘title’, ‘paragraphs’],    num_rows: 442})test: Dataset({    features: [‘title’, ‘paragraphs’],    num_rows: 48})})’’’
12345    ## 加载服务器数据



url &#x3D; “https://github.com/crux82/squad-it/raw/master/&quot;data_files &#x3D; {    “train”: url + “SQuAD_it-train.json.gz”,    “test”: url + “SQuAD_it-test.json.gz”,}squad_it_dataset &#x3D; load_dataset(“json”, data_files&#x3D;data_files, field&#x3D;”data”)
1234567891011# 数据处理## 分隔符如果你的数据不是传统的CSV格式(以逗号分割)，你可以指定分隔符
from datasets import load_dataset
data_files &#x3D; {“train”: “drugsComTrain_raw.tsv”, “test”: “drugsComTest_raw.tsv”}
\t is the tab character in Pythondrug_dataset &#x3D; load_dataset(“csv”, data_files&#x3D;data_files, delimiter&#x3D;”\t”)
12345## 随机选取样本
drug_sample &#x3D; drug_dataset[“train”].shuffle(seed&#x3D;42).select(range(1000))
Peek at the first few examplesdrug_sample[:3]
‘’’{‘Unnamed: 0’: [87571, 178045, 80482], ‘drugName’: [‘Naproxen’, ‘Duloxetine’, ‘Mobic’], ‘condition’: [‘Gout, Acute’, ‘ibromyalgia’, ‘Inflammatory Conditions’], ‘review’: [‘“like the previous person mention, I&amp;#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills…..Aleve works!”‘,  ‘“I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.”‘,  ‘“I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.”‘], ‘rating’: [9.0, 3.0, 10.0], ‘date’: [‘September 2, 2015’, ‘November 7, 2011’, ‘June 5, 2013’], ‘usefulCount’: [36, 13, 128]}’’’
12345## 重命名
drug_dataset &#x3D; drug_dataset.rename_column(    original_column_name&#x3D;”Unnamed: 0”, new_column_name&#x3D;”patient_id”)drug_dataset‘’’DatasetDict({    train: Dataset({        features: [‘patient_id’, ‘drugName’, ‘condition’, ‘review’, ‘rating’, ‘date’, ‘usefulCount’],        num_rows: 161297    })    test: Dataset({        features: [‘patient_id’, ‘drugName’, ‘condition’, ‘review’, ‘rating’, ‘date’, ‘usefulCount’],        num_rows: 53766    })})’’’
12345678910111213补充一个匿名表达式的细节&gt; `(lambda base, height: 0.5 * base * height)(4, 8)`&gt;&gt; 16 ## 转换大小写
def lowercase_condition(example):    return {“condition”: example[“condition”].lower()}
drug_dataset.map(lowercase_condition)‘’’AttributeError: ‘NoneType’ object has no attribute ‘lower’’’’
123456789这里报错了## filter`dataset.filter`
drug_dataset &#x3D; drug_dataset.filter(lambda x: x[“condition”] is not None)
drug_dataset &#x3D; drug_dataset.map(lowercase_condition)
Check that lowercasing workeddrug_dataset[“train”][“condition”][:3]
‘’’[‘left ventricular dysfunction’, ‘adhd’, ‘birth control’]’’’
1234567过滤筛选合格的数据样本## 增加列
def compute_review_length(example):    return {“review_length”: len(example[“review”].split())}
drug_dataset &#x3D; drug_dataset.map(compute_review_length)
Inspect the first training exampledrug_dataset[“train”][0]‘’’{‘patient_id’: 206461, ‘drugName’: ‘Valsartan’, ‘condition’: ‘left ventricular dysfunction’, ‘review’: ‘“It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil”‘, ‘rating’: 9.0, ‘date’: ‘May 20, 2012’, ‘usefulCount’: 27, ‘review_length’: 17}’’’
12345补充一个sort

drug_dataset[“train”].sort(“review_length”)[:3]‘’’{‘patient_id’: [103488, 23627, 20558],‘drugName’: [‘Loestrin 21 1 &#x2F; 20’, ‘Chlorzoxazone’, ‘Nucynta’],‘condition’: [‘birth control’, ‘muscle spasm’, ‘pain’],‘review’: [‘“Excellent.”‘, ‘“useless”‘, ‘“ok”‘],‘rating’: [10.0, 1.0, 6.0],‘date’: [‘November 4, 2008’, ‘March 24, 2017’, ‘August 20, 2016’],‘usefulCount’: [5, 2, 10],‘review_length’: [1, 1, 1]}’’’
123456789101112131415&gt;sort应该也有reverse选项，如果真要做EDA还是用Pandas好了, [查看可配置参数](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.sort)在补充一个`Dataset.add_column()`An alternative way to add new columns to a dataset is with the `Dataset.add_column()` function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where `Dataset.map()` is not well suited for your analysis.## 解析html字符
import html

text &#x3D; “I&amp;#039;m a transformer called BERT”html.unescape(text)
‘’’“I’m a transformer called BERT”‘’’
123
drug_dataset &#x3D; drug_dataset.map(lambda x: {“review”: html.unescape(x[“review”])})
12345678910111213# map 方法## batchWhen you specify `batched=True` the function receives a dictionary with the fields of the dataset, but each value is now a *list of values*, and not just a single value. The return value of `Dataset.map()` should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. 
new_drug_dataset &#x3D; drug_dataset.map(    lambda x: {“review”: [html.unescape(o) for o in x[“review”]]}, batched&#x3D;True)
123456789101112131415批量处理为True的话，每次传进来就是一个字典批次。一般我们做的就是更新这个数据集If you’re running this code in a notebook, you’ll see that this command executes way faster than the previous one. And it’s not because our reviews have already been HTML-unescaped — if you re-execute the instruction from the previous section (without `batched=True`), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a `for` loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.之前单个处理的用的是for循环，这里批量处理就可以用列表推导式，要快的多## 配合tokenizer使用
def tokenize_and_split(examples):    return tokenizer(        examples[“review”],        truncation&#x3D;True,        max_length&#x3D;128,        return_overflowing_tokens&#x3D;True,    )
result &#x3D; tokenize_and_split(drug_dataset[“train”][0])[len(inp) for inp in result[“input_ids”]]
[128, 49]12345使用`return_overflowing_tokens`参数来接受截断的部分，这里我们177的长度变成了128和49两份
tokenized_dataset &#x3D; drug_dataset.map(tokenize_and_split, batched&#x3D;True)
1234567891011121314151617# 数据类型转换## PandasTo enable the conversion between various third-party libraries, 🤗 Datasets provides a `Dataset.set_format()` function. This function only changes the *output format* of the dataset, so you can easily switch to another format without affecting the underlying *data format*, which is Apache Arrow. The formatting is done in place. To demonstrate, let’s convert our dataset to Pandas:`drug_dataset.set_format(&quot;pandas&quot;)`一般使用`train_df = drug_dataset[&quot;train&quot;][:]` 获得整体的切片作为新的Dataframe 可以自己尝试是否返回对象为hf的dataset
from datasets import Dataset
freq_dataset &#x3D; Dataset.from_pandas(frequencies)freq_dataset‘’’Dataset({    features: [‘condition’, ‘frequency’],    num_rows: 819})’’’
123456789可以转换回来# train_test_split
drug_dataset_clean &#x3D; drug_dataset[“train”].train_test_split(train_size&#x3D;0.8, seed&#x3D;42)
Rename the default “test” split to “validation”drug_dataset_clean[“validation”] &#x3D; drug_dataset_clean.pop(“test”)
Add the “test” set to our DatasetDictdrug_dataset_clean[“test”] &#x3D; drug_dataset[“test”]drug_dataset_clean
‘’’DatasetDict({    train: Dataset({        features: [‘patient_id’, ‘drugName’, ‘condition’, ‘review’, ‘rating’, ‘date’, ‘usefulCount’, ‘review_length’, ‘review_clean’],        num_rows: 110811    })    validation: Dataset({        features: [‘patient_id’, ‘drugName’, ‘condition’, ‘review’, ‘rating’, ‘date’, ‘usefulCount’, ‘review_length’, ‘review_clean’],        num_rows: 27703    })    test: Dataset({        features: [‘patient_id’, ‘drugName’, ‘condition’, ‘review’, ‘rating’, ‘date’, ‘usefulCount’, ‘review_length’, ‘review_clean’],        num_rows: 46108    })})’’’
12345678910111213141516171819# 保存文件| Data format | Function                 || ----------- | ------------------------ || Arrow       | `Dataset.save_to_disk()` || CSV         | `Dataset.to_csv()`       || JSON        | `Dataset.to_json ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/#content-inner">5</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/poyone1222/eris/eris11.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Poy One</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/poyone"><i></i><span>🛴前往github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/poyone" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:poyone1222@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客 <br> QQ 914987163</div></div><div class="sticky_layout"><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">26</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">45.5k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-12T09:34:34.102Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Poy One</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer src="/js/light.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka15.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/CV/&quot;);" href="javascript:void(0);">CV</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">计算机视觉</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka22.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/NLP/&quot;);" href="javascript:void(0);">NLP</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">自然语言处理</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka10.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Trick/&quot;);" href="javascript:void(0);">Trick</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr">import torch as tf</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka29.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Dive-Into-Paper/&quot;);" href="javascript:void(0);">Dive Into Paper</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">论文精读</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka16.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Python/&quot;);" href="javascript:void(0);">Python</a><span class="categoryBar-list-count">3</span><span class="categoryBar-list-descr">流畅的Python</span></li><li class="categoryBar-list-item" style="background:url(https://npm.elemecdn.com/poyone1222/Asuka/Asuka32.webp);"> <a class="categoryBar-list-link" onclick="pjax.loadUrl(&quot;categories/Universe/&quot;);" href="javascript:void(0);">Universe</a><span class="categoryBar-list-count">14</span><span class="categoryBar-list-descr">拥有一切 却变成太空</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>